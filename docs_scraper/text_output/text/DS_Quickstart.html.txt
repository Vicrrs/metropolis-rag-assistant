Quickstart Guide — DeepStream documentationSkip to main contentBack to topCtrl+KDeepStream documentationDeepStream documentationTable of ContentsDeepStream Getting StartedWelcome to the DeepStream DocumentationMigration GuideInstallationQuickstart GuideDocker ContainersDeepStream SamplesC/C++ Sample Apps Source DetailsPython Sample Apps and Bindings Source DetailsDeepStream Reference Application - deepstream-appDeepStream Reference Application - deepstream-test5 appDeepStream Reference Application - deepstream-nmos appDeepStream Reference Application on GitHubSample Configurations and StreamsImplementing a Custom GStreamer Plugin with OpenCV Integration ExampleTAO toolkit Integration with DeepStreamTAO Toolkit Integration with DeepStreamTutorials and How-to'sDeepStream-3D Custom Apps and Libs TutorialsDeepStream PerformancePerformanceDeepStream AccuracyAccuracy Tuning ToolsDeepStream Custom ModelUsing a Custom Model with DeepStreamDeepStream Key FeaturesDeepStream-3D Sensor Fusion Multi-Modal Application and FrameworkDeepStream-3D Multi-Modal BEVFusion SetupDeepStream-3D Multi-Modal V2XFusion SetupSmart Video RecordIoTOn the Fly Model UpdateNTP Timestamp in DeepStreamAV Sync in DeepStreamDeepStream With REST API SeverDeepStream 3D Action Recognition AppDeepStream 3D Depth Camera AppDeepStream 3D Lidar Inference AppNetworked Media Open Specifications (NMOS) in DeepStreamGst-nvdspostprocess in DeepStreamDeepStream Can Orientation AppDeepStream Application MigrationApplication Migration to DeepStream 7.1 from DeepStream 7.0DeepStream Plugin GuideGStreamer Plugin OverviewMetaData in the DeepStream SDKGst-nvdspreprocess (Alpha)Gst-nvinferGst-nvinferserverGst-nvtrackerGst-nvstreammuxGst-nvstreammux NewGst-nvstreamdemuxGst-nvmultistreamtilerGst-nvdsosdGst-nvdsmetautilsGst-nvdsvideotemplateGst-nvdsaudiotemplateGst-nvvideoconvertGst-nvdewarperGst-nvofGst-nvofvisualGst-nvsegvisualGst-nvvideo4linux2Gst-nvjpegdecGst-nvimagedecGst-nvjpegencGst-nvimageencGst-nvmsgconvGst-nvmsgbrokerGst-nvdsanalyticsGst-nvdsudpsrcGst-nvdsudpsinkGst-nvdspostprocess (Alpha)Gst-nvds3dfilterGst-nvds3dbridgeGst-nvds3dmixerGst-NvDsUcxGst-nvdsxferGst-nvvideotestsrcGst-nvmultiurisrcbinGst-nvurisrcbinDeepStream Troubleshooting and FAQTroubleshootingFrequently Asked QuestionsDeepStream On WSL2DeepStream On WSLFAQ for Deepstream On WSLDeepStream API GuideDeepStream API GuidesDeepStream Service MakerWhat is Deepstream Service MakerService Maker for C/C++ DevelopersService Maker for Python Developers(alpha)Quick Start GuideIntroduction to Flow APIsIntroduction to Pipeline APIsAdvanced FeaturesMigrating Traditional Deepstream Apps to Service Maker Apps in PythonWhat is a Deepstream Service Maker PluginDeepstream LibrariesDeepStream Libraries (Developer Preview)Graph ComposerOverviewPlatformsSupported platformsGetting StartedApplication Development WorkflowCreating an AI ApplicationReference graphsExtension Development WorkflowDeveloping Extensions for DeepStreamDeepStream ComponentsGXF InternalsGXF InternalsGraph eXecution EngineGraph Execution EngineGraph Composer ContainersGraph Composer and GXF ContainersGXF Component InterfacesGXF Component InterfacesGXF Application API'sGXF App C++ APIsGXF App Python APIsGXF Runtime API'sGXF Core C++ APIsGXF Core C APIsGXF Core Python APIsExtension ManualExtensionsCudaExtensionGXF Stream SyncStandardExtensionPython CodeletsNetworkExtensionNvTritonExtSerializationExtensionMultimediaExtensionVideoEncoderExtensionVideoDecoderExtensionBehavior TreesUCX ExtensionHttpExtensionGrpcExtensionTensorRTExtensionNvDs3dProcessingExtNvDsActionRecognitionExtNvDsAnalyticsExtNvDsBaseExtNvDsCloudMsgExtNvDsConverterExtNvDsDewarperExtNvDsInferenceExtNvDsInferenceUtilsExtNvDsInterfaceExtNvDsMuxDemuxExtNvDsOpticalFlowExtNvDsOutputSinkExtNvDsSampleExtNvDsSampleModelsExtNvDsSourceExtNvDsTemplateExtNvDsTrackerExtNvDsTranscodeExtNvDsTritonExtNvDsUcxExtNvDsUdpExtNvDsVisualizationExtToolsRegistryRegistry Command Line InterfaceComposerContainer BuilderGXF Command Line InterfacePipetuner GuideFAQ GuideFAQDeepStream Legal InformationDeepStream End User License AgreementDeepStream FeedbackFeedback formQuickstart GuideQuickstart Guide#Jetson [Not applicable for NVAIE customers]#This section explains how to use DeepStream SDK on a Jetson device.Boost the clocks#After you have installed DeepStream SDK, run these commands on the Jetson device to boost the clocks:$sudonvpmodel-m0$sudojetson_clocksYou should run these commands before running DeepStream application.Run deepstream-app (the reference application)#Navigate to the configs/deepstream-app directory on the development kit.$ cd /opt/nvidia/deepstream/deepstream-7.1/samples/configs/deepstream-appEnter the following command to run the reference application:# deepstream-app -c <path_to_config_file>e.g.$deepstream-app-csource30_1080p_dec_infer-resnet_tiled_display_int8.txtWhere<path_to_config_file>is the pathname of one of the reference application’s configuration files, found inconfigs/deepstream-app/. See Package Contents inconfigs/deepstream-app/for a list of the available files.Config files that can be run with deepstream-app:source30_1080p_dec_infer-resnet_tiled_display_int8.txtsource30_1080p_dec_preprocess_infer-resnet_tiled_display_int8.txtsource4_1080p_dec_infer-resnet_tracker_sgie_tiled_display_int8.txtsource1_usb_dec_infer_resnet_int8.txtsource1_csi_dec_infer_resnet_int8.txtsource2_csi_usb_dec_infer_resnet_int8.txtsource6_csi_dec_infer_resnet_int8.txtsource2_1080p_dec_infer-resnet_demux_int8.txtsource4_1080p_dec_infer-resnet_tracker_sgie_tiled_display_int8.ymlsource30_1080p_dec_infer-resnet_tiled_display_int8.ymlsource4_1080p_dec_preprocess_infer-resnet_preprocess_sgie_tiled_display_int8.txtsource2_dewarper_test.txtNoteRefer toSample Configurations and Streamsfor detailed explanation of each configuration file.NoteYou can find sample configuration files under/opt/nvidia/deepstream/deepstream-7.1/samplesdirectory. Enter this command to see application usage:$deepstream-app--helpTo save TensorRT Engine/Plan file, run the following command:$sudodeepstream-app-c<path_to_config_file>To show labels in 2D Tiled display view, expand the source of interest with mouseleft-clickon the source. To return to the tiled display,right-clickanywhere in the window.Keyboard selection of source is also supported. On the console where application is running, press thezkey followed by the desired row index (0 to 9), then the column index (0 to 9) to expand the source. To restore 2D Tiled display view, presszagain.Expected output (deepstream-app)#The image below shows the expected output of deepstream-app withsource30_1080p_dec_infer-resnet_tiled_display_int8.txtconfig:Run precompiled sample applications#Navigate to the chosen application directory insidesources/apps/sample_apps.Follow the directory’s README file to run the application.NoteIf the application encounters errors and cannot create Gst elements, remove the GStreamer cache, then try again. To remove the GStreamer cache, enter this command:$rm${HOME}/.cache/gstreamer-1.0/registry.aarch64.binWhen the application is run for a model which does not have an existing engine file, it may take up to a few minutes (depending on the platform and the model) for the file generation and the application launch. For later runs, these generated engine files can be reused for faster loading.dGPU for Ubuntu#This section explains how to use DeepStream SDK on a x86 machine.Run the deepstream-app (the reference application)#Navigate to the configs/deepstream-app directory on the development kit.$ cd /opt/nvidia/deepstream/deepstream-7.1/samples/configs/deepstream-appEnter the following command to run the reference application:# deepstream-app -c <path_to_config_file>e.g.$deepstream-app-csource30_1080p_dec_infer-resnet_tiled_display_int8.txtWhere<path_to_config_file>is the pathname of one of the reference application’s configuration files, found inconfigs/deepstream-app. See Package Contents inconfigs/deepstream-app/for a list of the available files.Config files that can be run with deepstream-app:source30_1080p_dec_infer-resnet_tiled_display_int8.txtsource30_1080p_dec_preprocess_infer-resnet_tiled_display_int8.txtsource4_1080p_dec_infer-resnet_tracker_sgie_tiled_display_int8.txtsource4_1080p_dec_infer-resnet_tracker_sgie_tiled_display_int8_gpu1.txtsource1_usb_dec_infer_resnet_int8.txtsource2_1080p_dec_infer-resnet_demux_int8.txtsource4_1080p_dec_infer-resnet_tracker_sgie_tiled_display_int8.ymlsource30_1080p_dec_infer-resnet_tiled_display_int8.ymlsource4_1080p_dec_preprocess_infer-resnet_preprocess_sgie_tiled_display_int8.txtsource2_dewarper_test.txtNoteRefer toSample Configurations and Streamsfor detailed explanation of each configuration file.NoteTo dump engine file, run the following command:$sudodeepstream-app-c<path_to_config_file>You can find sample configuration files under/opt/nvidia/deepstream/deepstream-7.1/samplesdirectory. Enter this command to see application usage:$deepstream-app--helpTo show labels in 2D tiled display view, expand the source of interest with a mouseleft-clickon the source. To return to the tiled display,right-clickanywhere in the window.Keyboard selection of source is also supported. On the console where application is running, press thezkey followed by the desired row index (0 to 9), then the column index (0 to 9) to expand the source. To restore the 2D Tiled display view, presszagain.Expected output (deepstream-app)#The image below shows the expected output of deepstream-app withsource30_1080p_dec_infer-resnet_tiled_display_int8.txtconfig:Run precompiled sample applications#Navigate to the chosen application directory insidesources/apps/sample_apps.Follow that directory’s README file to run the application.NoteIf the application encounters errors and cannot create Gst elements, remove the GStreamer cache, then try again. To remove the GStreamer cache, enter this command:$rm${HOME}/.cache/gstreamer-1.0/registry.x86_64.binWhen the application is run for a model which does not have an existing engine file, it may take up to a few minutes (depending on the platform and the model) for the file generation and application launch. For later runs, these generated engine files can be reused for faster loading.How to visualize the output if the display is not attached to the system#1 . Running with an X server by creating virtual display#Referhttps://docs.nvidia.com/grid/latest/grid-vgpu-user-guide/index.html#configuring-xorg-server-on-linux-serverfor details.2 . Running without an X server (applicable for applications supporting RTSP streaming output)#The default configuration files provided with the SDK have the EGL basednveglglessinkas the default renderer (indicated by type=2 in the [sink] groups). The renderer requires a running X server and fails without one.
In case of absence of an X server, DeepStream reference applications provide an alternate functionality of streaming the output over RTSP. This can be enabled by adding an RTSP out sink group in the configuration file. Refer to[sink2]group insource30_1080p_dec_infer-resnet_tiled_display_int8.txtfile for an example. Don’t forget to disable thenveglglessinkrenderer by setting enable=0 for the corresponding sink group.DeepStream Triton Inference Server Usage Guidelines#To migrate the Triton version in a DeepStream 7.1 deployment (Triton 24.08) to a newer version (say Triton 24.09 or newer), follow the instructions atDeepStream Triton Migration Guide.NoteBefore runningprepare_classification_test_video.sh, refer note in sectionDocker Containers.dGPU#Pull the DeepStream Triton Inference Server dockerdockerpullnvcr.io/nvidia/deepstream:7.1-triton-multiarchStart the dockerdockerrun--gpus"device=0"-it--rm-v/tmp/.X11-unix:/tmp/.X11-unix-eDISPLAY=$DISPLAY-eCUDA_CACHE_DISABLE=0nvcr.io/nvidia/deepstream:7.1-triton-multiarchNoteThe triton docker for x86 & jetson is based on Tritonserver 24.08 docker and has Ubuntu 22.04.When the triton docker is launched for the first time, it might take a few minutes to start since it has to generate compute cache.dGPU on ARM (IGX/dGPU, GH100, GH200, SBSA)#Pull the DeepStream Triton Inference Server dockerdockerpullnvcr.io/nvidia/deepstream:7.1-triton-arm-sbsaStart the dockersudodockerrun-it--rm--runtime=nvidia--network=host-eNVIDIA_DRIVER_CAPABILITIES=compute,utility,video,graphics--gpusall--privileged-eDISPLAY=:0-v/tmp/.X11-unix:/tmp/.X11-unix-v/etc/X11:/etc/X11nvcr.io/nvidia/deepstream:7.1-triton-arm-sbsaNoteThe triton docker for dGPU on ARM is based on Tritonserver 24.08 docker and has Ubuntu 22.04.When the triton docker is launched for the first time, it might take a few minutes to start since it has to generate compute cache.dGPU on ARM/SBSA docker currently supports only nv3dsink for video display via workaround mentioned in sectionKnown Limitation with Video Subsystem and Workaround.Jetson#DeepStream Triton container image (nvcr.io/nvidia/deepstream:7.1-triton-multiarch) has Triton Inference Server and supported backend libraries pre-installed.In order to run the Triton Inference Server directly on device, i.e., without docker, Triton Server setup will be required.Go to samples directory and run the following commands to set up the Triton Server and backends.$cd/opt/nvidia/deepstream/deepstream/samples/$sudo./triton_backend_setup.shNoteBy default script will download the Triton Server version 2.49. For setting up any other version change the package path accordingly.Triton backends are installed into/opt/nvidia/deepstream/deepstream/lib/triton_backendsby default by the script. User can updateinfer_configsettings for specific folders as follows:model_repo{backend_dir:/opt/nvidia/tritonserver/backends/}Using DLA for inference#DLA is Deep Learning Accelerator present on the Jetson AGX Orin and Jetson Orin NX. These platforms have two DLA engines. DeepStream can be configured to run inference on either of the DLA engines through the Gst-nvinfer plugin. One instance of Gst-nvinfer plugin and thus a single instance of a model can be configured to be executed on a single DLA engine or the GPU. However, multiple Gst-nvinfer plugin instances can be configured to use the same DLA. To configure Gst-nvinfer to use the DLA engine for inference, modify the corresponding property in nvinfer component configuration file (example: samples/configs/deepstream-app/config_infer_primary.txt):
Setenable-dla=1in [property] group. Setuse-dla-core=0oruse-dla-core=1depending on the DLA engine to use.DeepStream does support inferencing using GPU and DLAs in parallel. You can run this in separate processes or single process. You will need three separate sets of configs configured to run on GPU, DLA0 and DLA1:Separate processes#When GPU and DLA are run in separate processes, set the environment variableCUDA_DEVICE_MAX_CONNECTIONSas1from the terminal where DLA config is running.Single process#DeepStream reference application supports multiple configs in the same process. To run DLA and GPU in same process, set environment variableCUDA_DEVICE_MAX_CONNECTIONSas32:$deepstream-app-c<gpuconfig>-c<dla0config>-c<dla1config>previousInstallationnextDocker ContainersOn this pageJetson [Not applicable for NVAIE customers]Boost the clocksRun deepstream-app (the reference application)Expected output (deepstream-app)Run precompiled sample applicationsdGPU for UbuntuRun the deepstream-app (the reference application)Expected output (deepstream-app)Run precompiled sample applicationsHow to visualize the output if the display is not attached to the system1 . Running with an X server by creating virtual display2 . Running without an X server (applicable for applications supporting RTSP streaming output)DeepStream Triton Inference Server Usage GuidelinesdGPUdGPU on ARM (IGX/dGPU, GH100, GH200, SBSA)JetsonUsing DLA for inferenceSeparate processesSingle processPrivacy Policy|Manage My Privacy|Do Not Sell or Share My Data|Terms of Service|Accessibility|Corporate Policies|Product Security|ContactCopyright © 2024-2025, NVIDIA Corporation.Last updated on Jan 13, 2025.