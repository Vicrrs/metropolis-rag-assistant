Using a Custom Model with DeepStream — DeepStream documentationSkip to main contentBack to topCtrl+KDeepStream documentationDeepStream documentationTable of ContentsDeepStream Getting StartedWelcome to the DeepStream DocumentationMigration GuideInstallationQuickstart GuideDocker ContainersDeepStream SamplesC/C++ Sample Apps Source DetailsPython Sample Apps and Bindings Source DetailsDeepStream Reference Application - deepstream-appDeepStream Reference Application - deepstream-test5 appDeepStream Reference Application - deepstream-nmos appDeepStream Reference Application on GitHubSample Configurations and StreamsImplementing a Custom GStreamer Plugin with OpenCV Integration ExampleTAO toolkit Integration with DeepStreamTAO Toolkit Integration with DeepStreamTutorials and How-to'sDeepStream-3D Custom Apps and Libs TutorialsDeepStream PerformancePerformanceDeepStream AccuracyAccuracy Tuning ToolsDeepStream Custom ModelUsing a Custom Model with DeepStreamDeepStream Key FeaturesDeepStream-3D Sensor Fusion Multi-Modal Application and FrameworkDeepStream-3D Multi-Modal BEVFusion SetupDeepStream-3D Multi-Modal V2XFusion SetupSmart Video RecordIoTOn the Fly Model UpdateNTP Timestamp in DeepStreamAV Sync in DeepStreamDeepStream With REST API SeverDeepStream 3D Action Recognition AppDeepStream 3D Depth Camera AppDeepStream 3D Lidar Inference AppNetworked Media Open Specifications (NMOS) in DeepStreamGst-nvdspostprocess in DeepStreamDeepStream Can Orientation AppDeepStream Application MigrationApplication Migration to DeepStream 7.1 from DeepStream 7.0DeepStream Plugin GuideGStreamer Plugin OverviewMetaData in the DeepStream SDKGst-nvdspreprocess (Alpha)Gst-nvinferGst-nvinferserverGst-nvtrackerGst-nvstreammuxGst-nvstreammux NewGst-nvstreamdemuxGst-nvmultistreamtilerGst-nvdsosdGst-nvdsmetautilsGst-nvdsvideotemplateGst-nvdsaudiotemplateGst-nvvideoconvertGst-nvdewarperGst-nvofGst-nvofvisualGst-nvsegvisualGst-nvvideo4linux2Gst-nvjpegdecGst-nvimagedecGst-nvjpegencGst-nvimageencGst-nvmsgconvGst-nvmsgbrokerGst-nvdsanalyticsGst-nvdsudpsrcGst-nvdsudpsinkGst-nvdspostprocess (Alpha)Gst-nvds3dfilterGst-nvds3dbridgeGst-nvds3dmixerGst-NvDsUcxGst-nvdsxferGst-nvvideotestsrcGst-nvmultiurisrcbinGst-nvurisrcbinDeepStream Troubleshooting and FAQTroubleshootingFrequently Asked QuestionsDeepStream On WSL2DeepStream On WSLFAQ for Deepstream On WSLDeepStream API GuideDeepStream API GuidesDeepStream Service MakerWhat is Deepstream Service MakerService Maker for C/C++ DevelopersService Maker for Python Developers(alpha)Quick Start GuideIntroduction to Flow APIsIntroduction to Pipeline APIsAdvanced FeaturesMigrating Traditional Deepstream Apps to Service Maker Apps in PythonWhat is a Deepstream Service Maker PluginDeepstream LibrariesDeepStream Libraries (Developer Preview)Graph ComposerOverviewPlatformsSupported platformsGetting StartedApplication Development WorkflowCreating an AI ApplicationReference graphsExtension Development WorkflowDeveloping Extensions for DeepStreamDeepStream ComponentsGXF InternalsGXF InternalsGraph eXecution EngineGraph Execution EngineGraph Composer ContainersGraph Composer and GXF ContainersGXF Component InterfacesGXF Component InterfacesGXF Application API'sGXF App C++ APIsGXF App Python APIsGXF Runtime API'sGXF Core C++ APIsGXF Core C APIsGXF Core Python APIsExtension ManualExtensionsCudaExtensionGXF Stream SyncStandardExtensionPython CodeletsNetworkExtensionNvTritonExtSerializationExtensionMultimediaExtensionVideoEncoderExtensionVideoDecoderExtensionBehavior TreesUCX ExtensionHttpExtensionGrpcExtensionTensorRTExtensionNvDs3dProcessingExtNvDsActionRecognitionExtNvDsAnalyticsExtNvDsBaseExtNvDsCloudMsgExtNvDsConverterExtNvDsDewarperExtNvDsInferenceExtNvDsInferenceUtilsExtNvDsInterfaceExtNvDsMuxDemuxExtNvDsOpticalFlowExtNvDsOutputSinkExtNvDsSampleExtNvDsSampleModelsExtNvDsSourceExtNvDsTemplateExtNvDsTrackerExtNvDsTranscodeExtNvDsTritonExtNvDsUcxExtNvDsUdpExtNvDsVisualizationExtToolsRegistryRegistry Command Line InterfaceComposerContainer BuilderGXF Command Line InterfacePipetuner GuideFAQ GuideFAQDeepStream Legal InformationDeepStream End User License AgreementDeepStream FeedbackFeedback formUsing a...Using a Custom Model with DeepStream#The NVIDIA® DeepStream SDK on NVIDIA® Tesla® or NVIDIA® Jetson platforms can be customized to support custom neural networks for object detection and classification.
You can create your own model. You must specify the applicable configuration parameters in the [property] group of thenvinferconfiguration file (for example,config_infer_primary.txt).
The configuration parameters that you must specify include:model-file (Caffe model)proto-file (Caffe model)onnx-file (ONNX models)model-engine-file, if already generatedint8-calib-file for INT8 modemean-file, if requiredoffsets, if requiredmaintain-aspect-ratio, if requiredparse-bbox-func-name (detectors only)parse-classifier-func-name (classifiers only)custom-lib-pathnetwork-typemodel-color-formatprocess-modeengine-create-func-nameinfer-dims (UFF models)Custom Model Implementation Interface#nvinfer supports interfaces for these purposes:Custom bounding box parsing for custom neural network detectors and classifiersIPlugin implementation for layers not natively supported by NVIDIA® TensorRT™Initializing non-image input layers in cases where the network has more than one input layerCreating a CUDA engine using TensorRT Layer APIs instead of model parsing APIs. Read more about TensorRT docs here:https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.htmlIModelParser interface to parse the model and fill the layers in an INetworkDefinitionAll the interface implementations for the models must go into a single independent shared library. nvinfer dynamically loads the library withdlopen(), looks for implemented interfaces withdlsym(), and calls the interfaces as required.
For more information about the interface, refer to the header filenvdsinfer_custom_impl.h.Custom Output Parsing#For detectors, you must write a library that can parse the bounding box coordinates and the object class from the output layers. For classifiers, the library must parse the object attributes from the output layers. For example, in segmentation models, the library must parse the bounding box, object class and the mask from the output layers. You can find example code and makefile in the source directory insources/libs/nvdsinfer_customparser.
The generated library path and the function name must be specified with the configuration parameters as mentioned in the section Custom Model. The README file insources/libs/nvdsinfer_customparserhas an example of how to use this custom parser.
Following output parsers are supported in the current release:MaskRCNNSSDYoloV3 / YoloV3Tiny / YoloV2 / YoloV2TinyDetectNetIPlugin Implementation#DeepStream supports networks containing layers not supported by TensorRT but supported through implementations of the IPlugin interface. TheobjectDetector_SSD,objectDetector_FasterRCNN, andobjectDetector_YoloV3sample applications show examples of IPlugin implementations.
DeepStream supports NVIDIA® TensorRT™ plugins for custom layers. The Gst-nvinfer plugin now has support for the IPluginV2 and IPluginCreator interface, introduced in TensorRT 5.0. For caffemodels and for backward compatibility with existing plugins, it also supports the following interfaces:nvinfer1::IPluginFactorynvuffparser::IPluginFactorynvuffparser::IPluginFactoryExtnvcaffeparser1::IPluginFactorynvcaffeparser1::IPluginFactoryExtnvcaffeparser1::IPluginFactoryV2See the TensorRT documentation for details on new and deprecated plugin interfaces.How to Use IPluginCreator#To use the new IPluginCreator interface you must implement the interface in an independent custom library. This library must be passed to the Gst-nvinfer plugin through its configuration file by specifying the library’s pathname with the custom-lib-path key.
Gst-nvinfer opens the library withdlopen(), which causes the plugin to be registered with TensorRT. There is no further direct interaction between the custom library and Gst-nvinfer. TensorRT calls the custom plugin functions as required.
The SSD sample provided with the SDK provides an example of using the IPluginV2 and IPluginCreator interface. This sample has been adapted from TensorRT.How to Use IPluginFactory#To use the IPluginFactory interface, you must implement the interface in an independent custom library. Pass this library to the Gst-nvinfer plugin through the plugin’s configuration file by specifying the library’s pathname in the custom-lib-path key. The custom library must implement the applicable functions:NvDsInferPluginFactoryCaffeGetNvDsInferPluginFactoryCaffeDestroyNvDsInferPluginFactoryUffGetNvDsInferPluginFactoryUffDestroyNvDsInferPluginFactoryRuntimeGetNvDsInferPluginFactoryRuntimeDestroyThese structures are defined innvdsinfer_custom_impl.h. The function definitions must be named as in the header file. Gst-nvinfer opens the custom library withdlopen()and looks for the names.For Caffe Files#During parsing and building of a Caffe network, Gst-nvinfer looks forNvDsInferPluginFactoryCaffeGet. If found, it calls the function to get the IPluginFactory instance. Depending on the type of IPluginFactory returned, Gst-nvinfer sets the factory using one of the ICaffeParser interface’s methodssetPluginFactory(),setPluginFactoryExt(), orsetPluginFactoryV2().
After the network has been built and serialized, Gst-nvinfer looks forNvDsInferPluginFactoryCaffeDestroyand calls it to destroy the IPluginFactory instance.For Uff Files#During parsing and building of a Caffe network, Gst-nvinfer looks forNvDsInferPluginFactoryUffGet. If found, it calls the function to get the IPluginFactory instance. Depending on the type of IPluginFactory returned, Gst-nvinfer sets the factory using one of the IUffParser inteface’s methodssetPluginFactory()orsetPluginFactoryExt().
After the network has been built and serialized, Gst-nvinfer looks forNvDsInferPluginFactoryUffDestroyand calls it to destroy the IPluginFactory instance.During Deserialization#If deserializing the models requires an instance ofNvInfer1::IPluginFactory, the custom library must also implementNvDsInferPluginFactoryRuntimeGet()and optionallyNvDsInferPluginFactoryRuntimeDestroy(). During deserialization, Gst-nvinfer calls the library’sNvDsInferPluginFactoryRuntimeGet()function to get the IPluginFactory instance, then callsNvDsInferPluginFactoryRuntimeDestroyto destroy the instance if it finds that function during Gst-nvinfer deinitialization.
The FasterRCNN sample provided with the SDK provides an example of using theIPluginV2+nvcaffeparser1::IPluginFactoryV2interface with DeepStream. This sample has been adapted from TensorRT. It also provides an example of using the legacyIPlugin+nvcaffeparser1::IPluginFactory+Gst-nvinfer1::IPluginFactoryinterface for backward compatibility.Input Layer Initialization#DeepStream supports initializing non-image input layers for networks having more than one input layer. The layers are initialized only once before the first inference call. The objectDetector_FasterRCNN sample application shows an example of an implementation.CUDA Engine Creation for Custom Models#DeepStream supports creating TensorRT CUDA engines for models which are not in Caffe, UFF, or ONNX format, or which must be created from TensorRT Layer APIs. The objectDetector_YoloV3 sample application shows an example of the implementation. When using a single custom library for multiple nvinfer plugin instances in a pipeline, each instance can have its own implementation of engine-create-func-name and this can be specified in the configuration file. An example would be back-to-back detector pipeline with different types of yolo models.IModelParser Interface for Custom Model Parsing#This is an alternative to the “CUDA Engine Creation” interface for parsing and filling a TensorRT network (INetworkDefinition). TheobjectDetector_YoloV3sample application shows an example of the implementation.previousAccuracy Tuning ToolsnextDeepStream-3D Sensor Fusion Multi-Modal Application and FrameworkOn this pageCustom Model Implementation InterfaceCustom Output ParsingIPlugin ImplementationHow to Use IPluginCreatorHow to Use IPluginFactoryFor Caffe FilesFor Uff FilesDuring DeserializationInput Layer InitializationCUDA Engine Creation for Custom ModelsIModelParser Interface for Custom Model ParsingPrivacy Policy|Manage My Privacy|Do Not Sell or Share My Data|Terms of Service|Accessibility|Corporate Policies|Product Security|ContactCopyright © 2024-2025, NVIDIA Corporation.Last updated on Jan 13, 2025.