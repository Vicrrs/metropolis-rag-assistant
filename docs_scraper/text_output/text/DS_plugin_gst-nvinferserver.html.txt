Gst-nvinferserver — DeepStream documentationSkip to main contentBack to topCtrl+KDeepStream documentationDeepStream documentationTable of ContentsDeepStream Getting StartedWelcome to the DeepStream DocumentationMigration GuideInstallationQuickstart GuideDocker ContainersDeepStream SamplesC/C++ Sample Apps Source DetailsPython Sample Apps and Bindings Source DetailsDeepStream Reference Application - deepstream-appDeepStream Reference Application - deepstream-test5 appDeepStream Reference Application - deepstream-nmos appDeepStream Reference Application on GitHubSample Configurations and StreamsImplementing a Custom GStreamer Plugin with OpenCV Integration ExampleTAO toolkit Integration with DeepStreamTAO Toolkit Integration with DeepStreamTutorials and How-to'sDeepStream-3D Custom Apps and Libs TutorialsDeepStream PerformancePerformanceDeepStream AccuracyAccuracy Tuning ToolsDeepStream Custom ModelUsing a Custom Model with DeepStreamDeepStream Key FeaturesDeepStream-3D Sensor Fusion Multi-Modal Application and FrameworkDeepStream-3D Multi-Modal BEVFusion SetupDeepStream-3D Multi-Modal V2XFusion SetupSmart Video RecordIoTOn the Fly Model UpdateNTP Timestamp in DeepStreamAV Sync in DeepStreamDeepStream With REST API SeverDeepStream 3D Action Recognition AppDeepStream 3D Depth Camera AppDeepStream 3D Lidar Inference AppNetworked Media Open Specifications (NMOS) in DeepStreamGst-nvdspostprocess in DeepStreamDeepStream Can Orientation AppDeepStream Application MigrationApplication Migration to DeepStream 7.1 from DeepStream 7.0DeepStream Plugin GuideGStreamer Plugin OverviewMetaData in the DeepStream SDKGst-nvdspreprocess (Alpha)Gst-nvinferGst-nvinferserverGst-nvtrackerGst-nvstreammuxGst-nvstreammux NewGst-nvstreamdemuxGst-nvmultistreamtilerGst-nvdsosdGst-nvdsmetautilsGst-nvdsvideotemplateGst-nvdsaudiotemplateGst-nvvideoconvertGst-nvdewarperGst-nvofGst-nvofvisualGst-nvsegvisualGst-nvvideo4linux2Gst-nvjpegdecGst-nvimagedecGst-nvjpegencGst-nvimageencGst-nvmsgconvGst-nvmsgbrokerGst-nvdsanalyticsGst-nvdsudpsrcGst-nvdsudpsinkGst-nvdspostprocess (Alpha)Gst-nvds3dfilterGst-nvds3dbridgeGst-nvds3dmixerGst-NvDsUcxGst-nvdsxferGst-nvvideotestsrcGst-nvmultiurisrcbinGst-nvurisrcbinDeepStream Troubleshooting and FAQTroubleshootingFrequently Asked QuestionsDeepStream On WSL2DeepStream On WSLFAQ for Deepstream On WSLDeepStream API GuideDeepStream API GuidesDeepStream Service MakerWhat is Deepstream Service MakerService Maker for C/C++ DevelopersService Maker for Python Developers(alpha)Quick Start GuideIntroduction to Flow APIsIntroduction to Pipeline APIsAdvanced FeaturesMigrating Traditional Deepstream Apps to Service Maker Apps in PythonWhat is a Deepstream Service Maker PluginDeepstream LibrariesDeepStream Libraries (Developer Preview)Graph ComposerOverviewPlatformsSupported platformsGetting StartedApplication Development WorkflowCreating an AI ApplicationReference graphsExtension Development WorkflowDeveloping Extensions for DeepStreamDeepStream ComponentsGXF InternalsGXF InternalsGraph eXecution EngineGraph Execution EngineGraph Composer ContainersGraph Composer and GXF ContainersGXF Component InterfacesGXF Component InterfacesGXF Application API'sGXF App C++ APIsGXF App Python APIsGXF Runtime API'sGXF Core C++ APIsGXF Core C APIsGXF Core Python APIsExtension ManualExtensionsCudaExtensionGXF Stream SyncStandardExtensionPython CodeletsNetworkExtensionNvTritonExtSerializationExtensionMultimediaExtensionVideoEncoderExtensionVideoDecoderExtensionBehavior TreesUCX ExtensionHttpExtensionGrpcExtensionTensorRTExtensionNvDs3dProcessingExtNvDsActionRecognitionExtNvDsAnalyticsExtNvDsBaseExtNvDsCloudMsgExtNvDsConverterExtNvDsDewarperExtNvDsInferenceExtNvDsInferenceUtilsExtNvDsInterfaceExtNvDsMuxDemuxExtNvDsOpticalFlowExtNvDsOutputSinkExtNvDsSampleExtNvDsSampleModelsExtNvDsSourceExtNvDsTemplateExtNvDsTrackerExtNvDsTranscodeExtNvDsTritonExtNvDsUcxExtNvDsUdpExtNvDsVisualizationExtToolsRegistryRegistry Command Line InterfaceComposerContainer BuilderGXF Command Line InterfacePipetuner GuideFAQ GuideFAQDeepStream Legal InformationDeepStream End User License AgreementDeepStream FeedbackFeedback formGStreamer Plugin OverviewGst-nvinferserverGst-nvinferserver#The Gst-nvinferserver plugin does inferencing on input data using NVIDIA® Triton Inference Server (previously called TensorRT Inference Server) Release 2.49.0, NGC Container 24.08 for both Jetson and dGPU on x86. Refer to the following READMEtriton-inference-server/serverThe plugin accepts batchedNV12/RGBAbuffers from upstream. The NvDsBatchMeta structure must already be attached to the Gst Buffers.The low-level library (libnvds_infer_server) operates on any of NV12 or RGBA buffers.
The Gst-nvinferserver plugin passes the input batched buffers to the low-level library and waits for the results to be available. Meanwhile, it keeps queuing input buffers to the low-level library as they are received. Once the results are available from the low-level library, the plugin translates and attaches the results back in to Gst-buffer for downstream plugins.The low-level library preprocesses the transformed frames (performs color conversion and scaling, normalization and mean subtraction) and produces finalFP32/FP16/INT8/UINT8/INT16/UINT16/INT32/UINT32RGB/BGR/GRAYplanar/packed data which is passed to the Triton for inferencing. The output type generated by the low-level library depends on the network type.The pre-processing function is:y=netscalefactor*(x-mean)Where:xis the input pixel value. It is an uint8 with range [0,255].meanis the corresponding mean value, read either from the mean file or as offsets[c], where c is the channel to which the input pixel belongs, and offsets is the array specified in the configuration file. It is a float.netscalefactoris the pixel scaling factor specified in the configuration file. It is a float.yis the corresponding output pixel value. It can be of typefloat/half/int8/uint8/int16/uint16/int32/uint32.Take specific example for uint8 to int8 conversion. setnetscalefactor=1.0andmean=[128,128,128]. Then the function looks like:y=(1.0)*(x-128)Gst-nvinferserver currently works on the following type of networks:Multi-class object detectionMulti-label classificationSegmentationThe Gst-nvinferserver plugin can work in three process modes:Primary mode: Operates on full frames.Secondary mode: Operates on objects added in the metadata by upstream components.When the plugin is operating as a secondary classifier inasyncmode along with the tracker, it tries to improve performance by avoiding re-inferencing on the same objects in every frame. It does this by caching the classification output in a map with the object’s unique ID as the key. The object is inferred upon only when it is first seen in a frame (based on its object ID) or when the size (bounding box area) of the object increases by 20% or more. This optimization is possible only when the tracker is added as an upstream element.Preprocessed Tensor Input mode: Operates on tensors attached by upstream components.When operating in preprocessed tensor input mode, the pre-processing inside Gst-nvinferserver is completely skipped. The plugin looks forGstNvDsPreProcessBatchMetaattached to the input buffer and passes the tensor as is to the Tirton Inference Server without any modifications. This mode currently supports processing on full-frame and ROI. The GstNvDsPreProcessBatchMeta is attached by the Gst-nvdspreprocess plugin. This mode is enabled by adding the input_tensor_from_meta configuration message in the InferenceConfig message.Detailed documentation of the Triton Inference Server is available at:triton-inference-server/serverThe plugin supports Triton features along with multiple deep-learning frameworks such as TensorRT, TensorFlow (GraphDef / SavedModel), ONNX and PyTorch on Tesla platforms. On Jetson, it also supports TensorRT and TensorFlow (GraphDef / SavedModel). TensorFlow and ONNX can be configured with TensorRT acceleration. For details, seeFramework-Specific Optimization.
The plugin requires a configurable model repository root directory path where all the models need to reside. All the plugin instances in a single process must share the same model root. For details, seeModel Repository. Each model also needs a specificconfig.pbtxtfile in its subdirectory. For details, seeModel Configuration.
The plugin supports Triton ensemble mode to enable users to perform preprocessing or postprocessing with Triton custom backend.
The plugin also supports the interface for custom functions for parsing outputs of object detectors, classifiers, and initialization of non-image input layers in cases where there is more than one input layer.
Refer tosources/includes/nvdsinfer_custom_impl.hfor the custom method implementations for custom models.Downstream components receive a Gst Buffer with unmodified contents plus the metadata created from the inference output of the Gst-nvinferserver plugin. The plugin can be used for cascaded inferencing. That is, it can perform primary inferencing directly on input data, then perform secondary inferencing on the results of primary inferencing, and so on. This is similar with Gst-nvinfer, see more details in Gst-nvinfer.Inputs and Outputs#This section summarizes the inputs, outputs, and communication facilities of the Gst-nvinferserver plugin.InputsGst BufferNvDsBatchMeta (attaching NvDsFrameMeta)Model repository directory path (model_repo.root)gRPC endpoint URL (grpc.url)Runtime model file with config.pbtxt file in model repositoryControl parametersGst-nvinferserver gets control parameters from a configuration file. You can specify this by setting the property config-file-path. For details, seeGst-nvinferserver Configuration File Specifications. Other control parameters that can be set through GObject properties are:Batch sizeProcess modeUnique idInference on GIE id and operate on class ids [secondary mode only]Inference intervalRaw output generated callback functionThe parameters set through the GObject properties override the parameters in the Gst-nvinferserver configuration file.OutputsGst BufferDepending on network type and configured parameters, one or more of:NvDsObjectMetaNvDsClassifierMetaNvDsInferSegmentationMetaNvDsInferTensorMetaGst-nvinferserver Configuration File Specifications#The Gst-nvinferserver configuration file uses prototxt format described inhttps://developers.google.com/protocol-buffers.The protobuf message structures of this configuration file are defined bynvdsinferserver_plugin.protoandnvdsinferserver_config.proto. All the basic data-type values are set to 0 or false from protobuf’s guide. Map, arrays and oneof are set to empty by default. See more details for each message definition.The message PluginControl innvdsinferserver_plugin.protois the entry point for this config-file.The message InferenceConfig configures the low-level settings forlibnvds_infer_server.The message PluginControl::InputControl configures the input buffers, objects filtering policy for model inference.The message PluginControl::OutputControl configures inference output policy for detections and raw tensor metadata.The message BackendParams configures backend input/output layers and Triton settings in InferenceConfig.The message PreProcessParams configures network preprocessing information in InferenceConfig.The message InputTensorFromMeta enables the preprocessed tensor input mode and configures the input tensor information in InferenceConfig.The message PostProcessParams configures the output tensor parsing methods such as detection, classification, semantic segmentation and others in InferenceConfig.There are also other messages (e.g. CustomLib, ExtraControl) and enum types (e.g. MediaFormat, TensorOrder, …) defined in the proto file for miscellaneous settings for InferenceConfig and PluginControl.Features#The following table summarizes the features of the plugin.Gst-nvinferserver plugin features#FeaturedGPUJetsonReleaseGst-nvinferserver Running on HostNoYesDS 5.0Running on Docker ImageYesYes(DS 6.0)DS 5.0DS Preprocessing: Network input format: RGB/BGR/GrayYesYesDS 5.0DS Preprocessing: Network input data types FP32/FP16/UINT8/INT8/UINT16/INT16/UINT32/INT32YesYesDS 5.0DS Preprocessing: Network input tensor orders
NCHW / NHWCYesYesDS 5.0Mem: Cuda(GPU) buf-sharing for Input TensorsYesYesDS 5.0Mem: Cuda Memory (GPU / CPU-pinned) for output tensorsYesYesDS 5.0Backend: TensorRT runtime (plan engine file)YesYesDS 5.0Backend: Tensorflow Runtime CPU/GPU (graphdef/savedmodel)YesYesDS 5.0Backend: Tensorflow Runtime with TF-TRT accelerationYesYesDS 5.0Backend: ONNX RuntimeYesYes(DS 6.0)DS 5.0Backend: ONNX Runtime with ONNX-TRT accelerationYesYes(DS 6.0)DS 5.0Backend: Pytorch RuntimeYesNoDS 5.0Postprocessing: DS Detection / Classification/ SegmentationYesYesDS 5.0Postprocessing: DS Detection cluster method: NMS / GroupRectangle / DBSCan / NoneYesYesDS 5.0Postprocessing: custom parsing (NvDsInferParseCustomTfSSD)YesYesDS 5.0Postprocessing: Triton native classificationYesYesDS 5.0Triton Ensemble Mode (Triton preproc/postproc) with specified media-format (RGB/BGR/Gray) with Cuda GPU buffer as inputsYesYesDS 5.0Postprocessing: Attach Triton raw tensor output in NvDsInferTensorMeta for downstream or application postprocessingYesYesDS 5.0deepstream-app: pipeline works with PGIE / SGIE / nvtrackerYesYesDS 5.0Sample App: deepstream-segmentation-test / deepstream-infer-tensor-meta-testYesYesDS 5.0Basic LSTM features on single batch and single stream (beta version, config file might be changed in future version)YesYesDS 5.0gRPC: Triton Server running as independent process and plugin communicates through gRPCYesYesDS 6.0Custom process interfaceIInferCustomProcessorfor extra multi-input tensors preprocess, multi-streams LSTM loop process, custom output data postprocess(parsing and metadata attaching). Basic single-stream LSTM could be replaced by custom loop processYesYesDS 6.0gRPC: CUDA buffer sharing with local Triton server for input tensorsYesYesDS 6.2Postprocessing: Clip object bounding boxes to fit within the ROIYesYesDS 6.3Gst-nvinferserver plugin features message PluginControl definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs(Primary/Secondary)infer_configLow-level libnvds_infer_server inference configuration settingsInferenceConfiginfer_config { … }
see details in InferenceConfigAllBothinput_controlControl plugin input buffers, objects filtering policy for inferencePluginControl ::InputControlinput_control{
process_mode: PROCESS_MODE_FULL_FRAME
}
see details in InputControlAllBothoutput_controlControl plugin output metadata filtering policy after inferencePluginControl ::OutputControloutput_control { … }
see details in OutputControlAllBothprocess_modeProcessing mode, selected from PluginControl::ProcessMode. In deepstream-app PGIE uses PROCESS_MODE_FULL_FRAME by default, SGIE use PROCESS_MODE_CLIP_OBJECTS by defaultenum PluginControl::ProcessModeprocess_mode: PROCESS_MODE_FULL_FRAMEAllBothoperate_on_gie_idUnique ID of the GIE on whose metadata (bounding boxes) this GIE is to operate onint32, >=0, valid gie-id.
-1, disable gie-id check, inference on all GIE Idsoperate_on_gie_id: 1AllSecondaryoperate_on_class_idsClass IDs of the parent GIE on which this GIE is to operate onComma delimited int32 arrayoperate_on_class_ids: [1, 2]
Operates on objects with class IDs 1, 2 generated by parent GIEAllSecondaryintervalSpecifies the number of consecutive, batches to be skipped for inference. default is 0uint32interval: 1AllPrimaryasync_modeEnables inference on detected objects and asynchronous metadata attachments. Works only when tracker-ids are attached. Pushes buffer downstream without waiting for inference results. Attaches metadata after the inferenceboolasync_mode: falseClassifierSecondaryobject_controlinput object filter settingsPluginControl::InputObjectControlobject_control {
bbox_filter {
min_width: 64
min_height: 64
}
}
see details in
InputObjectControlAllSecondaryGst-nvinferserver plugin message PluginControl-InputControl definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs(Primary/Secondary)process_modeProcessing mode, selected from PluginControl::ProcessMode. In deepstream-app PGIE uses PROCESS_MODE_FULL_FRAME by default, SGIE use PROCESS_MODE_CLIP_OBJECTS by defaultenum PluginControl::ProcessModeprocess_mode: PROCESS_MODE_FULL_FRAMEAllBothoperate_on_gie_idUnique ID of the GIE on whose metadata (bounding boxes) this GIE is to operate onint32, >=0, valid gie-id.
-1, disable gie-id check, inference on all GIE Idsoperate_on_gie_id: 1AllSecondaryoperate_on_class_idsClass IDs of the parent GIE on which this GIE is to operate onComma delimited int32 arrayoperate_on_class_ids: [1, 2]
Operates on objects with class IDs 1, 2 generated by parent GIEAllSecondaryintervalSpecifies the number of consecutive, batches to be skipped for inference. default is 0uint32interval: 1AllPrimaryasync_modeEnables inference on detected objects and asynchronous metadata attachments. Works only when tracker-ids are attached. Pushes buffer downstream without waiting for inference results. Attaches metadata after the inferenceboolasync_mode: falseClassifierSecondaryobject_controlinput object filter settingsPluginControl::InputObjectControlobject_control {
bbox_filter {
min_width: 64
min_height: 64
}
}
see details in
InputObjectControlAllSecondarysecondary_reinfer_intervalRe-inference interval for objects, in framesuint32secondary_reinfer_interval: 90AllSecondaryGst-nvinferserver plugin message PluginControl-OutputControl definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs(Primary/Secondary)output_tensor_metaEnable attaching Inference output tensor metadata, tensor buffer pointer for host onlybooloutput_tensor_meta: falseAllBothdetect_controlSpecifies detection output filter policyPluginControl::OutputDetectionControldetect_control {
default_filter {
bbox_filter {
min_width: 32
min_height: 32
}
}
}
see details in OutputDetectionControlDetectorBothclassifier_typeThe classifier type to be added in the NvDsClassifierMeta in case of classification networksstringclassifier_type: multi_class_classificationClassifierBothGst-nvinferserver plugin message PluginControl-InputObjectControl definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs(Primary/Secondary)bbox_filterBounding box filterPluginControl::BBoxFilterbbox_filter {
min_width: 32
min_height: 32
}
see details in BBoxFilterAllSecondaryGst-nvinferserver plugin message PluginControl-BBoxFilter definition details for Input and Output controls#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)min_widthBounding box minimum widthuint32min_width: 64AllBothmin_heightBounding box minimum heightuint32min_height: 64AllBothmax_widthBounding box maximum width, default 0, max_width is ignoreduint32max_width: 640AllBothmax_heightBounding box maximum height, 
default 0, max_height is ignoreduint32max_height: 640AllBothGst-nvinferserver plugin message PluginControl-OutputDetectionControl definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)default_filterdefault detection filter for output controlsPluginControl::DetectClassFilterdefault_filter {
bbox_filter {
min_width: 32
min_height: 32
}
}
see details in DetectClassFilterAllBothspecific_class_filtersspecifies detection filters per class to replace default filtermap<uint32, DetectClassFilter>specific_class_filters: [
{ key: 1, value {…} },
{ key: 2,
value {…} }
]AllBothGst-nvinferserver plugin message PluginControl-DetectClassFilter definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)bbox_filterdetection bounding box filterPluginControl::BBoxFilterbbox_filter {
min_width: 64
min_height: 64
}DetectionBothroi_top_offsetOffset of the RoI from the top of the frame. Only objects within the RoI are output.uint32roi_top_offset: 128DetectionBothroi_bottom_offsetOffset of the RoI from the bottom of the frame. Only objects within the RoI are output.uint32roi_bottom_offset:DetectionBothborder_colorspecify border color for detection bounding boxesPluginControl::Colorborder_color { 
r: 1.0
g: 0.0
b: 0.0
a: 1.0
}DetectionBothbg_colorspecify background color for detection bounding boxesPluginControl::Colorborder_color { 
r: 0.0
g: 1.0
b: 0.0
a: 0.5
}DetectionBothGst-nvinferserver plugin message PluginControl-Color definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)rRed color valuefloat
Range[0.0, 1.0]r: 0.5AllBothgGreen color valuefloat. Range[0.0, 1.0]g: 0.5AllBothbBlue color valuefloat. Range[0.0, 1.0]b: 0.3AllBothaAlpha blending valuefloat. Range[0.0, 1.0]a: 1.0AllBothLow Levellibnvds_infer_server.soConfiguration File Specifications#The message InferenceConfig defines all the low-level structure fields innvdsinferserver_config.proto. It has major settings for inference backend, network preprocessing and postprocessing.Gst-nvinferserver message InferenceConfig definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)unique_idUnique ID identifying metadata generated by this GIEuint32, ≥0unique_id: 1AllBothgpu_idsDevice IDs of GPU to use for pre-processing/inference (single GPU support only)int32 array, ≥0gpu_ids: [0]AllBothmax_batch_sizeMax number of frames/objects to be inferred together in a batchuint32, ≥0max_batch_size: 1AllBothbackendInference backend settingsBackendParamsbackend{trt_is{...}}see details in BackendParamsAllBothpreprocessingOne of preprocess or input_tensor_from_meta. Use preprocess if using frame or object processing mode and input_tensor_from_meta when using preprocessed tensor input modepreprocess or input_tensor_from_meta“N/A. Refer preprocess and input_tensor_from_meta below”AllBothpreprocessNetwork preprocessing settings for color conversion， scale and normalization applicable when using frame or object processing modePreProcessParamspreprocess{normalize{…}}see details in PreProcessParamsAllBothinput_tensor_from_metaConfiguration for the input tensor applicable when using preprocessed tensor as inputInputTensorFromMetainput_tensor_from_meta{is_first_dim_batch:true}see details in InputTensorFromMetaAllPreprocessed tensor input modepostprocessInference output tensor parsing methods such as detection, classification, semantic segmentation and othersPostProcessParamspostprocess{detection{...}}see details in PostProcessParamsAllBothcustom_libSpecify custom lib path for custom parsing functions and preloads, optionalCustomLibcustom_lib{path:./libcustom_parsing.so}AllBothextraextra controls for inference config.ExtraControlextra{output_buffer_pool_size:2}see details in ExtraControlAllBothlstmLSTM control parameters, limited on batch-size 1 and single streamLstmParams
[optional]``lstm {loops {input: “init_lstm_c”
output: “output/lstm_c”
init_const { value: 0 }}}``
See details in LstmParamsAllBothclip_object_outside_roiClip the object bounding boxes to fit within the specified ROI boundary.boolclip_object_outside_roi: falseDetectorBothGst-nvinferserver message BackendParams definition details#NameDescriptionType and RangeExample NotesNetwork Types/Applicable to GIEs (Primary/Secondary)inputsBackend input layer settings, optionalInputLayer arrayssee details in InputLayerAll/BothoutputsBackend output layer settings, optionalOutputLayer arrayssee details in OutputLayerAll/Bothtritonbackend of Triton Inference Server settingsTritonParamssee details in TritonParamsAll/Bothoutput_mem_typeTriton native output tensor memory typeMemoryType select from [MEMORY_TYPE_DEFAULT, MEMORY_TYPE_CPU, MEMORY_TYPE_GPU]output_mem_type: MEMORY_TYPE_CPUAll/BothGst-nvinferserver message InputLayer definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)nameinput tensor namestringname: “input_0”AllBothdimsinput tensor shape, optional. Only required if backend cannot figure out fixed input shapesint32 array,
> 0dims: [299, 299, 3]AllBothdata_typeenum TensorDataType with types:TENSOR_DT_NONE,TENSOR_DT_FP32,TENSOR_DT_FP16,TENSOR_DT_INT8,TENSOR_DT_INT16,TENSOR_DT_INT32,TENSOR_DT_UINT8,TENSOR_DT_UINT16,TENSOR_DT_UINT32Default TENSOR_DT_NONE,usually can be deduced from Triton model config.pbtxtTensorDataTypedata_type:
TENSOR_DT_FP32AllBothGst-nvinferserver message OutputLayer definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)nameoutput tensor namestringname: “detection_boxes”AllBothmax_buffer_bytesoutput tensor reserved buffer bytesuint64max_buffer_bytes: 2048AllBothGst-nvinferserver message TritonParams definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)model_nameTriton inference model namestringmodel_name: “ssd_inception_graphdef”AllBothversionTriton model version number.
-1, latest version number.
>0, reserved for specific version number in future versionint64version: -1AllBothmodel_repoTriton model repository settings.
Note, all model_repo settings must be same in single processTritonParams::TritonModelRepomodel_repo {
root: “../triton_model_repo”
log_level: 2
}
Refer the details in TritonModelRepoAllBothgrpcTriton gRPC server settings.TritonParams::TritonGrpcParamsgrpc {
url: “localhost:8001”
enable_cuda_buffer_sharing: false
}
Refer the details in TritonGrpcParamsAllBothGst-nvinferserver message TritonParams-TritonModelRepo definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)rootTriton inference model repository directory pathstringroot: “../triton_model_repo”AllBothlog_levelTriton log output levelsuint32;
0, ERROR;
1, WARNING;
2, INFO;
>=3, VERBOSE Levellog_level: 1AllBothstrict_model_configEnable Triton strict model configuration, see details in TritonGenerated Model Configuration. Suggest setting value trueboolstrict_model_config: trueAllBothtf_gpu_memory_fractionTensorFlow GPU memory fraction per process. Valid for Tensorflow models only.
Default 0 means no GPU memory limitation. Suggest tuning to a proper value (e.g. in range of [0.2, 0.6]) in case Tensorflow uses up whole GPU memoryfloat,
Range (0, 1.0]tf_gpu_memory_fraction: 0.6AllBothtf_disable_soft_placementDisable TensorFlow soft placement of operators. It’s enabled by default.booltf_disable_soft_placement: falseAllBothmin_compute_capacitySpecify minimal GPU compute capacity.
The default value is 6.0 on x86 and 5.0 on Jetson.min_compute_capacity: 6.0AllBothbackend_dirSpecify Triton backend directory which store Tensorflow/Onnx/Pytorch and custom backends.
The Default value  is /opt/tritonserver/backends on X86 and opt/nvidia/deepstream/deepstream-x.x/lib/triton_backends on Jetson.stringbackend_dir: /opt/tritonserver/backends/AllBothcuda_device_memorySpecify a list of CudaDeviceMem blocks with pre-allocated memory pool. Use Triton’s default value if list is empty.message listcuda_device_memory [ {device: 0
memory_pool_byte_size: 2000000000} ]AllBothCudaDeviceMem::deviceSpecify device IDuint32;
>= 0device: 0AllBothCudaDeviceMem::memory_pool_byte_sizeIndicate pre-allocated memory pool byte size on according device for Triton runtimeuint64;
>= 0memory_pool_byte_size: 8000000000AllBothpinned_memory_pool_byte_sizeIndicate pre-allocated Pinned memory on host for Triton runtime. Use Triton’s defult value (around 256MB) if not set.uint64;
>= 0pinned_memory_pool_byte_size: 128000000AllBothbackend_configsA list of BackendConfig blocks for Tritonserver backend config settingsmessage listbackend_configs [ {backend: tensorflow
setting: “allow-soft-placement”
value: “true”} ]AllBothBackendConfig::backendSpecify backend namestringbackend: tensorflowAllBothBackendConfig::settingSpecify backend setting namestringsetting: “allow-soft-placement”AllBothBackendConfig::valueSpecify backend setting valuesstringvalue: “true”AllBothGst-nvinferserver message TritonParams-TritonGrpcParams definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)urlTriton server name and portstringurl: “localhost:8001”AllBothenable_cuda_buffer_sharing“Enable sharing of CUDA buffers with local Triton server for input tensors.If enabledthe input CUDA buffers are shared with the Triton server to improve performance.This feature should be enabled only when the Triton server is on the same machine.Applicable for x86 dGPU platformnot supported on Jetson devices.By default disabled i.e. CUDA buffers are copied to system memory while creating the inference request.”Boolenable_cuda_buffer_sharing: trueAllBothGst-nvinferserver message PreProcessParams definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs(Primary/Secondary)network_formatenum MediaFormat with formats:
MEDIA_FORMAT_NONE
IMAGE_FORMAT_RGB
IMAGE_FORMAT_BGR
IMAGE_FORMAT_GRAY
. use IMAGE_FORMAT_RGB by default.MediaFormatnetwork_format: IMAGE_FORMAT_RGBAllBothtensor_orderenum TensorOrder with order types:
TENSOR_ORDER_NONE,
TENSOR_ORDER_LINEAR(this includes NCHW, CHW, DCHW, … orders),
TENSOR_ORDER_NHWC.
It can deduce the value from backend layers info if set to TENSOR_ORDER_NONETensorOrdertensor_order: TENSOR_ORDER_NONEAllBothtensor_nameSpecify the tensor name for the preprocessing buffer.
This is in the case when multiple input tensors in a single network.string;Optionaltensor_name: “input_0”AllBothframe_scaling_hwCompute hardware to use for scaling frames / object crops to network resolutionenum FrameScalingHW
FRAME_SCALING_HW_DEFAULT: Platform default – GPU (dGPU), VIC (Jetson)
FRAME_SCALING_HW_GPU
FRAME_SCALING_HW_VIC (Jetson only)frame_scaling_hw:  FRAME_SCALING_HW_GPUAllBothframe_scaling_filterThe filter to use for scaling frames / object crops to network resolutionint32, refer to enum NvBufSurfTransform_Inter in nvbufsurftransform.h for valid valuesframe_scaling_filter: 1AllBothmaintain_aspect_ratioIndicates whether to maintain aspect ratio while scaling input.int32;
0 or 1maintain_aspect_ratio: 0AllBothsymmetric_paddingIndicates whether to pad image symmetrically while scaling input. DeepStream pads the images asymmetrically by default.int32;
0 or 1symmetric_padding: 0AllBothnormalizeNetwork input tensor normalization settings for scale-factors, offsets and mean-subtractionPreProcessParams::ScaleNormalizenormalize {
scale_factor: 1.0
channel_offsets: [0, 0, 0]
}
see details in PreProcessParams::ScaleNormalizeGst-nvinferserver message PreProcessParams-ScaleNormalize definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)scale_factorPixel normalization factorfloatscale_factor: 0.0078AllBothchannel_offsetsArray of mean values of color components to be subtracted from each pixel. Array length must equal the number of color components in the frame. The plugin multiplies mean values by scale_factor.float array, Optionalchannel_offsets: [77.5, 21.2, 11.8]AllBothmean_filePathname of mean data file (PPM format)string;
Optionalmean_file: “./model_meanfile.ppm”AllBothGst-nvinferserver message InputTensorFromMeta definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs(Primary/Secondary)is_first_dim_batchBoolean indicating whether the preprocessed input tensor has first dimention as batch. Set true for batched input, false otherwise.Booleanis_first_dim_batch: trueAllPreprocessed tensor input modeGst-nvinferserver message PostProcessParams definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)labelfile_pathPathname of a text file containing the labels for the modelstringlabelfile_path: “=/home/ubuntu/model_labels.txt”AllBothoneof process_typeIndicates one of the postprocessing type
detection;
classification;
segmentation;
other;NoneN/AAllBothdetectionSpecify detection parameters for the network.
It must be oneof process_typeDetectionParamsdetection {
num_detected_classes: 4
simple_cluster {
threshold: 0.2
}
}
see details in DetectionParamsDetectorBothclassificationSpecify classification parameters for the network
It is oneof process_typeClassificationParamsclassification {
threshold: 0.6
}
see details in ClassificationParamsClassifierBothsegmentationSpecify semantic segmentation parameters for the network
It is oneof process_typeSegmentationParamssegmentation {
threshold: 0.2
num_segmentation_classes: 2
}SegmentationBothotherSpecify other network parameters.
This is for user-defined networks and usually coexists with output_control.output_tensor_meta: true. Tensor output data would be attached into GstBuffer. Data can be parsed in application. User can increase extra.output_buffer_pool_size if need to hold metadata longer.
It is oneof process_typeOtherNetworkParamsother {}
see details in OtherNetworkParamsOthersBothtriton_classificationSpecify Triton classification parameters for the network
It is oneof process_typeTritonClassifyParamsTriton_classification {
topk: 1
}
see details in TritonClassifyParamsClassifierBothGst-nvinferserver message DetectionParams definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)num_detected_classesDefine number of classes detected by the networkint32, > 0num_detected_classes:4DetectorBothper_class_paramsMap of specific detection parameters per class. Key-value follows <class_id: per_class_params> order.map<int32, PerClassParams>;
Optionalper_class_params [{ key: 1,
value { pre_threshold : 0.4}
},{ key: 2,
value { pre_threshold : 0.5}
}]
see details for PerClassParamsDetectorBothcustom_parse_bbox_funcName of the custom bounding box parsing function. If not specified, Gst-nvinferserver uses the internal function for the resnet model provided by the SDK.
If specified, also need to set custom_lib to load custom library.string;custom_parse_bbox_func: “NvDsInferParseCustomTfSSD”DetectorBothoneof clustering_policyIndicates one of the clustering policies from
nms;
dbscan;
group_rectangle;
simple_cluster;NoneN/ADetectorBothnmsIndicates clustering bounding boxes by Non-Maximum-Suppression method detected objects.
It is oneof clustering_policyNmsnms {
confidence_threshold: 0.3
iou_threshold: 0.4
}
see details in NmsDetectorBothdbscanIndicates clustering bounding boxes by DBSCAN method for detected objects.
It is oneof clustering_policyDbScandbscan {
pre_threshold: 0.3
eps: 0.7
min_boxes: 3
}
see details in DbScanDetectorBothgroup_rectangleIndicates clustering bounding boxes by groupRectangles() function for grouping detected objects
It is oneof clustering_policyGroupRectanglegroup_rectangle {
confidence_threshold: 0.2
group_threshold: 2
eps: 0.2
}DetectorBothsimple_clusterIndicates simple clustering method by outlier boxes through thresholdSimpleClustersimple_cluster {
threshold: 0.2
}DetectorBothGst-nvinferserver message DetectionParams-PerClassParams definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)pre_thresholdDefine confidence threshold per classfloatpre_threshold:0.3DetectorBothGst-nvinferserver message DetectionParams-Nms definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)confidence_thresholdDetection score lesser than this threshold would be rejectedfloatconfidence_threshold:0.5DetectorBothiou_thresholdMaximum IOU score between two proposals after which the proposal with the lower confidence will be rejected.floatiou_threshold: 0.3DetectorBothtopkSpecify top k detection results to keep after nmsint32, >= 0topk: 2;
value 0, means keep all.DetectorBothGst-nvinferserver message DetectionParams-DbScan definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)pre_thresholdDetection score lesser than this threshold would be rejected before DBSCAN clusteringfloatpre_threshold:0.2DetectorBothepsDBSCAN epsilon to control merging of overlapping boxes.floateps: 0.7DetectorBothmin_boxesMinimum boxes in DBSCAN cluster to be considered an objectint32, > 0min_boxes: 3;DetectorBothmin_scoreMinimum score in DBSCAN cluster for it to be considered as an objectfloatmin_score: 0.7Default value is 0DetectorBothGst-nvinferserver message DetectionParams-GroupRectangle definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)confidence_thresholdDetection score lesser than this threshold would be rejectedfloatconfidence_threshold:0.2DetectorBothgroup_thresholdThreshold value for rectangle merging for OpenCV grouprectangles() functionint32; >= 0group_threshold: 1DetectorBothepsEpsilon to control merging of overlapping boxesfloateps: 0.2DetectorBothGst-nvinferserver message DetectionParams-SimpleCluster definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)thresholdDetection score lesser than this threshold would be rejectedfloatconfidence_threshold:0.6DetectorBothGst-nvinferserver message ClassificationParams definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)thresholdClassification score lesser than this threshold would be rejectedfloatthreshold: 0.5ClassifierBothcustom_parse_classifier_funcName of the custom classifier output parsing function.
If not specified, Gst-nvinfer uses the internal parsing function with NCHW tensor order for softmax layers. User can reshape other output tensor order to NCHW in Triton config.pbtxt to run internal parsing.
If specified, also need to set custom_lib to load custom library.stringparse-classifier-func-name: “parse_bbox_softmax”ClassifierBothGst-nvinferserver message SegmentationParams definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)thresholdSegmentation score lesser than this threshold would be rejectedfloatthreshold: 0.5SegmentationBothnum_segmentation_classesNumber of output classes for the segmentation networkint32, >0num_segmentation_classes: 2SegmentationBothcustom_parse_segmentation_funcName of the custom segmentation output parsing function. If not specified, Gst-nvinferserver uses the internal function for the UNet model provided by the SDK. If specified, users also need to set custom_lib to load custom library.stringcustom_parse_segmentation_func: “NvDsInferParseCustomPeopleSemSegNet”SegmentationBothGst-nvinferserver message OtherNetworkParams definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)type_nameSpecify a user-defined network namestring; | Optionaltype_name: “face”OthersBothGst-nvinferserver message TritonClassifyParams definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)topkSpecify top k elements need to keep from Triton’s native classificationuint32; >=0topk : 1
Value 0 or empty would keep the top 1 result.ClassifierBoththresholdClassification score lesser than this threshold would be rejectedfloatthreshold: 0.5ClassifierBothGst-nvinferserver message CustomLib definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)pathPathname that points to a custom library for preloadstringpath: “/home/ubuntu/lib_custom_impl.so”AllBothGst-nvinferserver message ExtraControl definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs(Primary/Secondary)copy_input_to_host_buffersEnable to copy input. If enabled, input tensor would be attached as NvDsInferTensorMeta into GstBuffer with output tensors together tensor data to host buffers.boolcopy_input_to_host_buffers: falseAllBothoutput_buffer_pool_sizeSpecify the buffer pool size for each output tensor.
When infer_config.postprocess.other is specified or output_control.output_tensor_meta is enabled, the output tensor would be attached as NvDsInferTensorMeta into GstBufferint32;
Range [2, 10]output_buffer_pool_size: 4AllBothcustom_process_funcioncustom function to create a specific user-defined processor IInferCustomProcessor.
The function symbol is loaded by infer_config.custom_libStringpath: “libnvdsinfer_custom_impl_fasterRCNN.so”AllBothNoteLstmParams structures may be changed in future versionsGst-nvinferserver message LstmParams definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)loopsSpecify LSTM loops between input and output tensors.LstmLoop
[repeated]loops [ {input: “init_state”
output: “out_state”} ]
See details in LstmParams::LstmLoopAllBothNoteInput and output tensors must have same datatype/dimensions, FP16 is not supportedLstmParams::LstmLoopstructures might be changed in future versionsGst-nvinferserver message LstmParams-LstmLoop definition details#NameDescriptionType and RangeExample NotesNetwork Types /Applicable to GIEs (Primary/Secondary)inputSpecify input tensor name of the current loop.stringInput: “init_state”AllBothoutputSpecify input tensor name of the current loop. Tensor data will feedback to the input tensorstringonput: “output_state”AllBothinit_constSpecify the constant values for the input in first frameInitConst | value: floatInit_const { value: 0 }AllBothGst Properties#The values set through Gst properties override the values of properties in the configuration file. The application does this for certain properties that it needs to set programmatically. If user set property though plugin, these values would replace the original value in config files. The following table describes the Gst-nvinferserver plugin’s Gst properties.Gst-nvinferserver plugin Gst properties#PropertyMeaningType and RangeExample Notesconfig-file-pathAbsolute pathname of configuration file for the Gst-nvinferserver elementStringconfig-file-path=config_infer_primary.txtprocess-modeInfer Processing Mode
(0):None, (1)FullFrame, (2)ClipObject.
If set, it could replace input_control.process_modeInteger, 0, 1 or 2process-mode=1unique-idUnique ID identifying metadata generated by this GIE.
If set, it could replace infer_config.unique_idInteger,
0 to 4,294,967,295unique-id=1infer-on-gie-idSee input_control.operate_on_gie_id in the configuration file tableInteger,
0 to 4,294,967,295infer-on-gie-id=1operate-on-class-idsSee input_control.operate_on_class_ids in the configuration file tableAn array of colon- separated integers (class-ids)operate-on-class-ids=1:2:4batch-sizeNumber of frames/objects to be inferred together in a batch.
If set, it could replace infer_config.max_batch_sizeInteger,
1 – 4,294,967,295batch-size=4IntervalNumber of consecutive batches to be skipped for inference
If set, it could replace input_control.intervalInteger, 0 to 32interval=0raw-output-generated-callbackPointer to the raw output generated callback functionPointerCannot be set through gst-launchraw-output-generated-userdataPointer to user data to be supplied with raw-output-generated-callbackPointerCannot be set through gst-launchDeepStream Triton samples#DeepStream Triton samples are located in the foldersamples/configs/deepstream-app-triton. In terms of Triton model specification, all related models and Triton config files(config.pbtxt) must be gathered into same root directory which
issamples/triton_model_repo. Follow the instructions insamples/configs/deepstream-app-triton/READMEto run the samples.DeepStream Triton gRPC support#In addition to native Triton server, gst-nvinferserver supports the Triton Inference Server running as independent process. Communication to the server happens through gRPC.
Config files to run the application in gRPC mode are located atsamples/config/deepstream-app-triton-grpc. Follow the instructions insamples/configs/deepstream-app-triton-grpc/READMEto run the samples.Triton Ensemble Models#The Gst-nvinferserver plugin can support Triton ensemble models for further custom preprocessing, backend and postprocessing through Triton custom-backends.
Triton ensemble model represents a pipeline of one or more models and the connection of input and output tensors between those models, such as“datapreprocessing->inference->datapostprocessing”. See more detailstriton-inference-server/server.
To manage memory efficiency and keep clean interface, The Gst-nvinferserver Plugin’s default preprocessing cannot be disabled. Color conversion, datatype conversion, input scaling and object cropping are continue working innvds_infer_servernatively. For example, in the case native normalization is not needed, update scale_factor to 1.0:infer_config { preprocess {
network_format: IMAGE_FORMAT_RGB
tensor_order: TENSOR_ORDER_LINEAR
normalize { scale_factor: 1.0 } } }The low levelnvds_infer_serverlibrary could deliver specified media-format (RGB/BGR/Gray) in any kind of tensor orders and datatypes as a Cuda GPU buffer input to Triton backend. User’s custom-backend must support GPU memory on this input. Triton custom-backend sample identity can work with Gst-nvinferserver plugin.NoteCustom backend API must have same Triton codebase version (24.08). Read more details from Triton server releasetriton-inference-server/serverTo learn details how to implement Triton custom-backend, please refer totriton-inference-server/backendFor Triton model’s output, TRTSERVER_MEMORY_GPU and TRTSERVER_MEMORY_CPU buffer allocation are supported innvds_infer_serveraccording to Triton output request. This also works for ensemble model’s final output tensors.
Finally, inference data can be parsed by default for detection, classification, or semantic segmentation. Alternatively, user can implement custom-backend for postprocessing, then deliver the final output to Gst-nvinferserver plugin to do further processing. Besides that, User can also optionally attach raw tensor output data into metadata for downstream or application to parse.Custom Process interfaceIInferCustomProcessorfor Extra Input, LSTM Loop, Output Tensor Postprocess#Gst-nvinferserver plugin supports extra(multiple) input tensors custom preprocessing, input / output tensor custom loop processing (LSTM-based) with multiple streams, output tensor data custom parsing and attaching into NvDsBatchMeta. This custom function is loaded though gst-nvinferserver’s config file:infer_config {
  backend {
    triton {
      model_name: "yolov3-10_onnx"
      # option 1: for CAPI inference
      # model_repo { root: "./model_repo" }
      # option 2: for gRPC inference
      # grpc { url: "localhost:8001" }
    }
    # specify output tensor memory type, MEMORY_TYPE_CPU/MEMORY_TYPE_GPU
    output_mem_type: MEMORY_TYPE_CPU
  }
  preprocess { ... } # specify scale and normalization
  # postprocess{ other{} } # skip generic postprocess

  # specify custom processing library
  custom_lib {
     path: "/path/to/libnvdsinferserver_custom_process.so"
  }
  extra {
    # specify custom processing function entrypoint from custom_lib
    custom_process_funcion: "CreateInferServerCustomProcess"
  }
}The interfaceIInferCustomProcessoris defined insources/includes/nvdsinferserver/infer_custom_process.h.classIInferCustomProcessor{virtualvoidsupportInputMemType(InferMemType&type);// return supported memory type for `extraInputs`virtualboolrequireInferLoop()const;// indicate whether LSTM loop is needed. return 'false' if not needed.// custom implementation for extra input tensors processing, `primaryInputs` is processed by preprocess{} from config file.// param `options` is helpful to carry extra information such as stream_ids, `NvBufSurface`, `NvDsBatchMeta`, `GstBuffer`virtualNvDsInferStatusextraInputProcess(constvector<IBatchBuffer*>&primaryInputs,vector<IBatchBuffer*>&extraInputs,constIOptions*options)=0;// param `outputs` is a array of all batched output tensors. param `inOptions` is same as extraInputProcessvirtualNvDsInferStatusinferenceDone(constIBatchArray*outputs,constIOptions*inOptions)=0;virtualvoidnotifyError(NvDsInferStatusstatus)=0;};Users need derive fromIInferCustomProcessorto implement their own extra preprocessing throughextraInputProcessand fully postprocessing throughinferenceDone. the param structred inIOptionscarry all information from GstBuffer and NvDsBatchMeta. Users can query them throughIOptionsfor each frame and batch.
see more examples in/opt/nvidia/deepstream/deepstream/sources/TritonOnnxYolo/nvdsinferserver_custom_impl_yolo/nvdsinferserver_custom_process_yolo.cppTake a example for a simple postprocessing to add output tensors.#include<inttypes.h>#include<unistd.h>#include<cassert>#include<unordered_map>#include"infer_custom_process.h"#include"nvbufsurface.h"#include"nvdsmeta.h"typedefstruct_GstBufferGstBuffer;usingnamespacenvdsinferserver;#defin INFER_ASSERT assertclassNvInferServerCustomProcess:publicIInferCustomProcessor{// memtype for ``extraInputs``, set ``kGpuCuda`` for performancevoidsupportInputMemType(InferMemType&type)override{type=InferMemType::kGpuCuda;}// for LSTM loop. return false if not required.boolrequireInferLoop()constoverride{returnfalse;}// skip extraInputProcess if there is no extra input tensorsNvDsInferStatusextraInputProcess(conststd::vector<IBatchBuffer*>&primaryInputs,std::vector<IBatchBuffer*>&extraInputs,constIOptions*options)override{returnNVDSINFER_SUCCESS;}// output tensor postprocessing function.NvDsInferStatusinferenceDone(constIBatchArray*outputs,constIOptions*inOptions)override{GstBuffer*gstBuf=nullptr;std::vector<uint64_t>streamIds;NvDsBatchMeta*batchMeta=nullptr;std::vector<NvDsFrameMeta*>frameMetaList;NvBufSurface*bufSurf=nullptr;std::vector<NvBufSurfaceParams*>surfParamsList;int64_tunique_id=0;INFER_ASSERT(inOptions->getValueArray(OPTION_NVDS_SREAM_IDS,streamIds)==NVDSINFER_SUCCESS);INFER_ASSERT(inOptions->getObj(OPTION_NVDS_BUF_SURFACE,bufSurf)==NVDSINFER_SUCCESS);INFER_ASSERT(inOptions->getObj(OPTION_NVDS_BATCH_META,batchMeta)==NVDSINFER_SUCCESS);INFER_ASSERT(inOptions->getInt(OPTION_NVDS_UNIQUE_ID,unique_id)==NVDSINFER_SUCCESS);INFER_ASSERT(inOptions->getValueArray(OPTION_NVDS_BUF_SURFACE_PARAMS_LIST,surfParamsList)==NVDSINFER_SUCCESS);INFER_ASSERT(inOptions->getValueArray(OPTION_NVDS_FRAME_META_LIST,frameMetaList)==NVDSINFER_SUCCESS);uint64_tnsTimestamp=UINT64_MAX;// nano-secondsif(inOptions->hasValue(OPTION_TIMESTAMP)){INFER_ASSERT(inOptions->getUInt(OPTION_TIMESTAMP,nsTimestamp)==NVDSINFER_SUCCESS);}std::unordered_map<std::string,SharedIBatchBuffer>tensors;for(uint32_ti=0;i<outputs->getSize();++i){SharedIBatchBufferoutTensor=outputs->getSafeBuf(i);INFER_ASSERT(outTensor);autodesc=outTensor->getBufDesc();tensors.emplace(desc.name,outTensor);}// parsing output tensorsfloat*boxesPtr=(float*)tensors["output_bbox"]->getBufPtr(0);auto&bboxDesc=tensors["output_bbox"]->getBufDesc();float*scoresPtr=(float*)tensors["output_score"]->getBufPtr(0);float*numPtr=(float*)tensors["output_bbox_num"]->getBufPtr(0);int32_tbatchSize=bboxDesc.dims.d[0];// e.g. tensor shape [Batch, num, 4]std::vector<std::vector<NvDsInferObjectDetectionInfo>>batchedObjs(batchSize);// parsing data into batchedObjs...// attach to NvDsBatchMetafor(intiB=0;iB<batchSize;++iB){constauto&objs=batchedObjs[iB];for(constauto&obj:objs){NvDsObjectMeta*objMeta=nvds_acquire_obj_meta_from_pool(batchMeta);objMeta->unique_component_id=unique_id;objMeta->confidence=obj.detectionConfidence;objMeta->class_id=obj.classId;objMeta->rect_params.left=obj.left;objMeta->rect_params.top=obj.top;objMeta->rect_params.width=obj.width;objMeta->rect_params.height=obj.height;// other settings...// add NvDsObjectMeta obj into NvDsFrameMeta frame.nvds_add_obj_meta_to_frame(frameMetaList[iB],objMeta,NULL);}}}};extern"C"{IInferCustomProcessor*CreateInferServerCustomProcess(constchar*config,uint32_tconfigLen){returnnewNvInferServerCustomProcess();}}For extra input tensors preprocess: If the model requires multiple tensor inputs more than the primary image input, Users can derive from this interfaceIInferCustomProcessorand implementextraInputProcess()to process extra inputs tensors. This function is for extra input process only. the parameterIOptions* optionswould carry all the information from GstBuffer, NvDsBatchMeta, NvDsFrameMeta, NvDsObjectMeta and so on. User can leverage all of the information fromoptionsto fill the extra input tensors. All of the input tensor memory is allocated by nvdsinferserver low-level lib.For output tensor postprocess(parsing and metadata attaching): If user want to do custom parsing on output tensors into user metadata and attach them into GstBuffer, NvDsBatchMeta, NvDsFrameMeta or NvDsObjectMeta. User can implement ‘inferenceDone(outputs, inOptions)’ to parse all output tensors inoutputs, and get above GstBuffer, NvDsBatchMeta and other DeepStream information frominOptions. Then attach the parsed user metadata into NvDs metadata. This function supports multiple-streams parsing and attaching. See examples in/opt/nvidia/deepstream/deepstream/sources/TritonOnnxYolo/nvdsinferserver_custom_impl_yolo/nvdsinferserver_custom_process_yolo.cpp:NvInferServerCustomProcess::inferenceDone()how to parse and attach output metadata.NoteIf user need specific memory type(e.g. CPU) for output tensors ininferenceDone(). Update config file.infer_config { backend {
output_mem_type: MEMORY_TYPE_CPU
} }For multi-stream custom loop process: If the model is LSTM based, and next frame’s inputs are generated by previous frame’s output data. User can derive interfaceIInferCustomProcessor, then implementextraInputProcess()andinferenceDone()for loop process.extraInputProcess()could initialize first input tensor states. Then‘inferenceDone()’can get the output data and do post processing and store the result into the context. When next‘extraInputProcess()’is coming, it can check the stored results and feedback into tensor states. When user overrideboolrequireInferLoop()const{returntrue;}. The nvdsinferver low-level lib shall keep theextraInputProcessandinferenceDonerunning in sequence along with its nvds_stream_ids which could be get fromoptions->getValueArray(OPTION_NVDS_SREAM_IDS,streamIds). see examples and details in/opt/nvidia/deepstream/deepstream/sources/TritonOnnxYolo/nvdsinferserver_custom_impl_yolo/nvdsinferserver_custom_process_yolo.cpp. Inside this example, see functionNvInferServerCustomProcess::feedbackStreamInputhow to feedback output into next input loop.Tensor Metadata Output for Downstream Plugins#The Gst-nvinferserver plugin can attach raw output tensor data generated by the inference backend as metadata. It is added as an NvDsInferTensorMeta in theframe_user_meta_listmember of NvDsFrameMeta for primary (full frame) mode, or in the obj_user_meta_list member of NvDsObjectMeta for secondary (object) mode. It uses same metadata structure with Gst-nvinferserver plugin.NoteGst-nvinferserver plugin does not attach device buffer pointerNvDsInferTensorMeta::attachout_buf_ptrs_devat this moment.To read or parse inference raw tensor data of output layers#Enable the following fields in the configuration file for the Gst-nvinferserver plugin:output_control { output_tensor_meta : true }If native postprocessing need be disabled, update:infer_config { postprocess { other {} } }When operating as primary GIE,NvDsInferTensorMetais attached to each frame’s (each NvDsFrameMeta object’s)frame_user_meta_list. When operating as secondary GIE, NvDsInferTensorMeta is attached to each NvDsObjectMeta object’sobj_user_meta_list.Metadata attached by Gst-nvinferserver can be accessed in a GStreamer pad probe attached downstream from the Gst-nvinferserver instance.TheNvDsInferTensorMetaobject’s metadata type is set to NVDSINFER_TENSOR_OUTPUT_META. To get this metadata you must iterate over the NvDsUserMeta user metadata objects in the list referenced byframe_user_meta_listor obj_user_meta_list.For more information about Gst-infer tensor metadata usage, see the source code insources/apps/sample_apps/deepstream_infer_tensor_meta-test.cpp, provided in the DeepStream SDK samples.Segmentation Metadata#The Gst-nvinferserver plugin attaches the output of the semantic segmentation model as user metadata in an instance of NvDsInferSegmentationMeta with meta_type set to NVDSINFER_SEGMENTATION_META. The user metadata is added to theframe_user_meta_listmember of NvDsFrameMeta for primary (full frame) mode, or the obj_user_meta_list member of NvDsObjectMeta for secondary (object) mode. For guidance on how to access user metadata, see User/Custom Metadata Addition Inside NvDsMatchMeta and Tensor Metadata, above.previousGst-nvinfernextGst-nvtrackerOn this pageInputs and OutputsGst-nvinferserver Configuration File SpecificationsFeaturesLow Levellibnvds_infer_server.soConfiguration File SpecificationsGst PropertiesDeepStream Triton samplesDeepStream Triton gRPC supportTriton Ensemble ModelsCustom Process interfaceIInferCustomProcessorfor Extra Input, LSTM Loop, Output Tensor PostprocessTensor Metadata Output for Downstream PluginsTo read or parse inference raw tensor data of output layersSegmentation MetadataPrivacy Policy|Manage My Privacy|Do Not Sell or Share My Data|Terms of Service|Accessibility|Corporate Policies|Product Security|ContactCopyright © 2024-2025, NVIDIA Corporation.Last updated on Jan 13, 2025.