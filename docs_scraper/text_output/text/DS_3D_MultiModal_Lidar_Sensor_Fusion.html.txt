DeepStream-3D Sensor Fusion Multi-Modal Application and Framework — DeepStream documentationSkip to main contentBack to topCtrl+KDeepStream documentationDeepStream documentationTable of ContentsDeepStream Getting StartedWelcome to the DeepStream DocumentationMigration GuideInstallationQuickstart GuideDocker ContainersDeepStream SamplesC/C++ Sample Apps Source DetailsPython Sample Apps and Bindings Source DetailsDeepStream Reference Application - deepstream-appDeepStream Reference Application - deepstream-test5 appDeepStream Reference Application - deepstream-nmos appDeepStream Reference Application on GitHubSample Configurations and StreamsImplementing a Custom GStreamer Plugin with OpenCV Integration ExampleTAO toolkit Integration with DeepStreamTAO Toolkit Integration with DeepStreamTutorials and How-to'sDeepStream-3D Custom Apps and Libs TutorialsDeepStream PerformancePerformanceDeepStream AccuracyAccuracy Tuning ToolsDeepStream Custom ModelUsing a Custom Model with DeepStreamDeepStream Key FeaturesDeepStream-3D Sensor Fusion Multi-Modal Application and FrameworkDeepStream-3D Multi-Modal BEVFusion SetupDeepStream-3D Multi-Modal V2XFusion SetupSmart Video RecordIoTOn the Fly Model UpdateNTP Timestamp in DeepStreamAV Sync in DeepStreamDeepStream With REST API SeverDeepStream 3D Action Recognition AppDeepStream 3D Depth Camera AppDeepStream 3D Lidar Inference AppNetworked Media Open Specifications (NMOS) in DeepStreamGst-nvdspostprocess in DeepStreamDeepStream Can Orientation AppDeepStream Application MigrationApplication Migration to DeepStream 7.1 from DeepStream 7.0DeepStream Plugin GuideGStreamer Plugin OverviewMetaData in the DeepStream SDKGst-nvdspreprocess (Alpha)Gst-nvinferGst-nvinferserverGst-nvtrackerGst-nvstreammuxGst-nvstreammux NewGst-nvstreamdemuxGst-nvmultistreamtilerGst-nvdsosdGst-nvdsmetautilsGst-nvdsvideotemplateGst-nvdsaudiotemplateGst-nvvideoconvertGst-nvdewarperGst-nvofGst-nvofvisualGst-nvsegvisualGst-nvvideo4linux2Gst-nvjpegdecGst-nvimagedecGst-nvjpegencGst-nvimageencGst-nvmsgconvGst-nvmsgbrokerGst-nvdsanalyticsGst-nvdsudpsrcGst-nvdsudpsinkGst-nvdspostprocess (Alpha)Gst-nvds3dfilterGst-nvds3dbridgeGst-nvds3dmixerGst-NvDsUcxGst-nvdsxferGst-nvvideotestsrcGst-nvmultiurisrcbinGst-nvurisrcbinDeepStream Troubleshooting and FAQTroubleshootingFrequently Asked QuestionsDeepStream On WSL2DeepStream On WSLFAQ for Deepstream On WSLDeepStream API GuideDeepStream API GuidesDeepStream Service MakerWhat is Deepstream Service MakerService Maker for C/C++ DevelopersService Maker for Python Developers(alpha)Quick Start GuideIntroduction to Flow APIsIntroduction to Pipeline APIsAdvanced FeaturesMigrating Traditional Deepstream Apps to Service Maker Apps in PythonWhat is a Deepstream Service Maker PluginDeepstream LibrariesDeepStream Libraries (Developer Preview)Graph ComposerOverviewPlatformsSupported platformsGetting StartedApplication Development WorkflowCreating an AI ApplicationReference graphsExtension Development WorkflowDeveloping Extensions for DeepStreamDeepStream ComponentsGXF InternalsGXF InternalsGraph eXecution EngineGraph Execution EngineGraph Composer ContainersGraph Composer and GXF ContainersGXF Component InterfacesGXF Component InterfacesGXF Application API'sGXF App C++ APIsGXF App Python APIsGXF Runtime API'sGXF Core C++ APIsGXF Core C APIsGXF Core Python APIsExtension ManualExtensionsCudaExtensionGXF Stream SyncStandardExtensionPython CodeletsNetworkExtensionNvTritonExtSerializationExtensionMultimediaExtensionVideoEncoderExtensionVideoDecoderExtensionBehavior TreesUCX ExtensionHttpExtensionGrpcExtensionTensorRTExtensionNvDs3dProcessingExtNvDsActionRecognitionExtNvDsAnalyticsExtNvDsBaseExtNvDsCloudMsgExtNvDsConverterExtNvDsDewarperExtNvDsInferenceExtNvDsInferenceUtilsExtNvDsInterfaceExtNvDsMuxDemuxExtNvDsOpticalFlowExtNvDsOutputSinkExtNvDsSampleExtNvDsSampleModelsExtNvDsSourceExtNvDsTemplateExtNvDsTrackerExtNvDsTranscodeExtNvDsTritonExtNvDsUcxExtNvDsUdpExtNvDsVisualizationExtToolsRegistryRegistry Command Line InterfaceComposerContainer BuilderGXF Command Line InterfacePipetuner GuideFAQ GuideFAQDeepStream Legal InformationDeepStream End User License AgreementDeepStream FeedbackFeedback formDeepStream-3...DeepStream-3D Sensor Fusion Multi-Modal Application and Framework#Thisdeepstream-3d-lidar-sensor-fusionsample application showcases multi-modal sensor fusion pipelines for LiDAR and camera data using the DS3D framework. This application with DS3D framework could setup different LiDAR/RADAR/Camera sensor fusion models, late fusion inference pipelines with several key features.Camera processing pipeline leveraging DeepStream’s generic 2D video pipeline with batchMeta.Customds3d::dataloaderfor LiDAR capture with pre-processing options.Custom ds3d::databridge converts DeepStreamNvBufSurfaceandGstNvDsPreProcessBatchMetadata into shaped based tensor datads3d::Frame2DGuardandds3d::FrameGuardformats, and embeds key-value pairs withinds3d::datamap.ds3d::mixerfor efficient merging of camera, LiDAR and any sensor data intods3d::datamap.ds3d::datatfilerfollowed bylibnvds_tritoninferfilter.sofor multi-modalds3d::datamapinference and custom pre/post-processing.ds3d::datasinkwithds3d_gles_ensemble_renderfor 3D detection result visualization with a multi-view display.Thedeepstream-3d-lidar-sensor-fusionsample application and source code is located atapp/sample_apps/deepstream-3d-lidar-sensor-fusion/for your reference.There are 2 multi-modal sensor fusion pipelines for LiDAR and camera data, enabling 3D detections.Example 1. BEVFusion Multi-Modal with 6-Camera Plus 1-LiDAR Data Fusion Pipeline#Refer to the provided instructions for the setup.DS3D BEVFusion Setup with TritonProcesses data from 6 cameras and 1 LiDAR.Utilizes pre-trainedPyTroch BEVFusionmodel, optimized for NVIDIA GPUs using TensorRT and CUDA byCUDA-BEVFusion.PyTritonmulti-modal inference module (triton-lmm) simplifies Python model integration, allowing inclusion of any Python inference.Theds3d::datatfilerbased triton inference through gRPC.Visualizes theds3d::datamapthrough 6 camera views, projecting LiDAR data into each. Additionally, it provides a top view and a front view of the same LiDAR data for easier comprehension.Example 2. V2XFusion multi-modal batched 4-Camera and 4-LiDAR Inference Pipeline:#Refer to the provided instructions for the setup.DS3D V2XFusion setupProcesses data from a single camera and a LiDAR, utilizing a batch size of 4.Utilizes pre-trained V2XFusion model which is based on BEVFusion and BEVHeight.Build the V2X ONNX model into TensorRT for GPU acceleration.Visualizes 4 batched camera and lidar data together into multiviews.Quick Start#The following development packages must be installed.GStreamer-1.0GStreamer-1.0 Base PluginsGLES librarylibyaml-cpp-devDownload and installDeepStream SDKlocally on the host.
Follow instructions at pageInstall the DeepStream SDKwith method 1 or 2 to install DeepStream SDK locally.BEVFusion requires a local installation of DeepStream SDK which includes the scripts to build/run the container, model and dataset for ease of use.Prerequisites before starting the container.# run cmdline outside of the container$exportDISPLAY=0.0# set the correct display number if DISPLAY is not exported$xhost+
$cd/opt/nvidia/deepstream/deepstream/sources/apps/sample_apps/deepstream-3d-lidar-sensor-fusion# make directory for dataset and model repo in host, it would be mounted into the container for bevfusion tests$sudochmod-Ra+rw.# Grant read/write permission for all of the files in this folder$mkdir-pdatabevfusion/model_rootIf any scripts are run outside of container, or if file read/write permission errors are experienced, please run the commands withsudo-E.NoteUsers have to run the following commandline on every terminal outside of the container or seeing errors such asxhost:unabletoopendisplay$exportDISPLAY=0.0# set the correct display number if DISPLAY is not exported$xhost+Bevfusion and V2XFusion Setup difference.BEVFusion would build a new local docker imagedeepstream-triton-bevfusion:{xx.xx}on top of deepstream-triton base imagenvcr.io/nvidia/deepstream:{xx.xx.xx}-triton-multiarchto install all CUDA-BEVFusion dependencies, build offline models, and setup triton server for gRPC remote inference on x86 dGPU. and client fusion pipeline could be running on x86 and Jetson.V2XFusion setup instructions inside deepstream-triton base containernvcr.io/nvidia/deepstream:{xx.xx.xx}-triton-multiarchand inference through Triton CAPI(native) locally. It also supports Jetson test on host without container if Triton dependencies installed manually.deepstream-triton-bevfusion:{xx.xx},{xx.xx}is from deepstream sdk major.minor version number installed on local.nvcr.io/nvidia/deepstream:{xx.xx.xx}-triton-multiarch,{xx.xx.xx}is matched fromDeepStream X86 containersandDeepStream Jetson containersBEVFusion pipeline Demo Setup#Prepare all required containers, inference models, sample dataset.
Refer to the detailed provided instructions inDS3D BEVFusion setupNoteAll the following commandline for BEVFusion setup are run outside of the container unless other comments specifiedBEVFusion pipeline Quick start.Option 1:  build bevfusion model container, start triton server and finally run the pipeline.Run the following commandline to build a local bevfusion model containerdeepstream-triton-bevfusion:{DS_VERSION_NUM}on x86 with dGPU.$cd/opt/nvidia/deepstream/deepstream/sources/apps/sample_apps/deepstream-3d-lidar-sensor-fusion
$bevfusion/docker_build_bevfusion_image.shnvcr.io/nvidia/deepstream:{xx.xx.xx}-triton-multiarchRun the following commandline to download the model and build TensorRT engine files on x86 with dGPU.$mkdir-pbevfusion/model_root
$bevfusion/docker_run_generate_trt_engine_models.shbevfusion/model_rootStart triton server with the models built from last step on x86 with dGPU.$bevfusion/docker_run_triton_server_bevfusion.shbevfusion/model_rootOpen another terminal to start deepstream 3d sensor fusion pipeline with bevfusion config on x86.$exportNUSCENE_DATASET_URL="https://github.com/NVIDIA-AI-IOT/deepstream_reference_apps/raw/DS_7.1/deepstream-3d-sensor-fusion/data/nuscene.tar.gz"$cd/opt/nvidia/deepstream/deepstream/sources/apps/sample_apps/deepstream-3d-lidar-sensor-fusion# this cmdline is also preparing the dataset for pipeline tests.# this config file would project lidar data back to camera view in display.$bevfusion/docker_run_ds3d_sensor_fusion_bevfusion_pipeline.shds3d_lidar_plus_multi_cam_bev_fusion.yaml# OR# this config file would keep clear lidar and camera data in display, meanwhile show lables in each view.$bevfusion/docker_run_ds3d_sensor_fusion_bevfusion_pipeline.shds3d_lidar_plus_multi_cam_bev_fusion_with_label.yamlSee more details about the instructions inDS3D BEVFusion setupOption 2: Once users setup everything ready from Option 1, and keep tritonserver running, and make sure dataset downloaded, then users can run the cmdline inside deepstream-triton container.Start deepstream triton container after model and dataset are ready in Option 1.exportDOCKER_GPU_ARG="--runtimenvidia"for Jetson. see more details how to modify config file to setup Jetson testDS3D BEVFusion setup$cd/opt/nvidia/deepstream/deepstream/sources/apps/sample_apps/deepstream-3d-lidar-sensor-fusion# make directory for dataset and model repo in host, it would be mounted into the container for bevfusion tests$mkdir-pdatabevfusion/model_root

$exportDOCKER_GPU_ARG="--gpus all"# for x86# export DOCKER_GPU_ARG="--runtime nvidia" # for Jetson# start the container interactively, and mount dataset and model folder into the container for tests$dockerrun$DOCKER_GPU_ARG-it--rm--ipc=host--net=host--privileged-v/tmp/.X11-unix:/tmp/.X11-unix\-eDISPLAY=$DISPLAY\-w/opt/nvidia/deepstream/deepstream/sources/apps/sample_apps/deepstream-3d-lidar-sensor-fusion\-v./data:/opt/nvidia/deepstream/deepstream/sources/apps/sample_apps/deepstream-3d-lidar-sensor-fusion/data\-v./bevfusion/model_root:/opt/nvidia/deepstream/deepstream/sources/apps/sample_apps/deepstream-3d-lidar-sensor-fusion/bevfusion/model_root\nvcr.io/nvidia/deepstream:{xx.xx.xx}-triton-multiarchStart deepstream bevfusion pipeline, run cmdline inside of the container.# run cmdline inside of this container$cd/opt/nvidia/deepstream/deepstream/sources/apps/sample_apps/deepstream-3d-lidar-sensor-fusion
$deepstream-3d-lidar-sensor-fusion-cds3d_lidar_plus_multi_cam_bev_fusion.yaml# Or render with the 3D-bbox labels.$deepstream-3d-lidar-sensor-fusion-cds3d_lidar_plus_multi_cam_bev_fusion_with_label.yamlBEVFusion Pipeline rendering results with nuscene dataset(nuscene dataset terms of use <https://www.nuscenes.org/terms-of-use> )V2XFusion pipeline Demo Setup#Refer to the detailed provided instructions inDS3D V2XFusion SetupStart the deepstream-triton base container for V2XFusion tests.Skip this step if users have installed Triton dependencies manually on Jetson host.# running cmdline outside of the container$xhost+# export DOCKER_GPU_ARG="--runtime nvidia --privileged" # for Jetson Orin$exportDOCKER_GPU_ARG="--gpus all"# for x86# start the container interactively$dockerrun$DOCKER_GPU_ARG-it--rm--ipc=host--net=host-v/tmp/.X11-unix:/tmp/.X11-unix\-eDISPLAY=$DISPLAY\-w/opt/nvidia/deepstream/deepstream/sources/apps/sample_apps/deepstream-3d-lidar-sensor-fusion\nvcr.io/nvidia/deepstream:{xx.xx.xx}-triton-multiarch# {xx.xx.xx} is deepstream sdk version numberWith thisdockerruncontainer, the following instructions of v2XFusion setup are running inside of this container.
If this step skipped on Jetson, the following instructions are running on host directly.Install dependencies$pipinstallgdownpython-lzf# with sudo if running on Jetson hostPrepare all required inference models, optimize the models and sample datasetFollow instructions inDownload V2XFusion Models and Build TensorRT Engine Filesto download the original V2X dataset.Note: The example dataset is provided byhttps://thudair.baai.ac.cn/coop-forecast. For each dataset an user elects to use, the user is responsible for checking if the dataset license is fit for the intended purpose.$cd/opt/nvidia/deepstream/deepstream/sources/apps/sample_apps/deepstream-3d-lidar-sensor-fusion/v2xfusion/scripts
$gdown1gjOmGEBMcipvDzu2zOrO9ex_OscUZMYY
$./prepare.shdataset#withsudoifrunningonJetsonhostStart V2XFusion pipeline once models and dataset are ready.# run the cmdline inside deepstream-triton container$cd/opt/nvidia/deepstream/deepstream/sources/apps/sample_apps/deepstream-3d-lidar-sensor-fusion
$deepstream-3d-lidar-sensor-fusion-cds3d_lidar_video_sensor_v2x_fusion.ymlUsers could see the pipeline running on display.Build application From Source#To compile the sample app deepstream-3d-lidar-sensor-fusion inside of container:$cd/opt/nvidia/deepstream/deepstream/sources/apps/sample_apps/deepstream-3d-lidar-sensor-fusion
$make
$sudomakeinstall(sudonotrequiredinthecaseofdockercontainers)NoteTo compile the sources, run make withsudo-Eor root permission.DS3D Components used in this sample application#This section describes the DS3D components used in the deepstream-3d-lidar-sensor-fusion pipeline.LiDAR Data Loading#ds3d::dataloader(implemented inlibnvds_lidarfileread.so)Reads a list of LiDAR point cloud files from disk into ads3d::datamapformat.Source code resides in/opt/nvidia/deepstream/deepstream/sources/libs/ds3d/dataloader/lidarsource.Refer to the README within that directory for compilation and installation instructions.See more details in DS3D Custom lib specificationCustom Dataloader libnvds_lidarfileread Configuration SpecificationsVideo Data Bridging Into DS3Dds3d::datamap#ds3d::databridge(implemented inlibnvds_3d_video_databridge.so)Transfers 2D video buffers into theds3d::datamapformat.DeepStream expects the 2D buffer to be invideo/x-raw(memory:NVMM)format (e.g., output fromnvv4l2decoder).See more details in DS3D databridge specificationConfiguration fileNote: Ads3d::datamapis a generic data structure consisting of key-value pairs. It serves as the primary input and output buffer format for components within the DeepStream ds3d framework.Data Mixing#ds3d::datamixer(implemented inlibnvds_3d_multisensor_mixer.so)Combines video data (2D) and LiDAR data (3D) into a singleds3d::datamap.The mixer operates at a user-specified frame rate. The processing speed might be limited by the slowest input source.See more details in DS3D mixer specificationConfiguration fileLiDAR/Camera Data Alignment/Calibration Filtering#ds3d::datafilter(implemented inlibnvds_3d_alignment_datafilter.so)Applies a series of transformations to align the LiDAR data with the camera image coordinate system.See more details in DS3D Custom lib specificationCustom ds3d::datafilter library: libnvds_3d_alignment_datafilter.soLiDAR Data V2X Preprocess Filtering#ds3d::datafilter(implemented inlibnvds_3d_lidar_preprocess_datafilter.so)Preprocess lidar data for v2x sensor fusion model.See more details in DS3D Custom lib specificationCustom Datafilter libnvds_3d_lidar_preprocess_datafilter Specificationsds3d custom point cloud data to point pillar scatter data conversion (implemented inlibnvds_3d_lidar_preprocess_datafilter.so)Implement the V2XFusion model pointpillar scatter data conversion function to adapt to ds3d lidar preprocess ds3d::datafilterRefer to the /opt/nvidia/deepstream/deepstream/sources/libs/ds3d/datafilter/lidar_preprocess/READMELiDAR/Camera Data GLES Rendering#ds3d::datarender(implemented inlibnvds_3d_gles_ensemble_render.so)Renders a 3D scene using OpenGL ES (GLES) with various elements (textures, LiDAR points, bounding boxes) within a single window, allowing for flexible layout customization in multi-view mode.See more details in alignment specificationCustom datarender libnvds_3d_gles_ensemble_render Configuration SpecificationsData Inference Filtering#ds3d::datafilter(implemented inlibnvds_tritoninferfilter.so)Executes multi-modal data inference using the Triton Inference Server. Any data element from theds3d::datamapcan be forwarded to Triton. It supports both Triton CAPI and gRPC modes. Custom pre-processing and post-processing might be required depending on the specific inference task.See more details inlibnvds_tritoninferfilter Configuration Specificationsds3d custom V2XFusion model inputs preprocessing library (implemented inlibnvds_3d_v2x_infer_custom_preprocess.so)Prepare and copy constant parameters and data for the tensor inputs of the V2XFusion modelCopy pointpillar scatter data to the model input tensorRefer to /opt/nvidia/deepstream/deepstream/sources/libs/ds3d/inference_custom_lib/ds3d_v2x_infer_custom_preprocess/READMECustom Post-Processing for LiDAR Detection#ds3d custom postprocessing library (implemented inlibnvds_3d_infer_postprocess_lidar_detection.so)Performs custom post-processing operations on the sensor fusion results (3D detection objects). The interface inherits fromnvdsinferserver::IInferCustomProcessor.Source code resides in/opt/nvidia/deepstream/deepstream/sources/libs/ds3d/inference_custom_lib/ds3d_lidar_detection_postprocess.Refer to the README within that directory for compilation and installation instructions.ds3d custom V2XFusion outputs postprocessing library (implemented inlibnvds_3d_v2x_infer_custom_postprocess.so)Parse the output tensor data from V2XFusion modelCalculate 3D bboxes from the output tensor dataRefer to /opt/nvidia/deepstream/deepstream/sources/libs/ds3d/inference_custom_lib/ds3d_v2x_infer_custom_postprocess/READMEBEVFusion Model Inference with Triton-LMM#triton_lmm Python module for bevfusionA Python module based on Triton and PyTriton, designed for multi-modal inference. It simplifies the integration of Python-based inference models into the Triton server. This sample application leverages the BEVFusion model (Python version) using this module.source code resides inapp/sample_apps/deepstream-3d-lidar-sensor-fusion/python/triton_lmmPython module license Apache-2.0DS3D Custom Components Configuration Specifications#See more details in theDS_3D supported custom components specificationssection in theDeepStream-3D Custom Apps and Libs Tutorials.previousUsing a Custom Model with DeepStreamnextDeepStream-3D Multi-Modal BEVFusion SetupOn this pageExample 1. BEVFusion Multi-Modal with 6-Camera Plus 1-LiDAR Data Fusion PipelineExample 2. V2XFusion multi-modal batched 4-Camera and 4-LiDAR Inference Pipeline:Quick StartBEVFusion pipeline Demo SetupV2XFusion pipeline Demo SetupBuild application From SourceDS3D Components used in this sample applicationLiDAR Data LoadingVideo Data Bridging Into DS3Dds3d::datamapData MixingLiDAR/Camera Data Alignment/Calibration FilteringLiDAR Data V2X Preprocess FilteringLiDAR/Camera Data GLES RenderingData Inference FilteringCustom Post-Processing for LiDAR DetectionBEVFusion Model Inference with Triton-LMMDS3D Custom Components Configuration SpecificationsPrivacy Policy|Manage My Privacy|Do Not Sell or Share My Data|Terms of Service|Accessibility|Corporate Policies|Product Security|ContactCopyright © 2024-2025, NVIDIA Corporation.Last updated on Jan 13, 2025.