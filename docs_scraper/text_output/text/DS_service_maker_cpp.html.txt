Service Maker for C/C++ Developers — DeepStream documentationSkip to main contentBack to topCtrl+KDeepStream documentationDeepStream documentationTable of ContentsDeepStream Getting StartedWelcome to the DeepStream DocumentationMigration GuideInstallationQuickstart GuideDocker ContainersDeepStream SamplesC/C++ Sample Apps Source DetailsPython Sample Apps and Bindings Source DetailsDeepStream Reference Application - deepstream-appDeepStream Reference Application - deepstream-test5 appDeepStream Reference Application - deepstream-nmos appDeepStream Reference Application on GitHubSample Configurations and StreamsImplementing a Custom GStreamer Plugin with OpenCV Integration ExampleTAO toolkit Integration with DeepStreamTAO Toolkit Integration with DeepStreamTutorials and How-to'sDeepStream-3D Custom Apps and Libs TutorialsDeepStream PerformancePerformanceDeepStream AccuracyAccuracy Tuning ToolsDeepStream Custom ModelUsing a Custom Model with DeepStreamDeepStream Key FeaturesDeepStream-3D Sensor Fusion Multi-Modal Application and FrameworkDeepStream-3D Multi-Modal BEVFusion SetupDeepStream-3D Multi-Modal V2XFusion SetupSmart Video RecordIoTOn the Fly Model UpdateNTP Timestamp in DeepStreamAV Sync in DeepStreamDeepStream With REST API SeverDeepStream 3D Action Recognition AppDeepStream 3D Depth Camera AppDeepStream 3D Lidar Inference AppNetworked Media Open Specifications (NMOS) in DeepStreamGst-nvdspostprocess in DeepStreamDeepStream Can Orientation AppDeepStream Application MigrationApplication Migration to DeepStream 7.1 from DeepStream 7.0DeepStream Plugin GuideGStreamer Plugin OverviewMetaData in the DeepStream SDKGst-nvdspreprocess (Alpha)Gst-nvinferGst-nvinferserverGst-nvtrackerGst-nvstreammuxGst-nvstreammux NewGst-nvstreamdemuxGst-nvmultistreamtilerGst-nvdsosdGst-nvdsmetautilsGst-nvdsvideotemplateGst-nvdsaudiotemplateGst-nvvideoconvertGst-nvdewarperGst-nvofGst-nvofvisualGst-nvsegvisualGst-nvvideo4linux2Gst-nvjpegdecGst-nvimagedecGst-nvjpegencGst-nvimageencGst-nvmsgconvGst-nvmsgbrokerGst-nvdsanalyticsGst-nvdsudpsrcGst-nvdsudpsinkGst-nvdspostprocess (Alpha)Gst-nvds3dfilterGst-nvds3dbridgeGst-nvds3dmixerGst-NvDsUcxGst-nvdsxferGst-nvvideotestsrcGst-nvmultiurisrcbinGst-nvurisrcbinDeepStream Troubleshooting and FAQTroubleshootingFrequently Asked QuestionsDeepStream On WSL2DeepStream On WSLFAQ for Deepstream On WSLDeepStream API GuideDeepStream API GuidesDeepStream Service MakerWhat is Deepstream Service MakerService Maker for C/C++ DevelopersService Maker for Python Developers(alpha)Quick Start GuideIntroduction to Flow APIsIntroduction to Pipeline APIsAdvanced FeaturesMigrating Traditional Deepstream Apps to Service Maker Apps in PythonWhat is a Deepstream Service Maker PluginDeepstream LibrariesDeepStream Libraries (Developer Preview)Graph ComposerOverviewPlatformsSupported platformsGetting StartedApplication Development WorkflowCreating an AI ApplicationReference graphsExtension Development WorkflowDeveloping Extensions for DeepStreamDeepStream ComponentsGXF InternalsGXF InternalsGraph eXecution EngineGraph Execution EngineGraph Composer ContainersGraph Composer and GXF ContainersGXF Component InterfacesGXF Component InterfacesGXF Application API'sGXF App C++ APIsGXF App Python APIsGXF Runtime API'sGXF Core C++ APIsGXF Core C APIsGXF Core Python APIsExtension ManualExtensionsCudaExtensionGXF Stream SyncStandardExtensionPython CodeletsNetworkExtensionNvTritonExtSerializationExtensionMultimediaExtensionVideoEncoderExtensionVideoDecoderExtensionBehavior TreesUCX ExtensionHttpExtensionGrpcExtensionTensorRTExtensionNvDs3dProcessingExtNvDsActionRecognitionExtNvDsAnalyticsExtNvDsBaseExtNvDsCloudMsgExtNvDsConverterExtNvDsDewarperExtNvDsInferenceExtNvDsInferenceUtilsExtNvDsInterfaceExtNvDsMuxDemuxExtNvDsOpticalFlowExtNvDsOutputSinkExtNvDsSampleExtNvDsSampleModelsExtNvDsSourceExtNvDsTemplateExtNvDsTrackerExtNvDsTranscodeExtNvDsTritonExtNvDsUcxExtNvDsUdpExtNvDsVisualizationExtToolsRegistryRegistry Command Line InterfaceComposerContainer BuilderGXF Command Line InterfacePipetuner GuideFAQ GuideFAQDeepStream Legal InformationDeepStream End User License AgreementDeepStream FeedbackFeedback formService...Service Maker for C/C++ Developers#Getting Started with Service Maker C++ APIs#Before starting on the actual application, we need to create a text file with name ‘CMakeLists.txt’ for building our application later:cmake_minimum_required(VERSION3.16)project(Sample)find_package(nvds_service_makerREQUIREDPATHS/opt/nvidia/deepstream/deepstream/service-maker/cmake)add_executable(my-deepstream-appmy_deepstream_app.cpp)target_link_libraries(my-deepstream-appPRIVATEnvds_service_maker)Now we can create a sample deepstream app (my_deepstream_app.cpp) with a pipeline to perform object detection from a incoming video stream:#include"pipeline.hpp"#include<iostream>usingnamespacedeepstream;#define CONFIG_FILE_PATH "/opt/nvidia/deepstream/deepstream/samples/configs/deepstream-app/config_infer_primary.yml"intmain(intargc,char*argv[]){try{Pipelinepipeline("sample-pipeline");pipeline.add("nvurisrcbin","src","uri",argv[1]).add("nvstreammux","mux","batch-size",1,"width",1280,"height",720).add("nvinferbin","infer","config-file-path",CONFIG_FILE_PATH).add("nvosdbin","osd").add("nveglglessink","sink").link({"src","mux"},{"","sink_%u"}).link("mux","infer","osd","sink");pipeline.start().wait();}catch(conststd::exception&e){std::cerr<<e.what()<<std::endl;return-1;}return0;}In order to build the application, we should use the CMake to create build environment:$ mkdir build && cd build && cmake .. && makeOnce the build is complete, we can run the application to start the object detection pipeline:$ ./my-deepstream-app file:///opt/nvidia/deepstream/deepstream/samples/streams/sample_720p.mp4Currently, despite the pipeline running, we aren’t obtaining object information from the application. To access this data, we need to create a buffer probe that iterates through the  metadata generated by the nvinfer plugin.A sample metadata probe can be implemented as follows:classObjectCounter:publicBufferProbe::IBatchMetadataObserver{public:ObjectCounter(){}virtual~ObjectCounter(){}virtualprobeReturnhandleData(BufferProbe&probe,constBatchMetadata&data){data.iterate([](constFrameMetadata&frame_data){autovehicle_count=0;frame_data.iterate([&](constObjectMetadata&object_data){autoclass_id=object_data.classId();if(class_id==0){vehicle_count++;}});std::cout<<"Object Counter: "<<" Pad Idx = "<<frame_data.padIndex()<<" Frame Number = "<<frame_data.frameNum()<<" Vehicle Count = "<<vehicle_count<<std::endl;});returnprobeReturn::Probe_Ok;}};By attaching the buffer probe into the inference plugin within the existing pipeline before starting it, we’ll get the vehicle count information from each frame of the video stream:pipeline.attach("infer",newBufferProbe("counter",newObjectCounter));Rebuild the application and run it again, vehicle count will be printed out.......
Object Counter:  Pad Idx = 0 Frame Number = 132 Vehicle Count = 7
Object Counter:  Pad Idx = 0 Frame Number = 133 Vehicle Count = 8
Object Counter:  Pad Idx = 0 Frame Number = 134 Vehicle Count = 7
Object Counter:  Pad Idx = 0 Frame Number = 135 Vehicle Count = 8
Object Counter:  Pad Idx = 0 Frame Number = 136 Vehicle Count = 9
Object Counter:  Pad Idx = 0 Frame Number = 137 Vehicle Count = 7
Object Counter:  Pad Idx = 0 Frame Number = 138 Vehicle Count = 9
Object Counter:  Pad Idx = 0 Frame Number = 139 Vehicle Count = 9
Object Counter:  Pad Idx = 0 Frame Number = 140 Vehicle Count = 9
Object Counter:  Pad Idx = 0 Frame Number = 141 Vehicle Count = 11
.....Application Developer Fundamentals#Pipeline#A pipeline serves as the foundation of Deepstream-based AI streaming applications. Media streams flow through interconnected functional blocks within a pipeline, processed via buffers widths metadata. A pipeline and the elements within it autonomously manage states and data flow, reducing the need for external intervention by application developers.A functional pipeline requires appropriate elements from Deepstream plugins to be added, configured and linked correctly. This can be seamlessly achieved using Pipeline APIs in a fluent manner:Pipelinepipeline("sample");// nvstreammux is the factory name in Deepstream to create a streammuxer Element// mux is the name of the Element instance// multiple key value pairs can be appended to configure the added Elementpipeline.add("nvstreammux","mux","batch-size",1,"width",1280,"height",720);For elements that have single static input and output, links can be established in a very straightforward manner:pipeline.link("mux","infer","osd","sink");However, if an element supports dynamic or multiple input/output, extra information is required to establish the link:// link the Element named "src" to the one named "mux"// given the input of a nvstreammux is dynamic, we must specify the input name "sink_%u" (refer to the plugin documentation)pipelin.link({"src","mux"},{"","sink_%u"});Pipeline construction can also be achieved through a declarative YAML configuration file, significantly reducing coding efforts in application development. The above pipeline can be defined as follows in YAML configuration.deepstream:nodes:-type:nvurisrcbinname:src-type:nvstreammuxname:muxproperties:batch-size:1width:1280height:720-type:nvinferbinname:inferproperties:config-file-path:/opt/nvidia/deepstream/deepstream/samples/configs/deepstream-app/config_infer_primary.yml-type:nvosdbinname:osd-type:nveglglessinkname:sinkedges:src:muxmux:inferinfer:osdosd:sinkAnd with the YAML configuration being applied, the application source can be simplified a lot:#include"pipeline.hpp"#include<iostream>usingnamespacedeepstream;intmain(intargc,char*argv[]){try{Pipelinepipeline("my-pipeline","config.yaml");pipeline["src"].set("uri",argv[1]);pipeline.start().wait();}catch(conststd::exception&e){std::cerr<<e.what()<<std::endl;return-1;}return0;}The YAML configuration of a pipeline begins with a “deepstream” keyword, and is composed of two sections:Node definition list under “nodes”: each item defines an instance to be added to the pipeline, with “type”, “name” and “properties” specified. The “type” field corresponds to the element name defined in DS plugins, such as “nvstreammux”; “name” is a string to identify the instance within the pipeline and must be unique; “properties” field initialize the supported properties for the instance.Link definition list under “edge”: each item defines one or more connections, where the key specifies the source and the value specifies the target(s). In the case that a source has more than one output, the output name should also be specified, such as “source.vsrc_%u: mux”.Guideline for Fine-tuning a Pipeline through Configuration File#The YAML configuration file also offers users a straightforward option to finely tune the pipeline, enabling them to optimize performance effortlessly.Users can always start with a pipeline design from the blueprint with appropriate nodes and edges,  tailored precisely to suit the specific objectives of their projects. Before proceeding, it’s essential to address the following parameters:batch-size: this is a important property that could affects the performance. It specifies the total number of frames for streammux to create a batch, it must equal to the maximum streams expected to be fed into the pipeline. It is an inherent parameter of “streammux”, while in the case of “nvmultiurisrcbin”, “max-batch-size” should be set instead.batched-push-timeout: this is another property for “streammux” or “nvmultiurisrcbin”. It determines how long streammux will wait until all the frames are collected, given the batch-size. Change in batched-push-timeout affects the framerates, especially when the input is live source, e.g RTSP.buffer-pool-size: this property defines the number of buffer pool used by streammux. In scenarios where the processing latency is prolonged to the extent that the buffer pool depletes before a buffer is returned, adjustments to certain properties may be necessary. Nonetheless, it’s imperative to ensure that these adjustments do not surpass a limit of 16.In the majority of cases, performance degradation stems from hardware bottlenecks, which may arise from the CPU, GPU, or other components. To identify bottlenecks, users can modify the type of potentially problematic nodes to “identity,” effectively disabling them, and then retest the performance iteratively until the actual bottleneck is pinpointed.Another factor contributing to performance degradation occurs when the pipeline lacks full parallelization, often resulting from irregular overloading of certain nodes. In such instances, inserting a queue before the problematic node can provide a solution.Additionally, unnecessary scaling can lead to wasteful computation. Therefore, it’s beneficial to carefully consider the processing dimensions for each node. Ensuring that the processing dimension aligns with either the input/output dimension or model dimension can effectively minimize scaling.Elements and Plugins#The Deepstream SDK introduces Element as the fundamental functional block accessible through various plugins. Each type of element offers specific capabilities, including encoding, decoding, pre-processing, post-processing, AI inference, and Object Tracking. When integrated into a pipeline, an element initiates the processing of media buffers flowing through the pipeline, generating metadata or messages as a result.Elements are highly configurable, supporting customization through key-value pairs or an external configuration file. This flexibility enables them to adapt to diverse task requirements. The output of an element, comprising data buffers, metadata, and messages, can be seamlessly captured by downstream counterparts for continuous processing. Alternatively, it can be retrieved by the application for external processing through customized objects.An element can be instantiated using its predefined type name within the Deepstream SDK:Elementstreammux("nvstreammux","mux").set("batch-size",1,"width",1280,"height",720);A more practical approach supports instantiating and configuring an element through the higher-level Pipeline API:pipeline.add("nvstreammux","mux","batch-size",1,"width",1280,"height",720);To properly direct the flow of media, elements need to be sequentially linked within the Pipeline. In most cases, a straightforward linking operation can be achieved using a simple link method, with the names of each Element instance specified in order. However, in certain scenarios, additional information may be required to facilitate the linking process. This complexity can arise from variations in how Elements handle inputs and outputs.pipeline.add("nvurisrcbin","src").add("nvvideoconvert","converter");// only the video output of the media source should be linked to the video converter// "vsrc_%u" specify the video output of a nvuirsrcbin instance (refer to the plugin documentation)pipeline.link({"src","converter"},{"vsrc_%u",""});Buffer#The Buffer object serves as a wrapper for the data segments flowing through the pipeline.For a read only buffer, you can invoke the read method to access the data in read-only mode, employing a customized callable object for data processing. Similarly, for a read-write buffer, the process is analogous, with the added capability of modifying the data within the buffer.// example of a video buffer observer interfaceclassSampleBufferObserver:publicBufferProbe::IBufferObserver{public:virtualprobeReturnhandleBuffer(BufferProbe&probe,constBuffer&buffer){// cast of a general buffer to a video buffer allows to tread the data in proper video formatVideoBuffervideo_buffer=buffer;video_buffer.read([byte](constvoid*data,size_tsize)->size_t{constunsignedchar*p_byte=(constunsignedchar*)data;for(autop=p_byte;p<p_byte+size;p++){// take a peek on the data}returnsize;});}};Metadata#In addition to the output data from an element, users may have a keen interest in the metadata generated by the element. This becomes particularly relevant in AI inference pipelines, where inference results are commonly conveyed in metadata format.The following is a list of currently supported metadata objects:Metadata classDescriptionMetaDatabase class of all the metadataBatchMetaDatametadata created through nvstreammux, acting as the root of all the other metadata and supporting iterating FrameMetaData and DisplayMetadata through the batchFrameMetaDatametadata associated with a specific video frame, iterable within the BatchMetaDataUserMetaDatauser defined metadataObjectMetadatametadata that describes a detected object, iterable with the FrameMetaData.ClassifierMetadatametadata that contains the classification information of an object, iterable within the ObjectMetadata.DisplayMetadatametadata that describes the display properties, being used by the nvdsosd to draw various shapes and text on a video frame, iterable with the FrameMetaData.EventMessageUserMetadataa specific user metadata for generating an event message, once appended to the FrameMetaData, it will be consumed by downstream message converter.More details about Deepstream Metadata can be foundhere.SourceConfig#SourceConfig is a convenience tool to load source configurations from a YAML file, allowing users append extra information to a source and create a list for multiple sources. A sample source config looks as followssource-list:-uri:"file:///opt/nvidia/deepstream/deepstream/samples/streams/sample_1080p_h264.mp4"sensor-id:UniqueSensorId1sensor-name:UniqueSensorName1-uri:"file:///opt/nvidia/deepstream/deepstream/samples/streams/sample_1080p_h265.mp4"sensor-id:UniqueSensorId2sensor-name:UniqueSensorName2source-config:source-bin:"nvmultiurisrcbin"properties:ip-address:"localhost"port:7000batched-push-timeout:33000live-source:truewidth:1920height:1080file-loop:truemax-batch-size:16There are 2 sections in the YAML file: source-list and source-config, each defines the inputs and the configurations of the source bin(s):source-list: each item defines the uri, sensor-id and sensor-name for a sourcesource-config: defines the type of the source node and corresponding properties. The property specification under “properties” MUST be consistent with the type of the source node.When the number of the streams fed into the pipeline is big and variable during runtime, separating the source config from the pipeline definition gives more clarity, however, it is always the developers’ decision for the most suitable approach.CameraConfig#CameraConfig is a convenience tool to load camera configurations from a YAML file, allowing users to create a list of multiple camera sources (V4L2/CSI). A sample camera config looks as follows:camera-list:-camera-type:"CSI"camera-csi-sensor-id:0camera-width:1280camera-height:720camera-fps-n:30camera-fps-d:1-camera-type:"V4L2"camera-v4l2-dev-node:2camera-width:640camera-height:480camera-fps-n:30camera-fps-d:1There is only one section in the YAML file: camera-list which defines the camera source and its configuration:camera-list: each item defines camera-type, camera-width, camera-height, camera-fps-n, camera-fps-d, for CSI camera source: camera-csi-sensor-id and for V4L2 camera source: camera-v4l2-dev-node.CommonFactory and custom Objects#Application developers can utilize custom objects to access the processing results of specific Deepstream Elements tailored to their unique needs. These custom objects can be instantiated directly through the ‘new’ keyword if they are defined within the application code. Alternatively, if implemented through a plugin, they can be created through the common factory interface.Custom objects can be incorporated into the pipeline using the ‘attach’ method, requiring the name of the element to which they should be attached.NoteUpon attachment, the designated element will assume control of the lifecycle of the custom object.The following is a list of currently supported custom objects:BufferProbe#Application developers can employ a buffer probe to retrieve the output buffer generated within the pipeline. Various interfaces can be implemented based on the specific requirements of the probe, offering flexibility for different purposes.Interface nameMethoddescriptionIBufferObserverhandleBufferaccess each processed buffer in read only modeIBufferOperatorhandleBufferaccess each processed buffer in read write modeIBatchMetadataObserverhandleMetadataaccess the batch metadata in read only modeIBatchMetadataOperatorhandleMetadataaccess the batch metadata in read write modeSignalHandler#Signaling is a vital mechanism that facilitates interaction between elements  within a running pipeline and the external world, notifying observers of specific events. An element can support a variety of signals, each uniquely registered during instantiation. The comprehensive details regarding signal support for an element can be found in the corresponding plugin manual.Application developers have the flexibility to enhance the functionality of their applications by attaching signal handlers to Element instances. Once attached, these handlers respond promptly to the emitted signals, allowing developers to implement custom logic in response to specific events.DataFeeder#The DataFeeder signal handler is specifically designed for attachment to an Element instance to capture data request related signals, such as “need-data” and “enough-data” signals from an “appsrc.” By using DataFeeder, application developers gain the ability to seamlessly inject data directly into the target Element during runtime.DataReceiver#The DataReceiver signal handler is specifically for attachment to an Element instance to capture data ready related signals, such as “new-sample” signal from an “appsink.” By using DataFeeder, By using DataReceiver, application developers can receive the processed data from the target Element during runtime.SignalEmitter#The signal emitter is another pivotal component in the signaling mechanism, responsible for emitting signals rather than handling them. When an element seeks to have certain activities driven by the external environment, it accepts signal emitters. In this mechanism, with a signal emitter attached to an Element instance that is configured to receive such signals,  corresponding activity will be triggered on the Element instance when the emit method is invoked on the signal emitter object.Prepare and Activate#For specific use cases, we have introduced alternative APIs to manage pipeline execution:Pipeline.prepare()#Unlike pipeline.start(), which transitions the pipeline’s state to playing in a new thread, pipeline.prepare() sets the pipeline’s state to paused within the same thread. This is particularly useful in multi-pipeline scenarios where each pipeline’s state needs to be changed to paused sequentially.Pipeline.activate()#After using pipeline.prepare(), pipeline.activate() transitions the pipeline’s state from paused to playing in a new thread. This effectively starts the pipeline. In multi-pipeline scenarios, this allows all pipelines to begin execution concurrently.Pipeline.wait()#This API is used to join the threads created by pipeline.activate(), ensuring that all threads complete their execution before proceeding.The Sample Test 5 Application demonstrates how to utilize these API calls effectively.State Transitions in Service Maker Pipelines#Managing state transitions in service pipelines is essential for ensuring efficient and orderly operations.
Service maker facilitates synchronous state transitions, enabling smooth and predictable changes in the pipeline’s status. Here’s a clearer and more structured explanation of these transitions:Start#Transition: From idle to running.Process: Utilize the pipeline.start() API to initiate the pipeline, moving it from an idle state to actively running tasks.Prepare and Activate#Transition: From idle to paused, then to running.Process: By executing pipeline.prepare() followed by pipeline.activate(), the pipeline transitions through a paused state before becoming fully operational. This sequence allows for any necessary setup or configuration prior to execution.Wait#Purpose: Suspend the current thread until the pipeline completes its execution.Process: The pipeline.wait() function is required after both the start and prepare/activate sequences to ensure that the current thread pauses until the pipeline has finished processing.Stop#Transition: Back to idle.Process: The pipeline.stop() API is used to return the pipeline to its idle state, effectively halting operations and resetting it for future tasks.These transitions are designed to manage workflow efficiently, allowing for both immediate execution and necessary preparatory phases.Sample Applications#Sample applications can be found from the Deepstream installation directory/opt/nvidia/deepstream/deepstream/service-maker/sources/apps, and with the following commands user can build them:$ cmake /opt/nvidia/deepstream/deepstream/service-maker/sources/apps/deepstream_test1_app && makeMigrating Traditional Deepstream Apps to Service Maker#For users who have been integrating Deepstream applications into their solutions for an extended period, migrating to the new Service Maker APIs requires minimal effort.Traditional Deepstream applications rely on configuration files for pipeline customization and parameter setup. The Service Maker APIs follow a similar approach, offering a configuration specification with slight differences:Global ParametersIn the traditional Deepstream applications, all the global parameters need to be defined under a specific section named ‘applications’:[application]
enable-perf-measurement=1
perf-measurement-interval-sec=5
gie-kitti-output-dir=/tmpWith Service Maker APIs, some of the parameters are expected to be added as command line argument. For example, the test5 app built from the Service Maker can change the performance measurement interval through a command line argument:—perf-measurement-interval-sec 5As for others which are not truly ‘global’, they can be set as a propertie of the corrsponding modules. For example, the ‘gie-kitti-output-dir’ is actually a property of the ‘kitti_dump_probe’ plugin built with the Service Maker, thus it can be set under the ‘Properties’ of the ‘kitti_dump_probe’ instance within the pipeline.-type:kitti_dump_probe.kitti_dump_probename:kitti_dumpproperties:kitti-dir:'/tmp'Pipeline customizationIn contrast to the traditional Deepstream application, Service Maker adopts a graph-based YAML definition to represent a pipeline structure, providing users with maximum flexibility for customization. The predefined sections in the traditional configuration, such as ‘primary-gie’, ‘secondary-gie’, ‘tracker’, ‘tiled-display’, and ‘osd’, can all be configured through the properties of each independent element within the Service Maker YAML definition.Source and BatchingIn traditional Deepstream applications, source management involves a combination of ‘sourceX’, ‘streammux’, ‘source-list’, and ‘source-attr-all’ in the configuration file. Service Maker simplifies this by separating the source management into a standalone source configuration file. In this file, source information, including ‘list’, ‘sensor-name-list’, and ‘sensor-id-list’, is placed within the ‘source-list’ section. Meanwhile, pipeline customization, such as specifying the source type and properties, is handled in the ‘source-config’ section.Sink managementCompared to the traditional Deepstream application, Service Maker offers support for a wider range of arbitrary sink options. These include existing types such as ‘Fakesink’, ‘Display’, ‘File’, ‘UDP’, and ‘MsgConvBroker’. Each of these sinks must be defined within the graph pipeline specification, which includes all necessary element nodes and the edges between them. This approach provides greater flexibility in configuring sinks for the pipeline.previousWhat is Deepstream Service MakernextService Maker for Python Developers(alpha)On this pageGetting Started with Service Maker C++ APIsApplication Developer FundamentalsPipelineGuideline for Fine-tuning a Pipeline through Configuration FileElements and PluginsBufferMetadataSourceConfigCameraConfigCommonFactory and custom ObjectsBufferProbeSignalHandlerDataFeederDataReceiverSignalEmitterPrepare and ActivatePipeline.prepare()Pipeline.activate()Pipeline.wait()State Transitions in Service Maker PipelinesStartPrepare and ActivateWaitStopSample ApplicationsMigrating Traditional Deepstream Apps to Service MakerPrivacy Policy|Manage My Privacy|Do Not Sell or Share My Data|Terms of Service|Accessibility|Corporate Policies|Product Security|ContactCopyright © 2024-2025, NVIDIA Corporation.Last updated on Jan 13, 2025.