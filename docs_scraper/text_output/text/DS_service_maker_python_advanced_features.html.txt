Advanced Features — DeepStream documentationSkip to main contentBack to topCtrl+KDeepStream documentationDeepStream documentationTable of ContentsDeepStream Getting StartedWelcome to the DeepStream DocumentationMigration GuideInstallationQuickstart GuideDocker ContainersDeepStream SamplesC/C++ Sample Apps Source DetailsPython Sample Apps and Bindings Source DetailsDeepStream Reference Application - deepstream-appDeepStream Reference Application - deepstream-test5 appDeepStream Reference Application - deepstream-nmos appDeepStream Reference Application on GitHubSample Configurations and StreamsImplementing a Custom GStreamer Plugin with OpenCV Integration ExampleTAO toolkit Integration with DeepStreamTAO Toolkit Integration with DeepStreamTutorials and How-to'sDeepStream-3D Custom Apps and Libs TutorialsDeepStream PerformancePerformanceDeepStream AccuracyAccuracy Tuning ToolsDeepStream Custom ModelUsing a Custom Model with DeepStreamDeepStream Key FeaturesDeepStream-3D Sensor Fusion Multi-Modal Application and FrameworkDeepStream-3D Multi-Modal BEVFusion SetupDeepStream-3D Multi-Modal V2XFusion SetupSmart Video RecordIoTOn the Fly Model UpdateNTP Timestamp in DeepStreamAV Sync in DeepStreamDeepStream With REST API SeverDeepStream 3D Action Recognition AppDeepStream 3D Depth Camera AppDeepStream 3D Lidar Inference AppNetworked Media Open Specifications (NMOS) in DeepStreamGst-nvdspostprocess in DeepStreamDeepStream Can Orientation AppDeepStream Application MigrationApplication Migration to DeepStream 7.1 from DeepStream 7.0DeepStream Plugin GuideGStreamer Plugin OverviewMetaData in the DeepStream SDKGst-nvdspreprocess (Alpha)Gst-nvinferGst-nvinferserverGst-nvtrackerGst-nvstreammuxGst-nvstreammux NewGst-nvstreamdemuxGst-nvmultistreamtilerGst-nvdsosdGst-nvdsmetautilsGst-nvdsvideotemplateGst-nvdsaudiotemplateGst-nvvideoconvertGst-nvdewarperGst-nvofGst-nvofvisualGst-nvsegvisualGst-nvvideo4linux2Gst-nvjpegdecGst-nvimagedecGst-nvjpegencGst-nvimageencGst-nvmsgconvGst-nvmsgbrokerGst-nvdsanalyticsGst-nvdsudpsrcGst-nvdsudpsinkGst-nvdspostprocess (Alpha)Gst-nvds3dfilterGst-nvds3dbridgeGst-nvds3dmixerGst-NvDsUcxGst-nvdsxferGst-nvvideotestsrcGst-nvmultiurisrcbinGst-nvurisrcbinDeepStream Troubleshooting and FAQTroubleshootingFrequently Asked QuestionsDeepStream On WSL2DeepStream On WSLFAQ for Deepstream On WSLDeepStream API GuideDeepStream API GuidesDeepStream Service MakerWhat is Deepstream Service MakerService Maker for C/C++ DevelopersService Maker for Python Developers(alpha)Quick Start GuideIntroduction to Flow APIsIntroduction to Pipeline APIsAdvanced FeaturesMigrating Traditional Deepstream Apps to Service Maker Apps in PythonWhat is a Deepstream Service Maker PluginDeepstream LibrariesDeepStream Libraries (Developer Preview)Graph ComposerOverviewPlatformsSupported platformsGetting StartedApplication Development WorkflowCreating an AI ApplicationReference graphsExtension Development WorkflowDeveloping Extensions for DeepStreamDeepStream ComponentsGXF InternalsGXF InternalsGraph eXecution EngineGraph Execution EngineGraph Composer ContainersGraph Composer and GXF ContainersGXF Component InterfacesGXF Component InterfacesGXF Application API'sGXF App C++ APIsGXF App Python APIsGXF Runtime API'sGXF Core C++ APIsGXF Core C APIsGXF Core Python APIsExtension ManualExtensionsCudaExtensionGXF Stream SyncStandardExtensionPython CodeletsNetworkExtensionNvTritonExtSerializationExtensionMultimediaExtensionVideoEncoderExtensionVideoDecoderExtensionBehavior TreesUCX ExtensionHttpExtensionGrpcExtensionTensorRTExtensionNvDs3dProcessingExtNvDsActionRecognitionExtNvDsAnalyticsExtNvDsBaseExtNvDsCloudMsgExtNvDsConverterExtNvDsDewarperExtNvDsInferenceExtNvDsInferenceUtilsExtNvDsInterfaceExtNvDsMuxDemuxExtNvDsOpticalFlowExtNvDsOutputSinkExtNvDsSampleExtNvDsSampleModelsExtNvDsSourceExtNvDsTemplateExtNvDsTrackerExtNvDsTranscodeExtNvDsTritonExtNvDsUcxExtNvDsUdpExtNvDsVisualizationExtToolsRegistryRegistry Command Line InterfaceComposerContainer BuilderGXF Command Line InterfacePipetuner GuideFAQ GuideFAQDeepStream Legal InformationDeepStream End User License AgreementDeepStream FeedbackFeedback formService Maker for Python Developers(alpha)Advanced FeaturesAdvanced Features#The pyservicemaker provides advanced APIs/utilities for developers to unlock the full potential of DeepStream SDK.Handling Buffers#A Buffer object in Service Maker represents a batch of data along with corresponding metadata. It provides two properties, batch_size and batch_meta, for developers to access the data and metadata of a batch.Buffer objects are commonly passed either through BufferOperator or BufferRetriever interfaces, both of which are designed to be implemented within the application to handle the data from buffers within the pipeline. One  difference is BufferOperator interface is required by a Probe and BufferRetriever by a Receiver. While it is technically possible to copy and process buffers within a probe, that’s generally not recommended. Probes are primarily intended for inspection purposes, and performing time-consuming tasks within them should be avoided. This is because probes are typically executed within the critical path of the pipeline, and any delays introduced by extensive processing could increase the overall latency. The other difference is BufferOperator only allows in-place operation on the Buffer data, and the result will be passed downstream and used by other nodes within the same pipeline, however, BufferRetriever is supposed to consume the buffer at the end of the pipeline.Below is a sample implementation of BufferOperator for skipping frames. If the handle_buffer returns ‘False’, the current buffer will be dropped.frompyservicemakerimportBufferOperator,ProbeclassFrameSkipper(BufferOperator):def__init__(self):super().__init__()self._frames=0defhandle_buffer(self,buffer):self._frames+=1return(self._frames%2)!=0Once we try applying the frame skipper to above sample application as follows, we’ll notice the downstream elements only get one frame in two.pipeline.attach("mux",Probe("probe",FrameSkipper()))For performing heavy operations on buffers, the preferred approach is to use BufferRetriever. By replacing the ‘nveglglessink’ with ‘appsink’ in the above sample and attaching a Receiver with a BufferRetriever implementation, we can access buffer in its ‘consume’ method. The most convenient way to access buffer data is to extract it into a tensor - a dlpack-compatible class designed to facilitate interoperability with other Python deep learning frameworks.frompyservicemakerimportBufferRetriever,ReceiverclassMyBufferRetriever(BufferRetriever):defconsume(self,buffer):# extract the data from the first buffer in a batchtensor=buffer.extract(0)pipeline.attach("sink",Receiver("receiver",MyBufferRetriever(queue)),tips="new-sample")A Service Maker Tensor object can by casted to corresponding tensors in any dlpack-compatible frameworks, e.g. pytorch:frompyservicemakerimportBufferRetriever,ReceiverimporttorchclassMyBufferRetriever(BufferRetriever):defconsume(self,buffer):tensor=buffer.extract(0)torch_tensor=torch.utils.dlpack.from_dlpack(tensor.clone())NoteIf developers want to preserve the tensor data for later use, they must use clone() method to duplicate the tensor, as the BufferRetriever is at the end of the pipeline and the buffer will be released after consume() returns.A buffer object can be created via a tensor as well. This approach is commonly used in the implementation of BufferProvider, which works with the Feeder class and ‘appsrc’ to inject data into the pipeline. Below sample demonstrates how to load a pytorch RGB tensor and inject it into the pipeline.frompyservicemakerimportBufferProvider,ColorFormat,as_tensorimporttorchclassMyBufferProvider(BufferProvider):defgenerate(self,size):torch_tensor=torch.load('tensor_data.pt')ds_tensor=as_tensor(torch_tensor,"HWC")returnds_tensor.wrap(ColorFormat.RGB)By replacing the ‘urisrcbin’ with ‘appsrc’ in the original sample code, MyBufferProvider can be incorporated into the pipeline:pipeline.add("appsrc","src",{"caps":f"video/x-raw(memory:NVMM), format=RGB, width={width}, height={height}, framerate=30/1","do-timestamp":True})Moreover, a buffer can be created via a python byte list too, for example, mono pictures in black are generated from below code:frompyservicemakerimportBufferProvider,BufferclassMyBufferProvider(BufferProvider):defgenerate(self,size):data=[0]*(320*240*3)returnBuffer(data)Leveraging Metadata#Metadata provides additional information about a buffer in Deepstream Service Maker. A BatchMetadata object, which can be retrieved from a buffer, serves as the root for all other metadata objects. Batch metadata manages a metadata pool and provides methods for initializing a variety of metadata objects.The BatchMetadata object contains metadata for a batch of frames within a buffer. It carries the batching information such as size of the batch, number of frames in the batch, and also provides access to a list of FrameMetadata objects through its frame_items attribute.The FrameMetadata objects encapsulate metadata for individual frames in the buffer batch. These frames can originate from different input streams. Through FrameMetadata, developers can access various pieces of information, including dimensions, timestamps, frame number, etc. Moreover, frame metadata carries other metadata on the frame level, including ObjectMetadata, DisplayMetadata and various user metadata.The ObjectMetadata objects carry the results of object detection. These metadata objects are typically created by an object detection model within the ‘nvinfer’ plugin. The ObjectMetadata can then be utilized by the ‘osd (On-Screen Display)’ plugin for graphical representation. By leveraging ObjectMetadata, developers can visualize detected objects on the frame. When using customized models, developers can create their own ObjectMetadata from the model output for visualization.The TensorOutputUserMetadata is a customized user metadata that carries the output of an inference model. When the output-tensor-meta property in the nvinfer plugin is set to True, TensorOutputUserMetadata objects will be attached to the frame metadata. This allows developers to have direct access to the output tensors of a model. By using TensorOutputUserMetadata, developers can directly work with the inference results, enabling advanced processing and analysis of model outputs.The EventMessageUserMetadata is another type of user metadata that transforms object detection information into message objects. These message metadata are utilized by the nvmsgconv plugin to post messages to various IoT servers. By employing EventMessageUserMetadata, developers can facilitate the communication of detection events to external systems, enhancing the integration of Deepstream Service Maker with IoT infrastructures.For the details of metadata structure in Deepstream, users should refer tothe document.Common Factory#Service maker python binding supports creating custom object directly from a service maker plugin.frompyservicemakerimportCommonFactory# create a smart recording controller instance from "smart_recording_action" and name it "sr_controller"CommonFactory.create("smart_recording_action","sr_controller")The feature usage has been demonstrated in service-maker/sources/apps/python/pipeline_api/deepstream_test5_app application.Pipeline Messages#While the pipeline is running, various messages are generated to inform the application about its status. Service Maker supports the ‘on_message’ parameter in the ‘start’ method, providing developers with an opportunity to inspect and respond to the messages they’re interested in.The on_message callable takes two parameters: the current pipeline and the message received.Service maker offers the bindings for two most important messages:StateTransitionMessageindicates a state transition within the pipeline:namedescriptionold_statethe last state of the transitionnew_statethe current state of the transitionoriginname of node triggering the messageDynamicSourceMessageindicates source is added or removed via REST APIs. The message could be generated in case ‘nvmultiurisrcbin’ is used.namedescriptionsource_addedadd or removalsource_idindex of the sourcesensor_idunique string identifying the sensorsensor_namea meaningful name of the sensoruriURI of the sensorThe feature usage has been demonstrated in service-maker/sources/apps/python/pipeline_api/deepstream_test5_app application.OSD#Service maker provides a submodule ‘osd’ for advanced display control with basic graphical gadgets.Text: a formatted stringRect: a colorful rectangleLine: straight lineArrow: arrowsCircle: circlesEventHandler: a pre-built class to respond on mouse eventsThe feature usage has been demonstrated in service-maker/sources/apps/python/flow_api/deepstream_test1_app application.Engine File Monitor#The EngineFileMonitor is defined in ‘utils’ submodule and can be used to monitor a model engine file, whenever it is updated the inference will restart without disrupting the whole pipeline.The feature usage has been demonstrated in service-maker/sources/apps/python/pipeline_api/deepstream_test5_app application.Performance Monitor#The PerfMonitor is defined in ‘utils’ submodule and can be used to monitor the framerate in realtime. Before a PerfMonitor takes effect, it must be applied on a specific node within the pipeline. When used with ‘nvmultiurisrcbin’, new source needs be registered with PerfMonitor using ‘add_stream’, and after a source is removed, ‘remove_stream’ of PerfMonitor must be called accordingly.The feature usage has been demonstrated in service-maker/sources/apps/python/pipeline_api/deepstream_test5_app application.MediaInfo#MediaInfo object can be used to retrieve the information of a media source.frompyservicemakerimportutilsmediainfo=utils.MediaInfo.discover("sample.mp4")MediaExtractor#MediaExtractor is a convenient class provided in utils module for extracting video frames from multiple media source. Each media source is defined by a MediaChunk object with ‘source’, ‘start_pts’ and ‘duration’. MediaExtractor is a callable and must be constructed with a list of input MediaChunk, once invoked it returns a list of Queue object from which decoded frames of each media chunk can be retrieved. From the decoded video frame, developers can access the data from its tensor attribute and timestamp from its ‘timestamp’ attribute.Below code snippet shows how to retrieve frames from media sources and is only applicable for x86 system:frompyservicemakerimportBufferProvider,Buffer,Flow,Pipeline,ColorFormatfrompyservicemaker.utilsimportMediaExtractor,MediaChunkfromconcurrent.futuresimportThreadPoolExecutorimportqueueVIDEO_FILE="/opt/nvidia/deepstream/deepstream/samples/streams/sample_720p.mp4"N_CHUNKS=8classMyBufferProvider(BufferProvider):def__init__(self,queue,width,height):print("MyBufferProvider")super().__init__()self._queue=queueself.format="RGB"self.width=widthself.height=heightself.framerate=30self.device='gpu'self.frames=0defgenerate(self,size):try:frame=self._queue.get(timeout=2)self.frames+=1returnframe.tensor.wrap(ColorFormat.RGB)exceptqueue.Empty:print("Buffer empty")returnBuffer()chunks=[MediaChunk(f)forfin[VIDEO_FILE]*N_CHUNKS]qs=MediaExtractor(chunks=chunks,batch_size=N_CHUNKS)()withThreadPoolExecutor(max_workers=N_CHUNKS)asexe:exe.map(lambdaq:Flow(Pipeline("renderer")).inject([MyBufferProvider(q,1280,720)]).render(sync=False)(),qs)Prepare and Activate#For specific use cases, we have introduced alternative APIs to manage pipeline execution:Pipeline.prepare()#Unlike pipeline.start(), which transitions the pipeline’s state to playing in a new thread, pipeline.prepare() sets the pipeline’s state to paused within the same thread. This is particularly useful in multi-pipeline scenarios where each pipeline’s state needs to be changed to paused sequentially.Pipeline.activate()#After using pipeline.prepare(), pipeline.activate() transitions the pipeline’s state from paused to playing in a new thread. This effectively starts the pipeline. In multi-pipeline scenarios, this allows all pipelines to begin execution concurrently.Pipeline.wait()#This API is used to join the threads created by pipeline.activate(), ensuring that all threads complete their execution before proceeding.The Sample Test 5 Application demonstrates how to utilize these API calls effectively.State Transitions in Service Maker Pipelines#Managing state transitions in service pipelines is essential for ensuring efficient and orderly operations.
Service maker facilitates synchronous state transitions, enabling smooth and predictable changes in the pipeline’s status. Here’s a clearer and more structured explanation of these transitions:Start#Transition: From idle to running.Process: Utilize the pipeline.start() API to initiate the pipeline, moving it from an idle state to actively running tasks.Prepare and Activate#Transition: From idle to paused, then to running.Process: By executing pipeline.prepare() followed by pipeline.activate(), the pipeline transitions through a paused state before becoming fully operational. This sequence allows for any necessary setup or configuration prior to execution.Wait#Purpose: Suspend the current thread until the pipeline completes its execution.Process: The pipeline.wait() function is required after both the start and prepare/activate sequences to ensure that the current thread pauses until the pipeline has finished processing.Stop#Transition: Back to idle.Process: The pipeline.stop() API is used to return the pipeline to its idle state, effectively halting operations and resetting it for future tasks.These transitions are designed to manage workflow efficiently, allowing for both immediate execution and necessary preparatory phases.previousIntroduction to Pipeline APIsnextMigrating Traditional Deepstream Apps to Service Maker Apps in PythonOn this pageHandling BuffersLeveraging MetadataCommon FactoryPipeline MessagesOSDEngine File MonitorPerformance MonitorMediaInfoMediaExtractorPrepare and ActivatePipeline.prepare()Pipeline.activate()Pipeline.wait()State Transitions in Service Maker PipelinesStartPrepare and ActivateWaitStopPrivacy Policy|Manage My Privacy|Do Not Sell or Share My Data|Terms of Service|Accessibility|Corporate Policies|Product Security|ContactCopyright © 2024-2025, NVIDIA Corporation.Last updated on Jan 13, 2025.