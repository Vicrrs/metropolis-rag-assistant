Introduction to Pipeline APIs — DeepStream documentationSkip to main contentBack to topCtrl+KDeepStream documentationDeepStream documentationTable of ContentsDeepStream Getting StartedWelcome to the DeepStream DocumentationMigration GuideInstallationQuickstart GuideDocker ContainersDeepStream SamplesC/C++ Sample Apps Source DetailsPython Sample Apps and Bindings Source DetailsDeepStream Reference Application - deepstream-appDeepStream Reference Application - deepstream-test5 appDeepStream Reference Application - deepstream-nmos appDeepStream Reference Application on GitHubSample Configurations and StreamsImplementing a Custom GStreamer Plugin with OpenCV Integration ExampleTAO toolkit Integration with DeepStreamTAO Toolkit Integration with DeepStreamTutorials and How-to'sDeepStream-3D Custom Apps and Libs TutorialsDeepStream PerformancePerformanceDeepStream AccuracyAccuracy Tuning ToolsDeepStream Custom ModelUsing a Custom Model with DeepStreamDeepStream Key FeaturesDeepStream-3D Sensor Fusion Multi-Modal Application and FrameworkDeepStream-3D Multi-Modal BEVFusion SetupDeepStream-3D Multi-Modal V2XFusion SetupSmart Video RecordIoTOn the Fly Model UpdateNTP Timestamp in DeepStreamAV Sync in DeepStreamDeepStream With REST API SeverDeepStream 3D Action Recognition AppDeepStream 3D Depth Camera AppDeepStream 3D Lidar Inference AppNetworked Media Open Specifications (NMOS) in DeepStreamGst-nvdspostprocess in DeepStreamDeepStream Can Orientation AppDeepStream Application MigrationApplication Migration to DeepStream 7.1 from DeepStream 7.0DeepStream Plugin GuideGStreamer Plugin OverviewMetaData in the DeepStream SDKGst-nvdspreprocess (Alpha)Gst-nvinferGst-nvinferserverGst-nvtrackerGst-nvstreammuxGst-nvstreammux NewGst-nvstreamdemuxGst-nvmultistreamtilerGst-nvdsosdGst-nvdsmetautilsGst-nvdsvideotemplateGst-nvdsaudiotemplateGst-nvvideoconvertGst-nvdewarperGst-nvofGst-nvofvisualGst-nvsegvisualGst-nvvideo4linux2Gst-nvjpegdecGst-nvimagedecGst-nvjpegencGst-nvimageencGst-nvmsgconvGst-nvmsgbrokerGst-nvdsanalyticsGst-nvdsudpsrcGst-nvdsudpsinkGst-nvdspostprocess (Alpha)Gst-nvds3dfilterGst-nvds3dbridgeGst-nvds3dmixerGst-NvDsUcxGst-nvdsxferGst-nvvideotestsrcGst-nvmultiurisrcbinGst-nvurisrcbinDeepStream Troubleshooting and FAQTroubleshootingFrequently Asked QuestionsDeepStream On WSL2DeepStream On WSLFAQ for Deepstream On WSLDeepStream API GuideDeepStream API GuidesDeepStream Service MakerWhat is Deepstream Service MakerService Maker for C/C++ DevelopersService Maker for Python Developers(alpha)Quick Start GuideIntroduction to Flow APIsIntroduction to Pipeline APIsAdvanced FeaturesMigrating Traditional Deepstream Apps to Service Maker Apps in PythonWhat is a Deepstream Service Maker PluginDeepstream LibrariesDeepStream Libraries (Developer Preview)Graph ComposerOverviewPlatformsSupported platformsGetting StartedApplication Development WorkflowCreating an AI ApplicationReference graphsExtension Development WorkflowDeveloping Extensions for DeepStreamDeepStream ComponentsGXF InternalsGXF InternalsGraph eXecution EngineGraph Execution EngineGraph Composer ContainersGraph Composer and GXF ContainersGXF Component InterfacesGXF Component InterfacesGXF Application API'sGXF App C++ APIsGXF App Python APIsGXF Runtime API'sGXF Core C++ APIsGXF Core C APIsGXF Core Python APIsExtension ManualExtensionsCudaExtensionGXF Stream SyncStandardExtensionPython CodeletsNetworkExtensionNvTritonExtSerializationExtensionMultimediaExtensionVideoEncoderExtensionVideoDecoderExtensionBehavior TreesUCX ExtensionHttpExtensionGrpcExtensionTensorRTExtensionNvDs3dProcessingExtNvDsActionRecognitionExtNvDsAnalyticsExtNvDsBaseExtNvDsCloudMsgExtNvDsConverterExtNvDsDewarperExtNvDsInferenceExtNvDsInferenceUtilsExtNvDsInterfaceExtNvDsMuxDemuxExtNvDsOpticalFlowExtNvDsOutputSinkExtNvDsSampleExtNvDsSampleModelsExtNvDsSourceExtNvDsTemplateExtNvDsTrackerExtNvDsTranscodeExtNvDsTritonExtNvDsUcxExtNvDsUdpExtNvDsVisualizationExtToolsRegistryRegistry Command Line InterfaceComposerContainer BuilderGXF Command Line InterfacePipetuner GuideFAQ GuideFAQDeepStream Legal InformationDeepStream End User License AgreementDeepStream FeedbackFeedback formService Maker for Python Developers(alpha)Introduction...Introduction to Pipeline APIs#For developers already familiar with the DeepStream SDK, pyservicemaker offers Pipeline APIs that enable them to fully harness the capabilities of DeepStream.Creating a sample Deepstream application in Python using Pipeline APIs closely mirrors the process with C++ APIs, with the notable distinction that it doesn’t require a Makefile or build process.frompyservicemakerimportPipelineimportsysCONFIG_FILE_PATH="/opt/nvidia/deepstream/deepstream/samples/configs/deepstream-app/config_infer_primary.yml"if__name__=='__main__':pipeline=Pipeline("sample-pipeline")pipeline.add("nvurisrcbin","src",{"uri":sys.argv[1]})pipeline.add("nvstreammux","mux",{"batch-size":1,"width":1280,"height":720})pipeline.add("nvinferbin","infer",{"config-file-path":CONFIG_FILE_PATH})pipeline.add("nvosdbin","osd").add("nveglglessink","sink")pipeline.link(("src","mux"),("","sink_%u")).link("mux","infer","osd","sink")pipeline.start().wait()A functional pipeline requires appropriate elements from Deepstream plugins to be added, configured and linked correctly.
This can be seamlessly achieved using Pipeline APIs in a fluent manner:Pipelinepipeline("sample-pipeline")//nvstreammuxisthefactorynameinDeepstreamtocreateastreammuxerElement//muxisthenameoftheElementinstance//multiplekeyvaluepairscanbeappendedtoconfiguretheaddedElementpipeline.add("nvstreammux","mux","batch-size",1,"width",1280,"height",720)The ‘add’ method is used to incorporate all necessary element nodes into a pipeline instance. This method takes the element’s registration name and node name as parameters, followed by a dictionary specifying the element’s properties. The node name given to an element during addition can be used to refer to the element within the pipeline.For detailed insights into how each property affects the respective element,DS Plugin Overviewserves as the primary and most comprehensive resource. Developers can also run gst-inspect-1.0 with element registration name to check its technical specification, e.g. after looking up thenvstreammuxfrom the plugin manual, we know the element is for batching buffers from multiple input and it requires “batch-size”, “width” and “height” to be set.After the element nodes are added into the pipeline, the ‘link’ method accomplishes the construction of the streaming path. This method offers two variations:The simpler one accepts the names of all instances to be linked sequentially.pipeline.link("mux","infer","osd","sink")The more sophisticated one links two instances, utilizing two tuples to specify the source name and target name, along with the source and target pads to indicate the specific media streams.pipeline.link(("src","mux"),("","sink_%u"))The second ‘link’ method primarily addresses dynamic paths, such as those encountered with the ‘nvstreammux’ element. This element features a dynamic input and a template pad named “sink_%u,” which requires the use of this method to establish the appropriate connections.To start the pipeline and wait until the stream reaches its end, the ‘start’ method and ‘wait’ method need to be called sequentially.pipeline.start().wait()Now let’s save the code as sample_app.py and run the application from the console:$ python3 sample_app.py file:///opt/nvidia/deepstream/deepstream/samples/streams/sample_720p.mp4The inference is operating as expected with the provided model configuration. However, if we wish to inspect the results, additional steps are required by attaching a probe.A probe is a predefined class utilized to intercept the processed buffer and metadata. By providing it with a suitable implementation of BatchMetadataOperator, we can extract the object detection results from the metadata generated by ‘nvinfer’.Below is a sample implementation of a BatchMetadataOperator, which counts the objects and displays the numbers in the downstream ‘osd’ element. The ‘handle_metadata’ method is called within the pipeline on every buffer batch, with ‘batch_meta’ object wrapping the batch metadata. Developers can iterate through the ‘batch_meta’ object for frame metadata, then frame metadata for object metadata.In the sample code, handle_metadata inspects the object information for each frame within a batch, calculates the count for each class and appends a display metadata object containing a text label to the frame. Thus, the numbers get displayed in the video output. For more details about the metadata usage, refer toLeveraging Metadata.frompyservicemakerimportBatchMetadataOperator,Probe,osdclassObjectCounterMarker(BatchMetadataOperator):defhandle_metadata(self,batch_meta):forframe_metainbatch_meta.frame_items:vehcle_count=0person_count=0forobject_metainframe_meta.object_items:class_id=object_meta.class_idifclass_id==0:vehcle_count+=1elifclass_id==2:person_count+=1print(f"Object Counter: Pad Idx={frame_meta.pad_index},"f"Frame Number={frame_meta.frame_number},"f"Vehicle Count={vehcle_count}, Person Count={person_count}")text=f"Person={person_count},Vehicle={vehcle_count}"display_meta=batch_meta.acquire_display_meta()label=osd.Text()label.display_text=text.encode('ascii')label.x_offset=10label.y_offset=12label.font.name=osd.FontFamily.Seriflabel.font.size=12label.font.color=osd.Color(1.0,1.0,1.0,1.0)label.set_bg_color=Truelabel.bg_color=osd.Color(0.0,0.0,0.0,1.0)display_meta.add_text(label)frame_meta.append(display_meta)By attaching the above buffer probe into the inference plugin within the existing pipeline before starting it, we extract object count information from each frame of the video stream and display it both in the console output and as an overlay on the video:pipeline.attach("infer",Probe("counter",ObjectCounterMarker()))Now let’s run the python application again and we’ll see the object counts printed out:Object Counter: Pad Idx=0,Frame Number=0,Vehicle Count=12, Person Count=6
Object Counter: Pad Idx=0,Frame Number=1,Vehicle Count=15, Person Count=7
Object Counter: Pad Idx=0,Frame Number=2,Vehicle Count=13, Person Count=5
Object Counter: Pad Idx=0,Frame Number=3,Vehicle Count=12, Person Count=6
Object Counter: Pad Idx=0,Frame Number=4,Vehicle Count=15, Person Count=8
Object Counter: Pad Idx=0,Frame Number=5,Vehicle Count=15, Person Count=5
Object Counter: Pad Idx=0,Frame Number=6,Vehicle Count=11, Person Count=5
Object Counter: Pad Idx=0,Frame Number=7,Vehicle Count=13, Person Count=5
Object Counter: Pad Idx=0,Frame Number=8,Vehicle Count=19, Person Count=4
Object Counter: Pad Idx=0,Frame Number=9,Vehicle Count=15, Person Count=5
Object Counter: Pad Idx=0,Frame Number=10,Vehicle Count=13, Person Count=4In addition to creating a Probe instance from Python code, the ‘attach’ method can also attach a probe from a shared library by specifying the module name. Below code attaches a pre-built probe for displaying the object information over ‘osd’:pipeline.attach("infer","sample_video_probe","my_probe")YAML configuration files for pipeline construction are supported, following the same specification as used by C++ APIs. Below is how the above pipeline can be defined in YAML configuration:deepstream:nodes:-type:nvurisrcbinname:srcproperties:uri:file:///opt/nvidia/deepstream/deepstream/samples/streams/sample_1080p_h264.mp4-type:nvstreammuxname:muxproperties:batch-size:1width:1280height:720-type:nvinferbinname:inferproperties:config-file-path:/opt/nvidia/deepstream/deepstream/samples/configs/deepstream-app/config_infer_primary.yml-type:nvosdbinname:osd-type:nveglglessinkname:sinkedges:src:muxmux:inferinfer:osdosd:sinkAnd with the YAML configuration being applied, the application source can be simplified to a single line:Pipeline(name="sample-pipeline",config_file="my_config.yaml").start().wait()PipelineAPI Sample Applications Reference Table#Reference test applicationPath insideservice-maker/sourcesdirectoryDescriptionSample test application 1apps/python/pipeline_api/deepstream_test1_appSample of how to use DeepStream elements for a single H.264 stream inference using pipelineAPI: filesrc -> decode -> nvstreammux -> nvinfer or nvinferserver (primary detector) -> nvdsosd ->  renderer. This app uses resnet18_trafficcamnet_pruned.onnx for detection.Sample test application 2apps/python/pipeline_api/deepstream_test2_appSample of how to use DeepStream elements for a single H.264 stream cascaded inference using pipelineAPI: filesrc -> decode -> nvstreammux -> nvinfer or nvinferserver (primary detector) -> nvtracker -> nvinfer or nvinferserver (secondary classifier) -> nvdsosd -> renderer. This app uses resnet18_trafficcamnet_pruned.onnx for detection and 2 classifier models (i.e., resnet18_vehiclemakenet_pruned.onnx, resnet18_vehicletypenet_pruned.onnx).Sample test application 3apps/python/pipeline_api/deepstream_test3_appBuilds on pipeline_api/deepstream_test1 (sample test application 1) to demonstrate how to:Use multiple sources in the pipeline for inference.Use a uridecodebin to accept any type of input (e.g. RTSP/File).Configure nvstreammux to generate a batch of frames and infer on it for better resource utilization.Extract the stream metadata, which contains useful information about the frames in the batched buffer.This app uses resnet18_trafficcamnet_pruned.onnx for detection.Sample test application 4apps/python/pipeline_api/deepstream_test4_appBuilds on pipeline_api/deepstream_test1 for a single H.264 stream inference: filesrc, decode, nvstreammux, nvinfer or nvinferserver, nvdsosd, renderer to demonstrate how to:Use the nvmsgconv and nvmsgbroker plugins in the pipeline.Create NVDS_META_EVENT_MSG type metadata and attach it to the buffer.Use NVDS_META_EVENT_MSG for different types of objects, e.g. vehicle and person.This app uses resnet18_trafficcamnet_pruned.onnx for detection.Sample test application 5apps/python/pipeline_api/deepstream_test5_appBuilds with pipelineAPI. Demonstrates:Use of nvmsgconv and nvmsgbroker plugins in the pipeline for multistream inference.How to configure nvmsgbroker plugin from the config file as a sink plugin (for KAFKA, Azure, etc.).How to work with a remote Kafaka server as producer and consumer.Leveraging nvmultiurisrcbin for dynamic source managementThis app uses resnet18_trafficcamnet_pruned.onnx for detection.Appsrc and Appsink exampleapps/python/pipeline_api/deepstream_appsrc_test_appDemonstrates how to create a BufferProvider for a Feeder class and how to create a BufferRetriever for a receiver class. A Feeder with customized BufferProvider can be used to inject user data to the DS pipeline and a receiver with a customized BufferRetriever can be used to extract buffer data from the pipeline.Kafka Custom Data exampleapps/python/pipeline_api/deepstream_kafka_test_appSample of how to use DeepStream elements for a single H.264 stream inference and send custom inference data directly to a kafka server using pipelineAPI: filesrc -> decode -> nvstreammux -> nvinfer or nvinferserver (primary detector) -> nvdsosd ->  renderer. This app uses resnet18_trafficcamnet_pruned.onnx for detection.previousIntroduction to Flow APIsnextAdvanced FeaturesOn this pageIntroduction to Pipeline APIsPipelineAPI Sample Applications Reference TablePrivacy Policy|Manage My Privacy|Do Not Sell or Share My Data|Terms of Service|Accessibility|Corporate Policies|Product Security|ContactCopyright © 2024-2025, NVIDIA Corporation.Last updated on Jan 13, 2025.