Quick Start Guide — DeepStream documentationSkip to main contentBack to topCtrl+KDeepStream documentationDeepStream documentationTable of ContentsDeepStream Getting StartedWelcome to the DeepStream DocumentationMigration GuideInstallationQuickstart GuideDocker ContainersDeepStream SamplesC/C++ Sample Apps Source DetailsPython Sample Apps and Bindings Source DetailsDeepStream Reference Application - deepstream-appDeepStream Reference Application - deepstream-test5 appDeepStream Reference Application - deepstream-nmos appDeepStream Reference Application on GitHubSample Configurations and StreamsImplementing a Custom GStreamer Plugin with OpenCV Integration ExampleTAO toolkit Integration with DeepStreamTAO Toolkit Integration with DeepStreamTutorials and How-to'sDeepStream-3D Custom Apps and Libs TutorialsDeepStream PerformancePerformanceDeepStream AccuracyAccuracy Tuning ToolsDeepStream Custom ModelUsing a Custom Model with DeepStreamDeepStream Key FeaturesDeepStream-3D Sensor Fusion Multi-Modal Application and FrameworkDeepStream-3D Multi-Modal BEVFusion SetupDeepStream-3D Multi-Modal V2XFusion SetupSmart Video RecordIoTOn the Fly Model UpdateNTP Timestamp in DeepStreamAV Sync in DeepStreamDeepStream With REST API SeverDeepStream 3D Action Recognition AppDeepStream 3D Depth Camera AppDeepStream 3D Lidar Inference AppNetworked Media Open Specifications (NMOS) in DeepStreamGst-nvdspostprocess in DeepStreamDeepStream Can Orientation AppDeepStream Application MigrationApplication Migration to DeepStream 7.1 from DeepStream 7.0DeepStream Plugin GuideGStreamer Plugin OverviewMetaData in the DeepStream SDKGst-nvdspreprocess (Alpha)Gst-nvinferGst-nvinferserverGst-nvtrackerGst-nvstreammuxGst-nvstreammux NewGst-nvstreamdemuxGst-nvmultistreamtilerGst-nvdsosdGst-nvdsmetautilsGst-nvdsvideotemplateGst-nvdsaudiotemplateGst-nvvideoconvertGst-nvdewarperGst-nvofGst-nvofvisualGst-nvsegvisualGst-nvvideo4linux2Gst-nvjpegdecGst-nvimagedecGst-nvjpegencGst-nvimageencGst-nvmsgconvGst-nvmsgbrokerGst-nvdsanalyticsGst-nvdsudpsrcGst-nvdsudpsinkGst-nvdspostprocess (Alpha)Gst-nvds3dfilterGst-nvds3dbridgeGst-nvds3dmixerGst-NvDsUcxGst-nvdsxferGst-nvvideotestsrcGst-nvmultiurisrcbinGst-nvurisrcbinDeepStream Troubleshooting and FAQTroubleshootingFrequently Asked QuestionsDeepStream On WSL2DeepStream On WSLFAQ for Deepstream On WSLDeepStream API GuideDeepStream API GuidesDeepStream Service MakerWhat is Deepstream Service MakerService Maker for C/C++ DevelopersService Maker for Python Developers(alpha)Quick Start GuideIntroduction to Flow APIsIntroduction to Pipeline APIsAdvanced FeaturesMigrating Traditional Deepstream Apps to Service Maker Apps in PythonWhat is a Deepstream Service Maker PluginDeepstream LibrariesDeepStream Libraries (Developer Preview)Graph ComposerOverviewPlatformsSupported platformsGetting StartedApplication Development WorkflowCreating an AI ApplicationReference graphsExtension Development WorkflowDeveloping Extensions for DeepStreamDeepStream ComponentsGXF InternalsGXF InternalsGraph eXecution EngineGraph Execution EngineGraph Composer ContainersGraph Composer and GXF ContainersGXF Component InterfacesGXF Component InterfacesGXF Application API'sGXF App C++ APIsGXF App Python APIsGXF Runtime API'sGXF Core C++ APIsGXF Core C APIsGXF Core Python APIsExtension ManualExtensionsCudaExtensionGXF Stream SyncStandardExtensionPython CodeletsNetworkExtensionNvTritonExtSerializationExtensionMultimediaExtensionVideoEncoderExtensionVideoDecoderExtensionBehavior TreesUCX ExtensionHttpExtensionGrpcExtensionTensorRTExtensionNvDs3dProcessingExtNvDsActionRecognitionExtNvDsAnalyticsExtNvDsBaseExtNvDsCloudMsgExtNvDsConverterExtNvDsDewarperExtNvDsInferenceExtNvDsInferenceUtilsExtNvDsInterfaceExtNvDsMuxDemuxExtNvDsOpticalFlowExtNvDsOutputSinkExtNvDsSampleExtNvDsSampleModelsExtNvDsSourceExtNvDsTemplateExtNvDsTrackerExtNvDsTranscodeExtNvDsTritonExtNvDsUcxExtNvDsUdpExtNvDsVisualizationExtToolsRegistryRegistry Command Line InterfaceComposerContainer BuilderGXF Command Line InterfacePipetuner GuideFAQ GuideFAQDeepStream Legal InformationDeepStream End User License AgreementDeepStream FeedbackFeedback formService Maker for Python Developers(alpha)Quick Start GuideQuick Start Guide#Getting to know Pipeline and Flow#The first thing users need to know for using pyservicemaker ispipeline. Apipelineconsists of various processing nodes linked together to process or manipulate data flows. Each node instance performs a specific function, such as capturing data, decoding, processing, or rendering it. These nodes are connected in a chain, creating apipelinethrough which the data flows.A simple empty pipeline can be created with a given name as follows:frompyservicemakerimportPipelinepipeline=Pipeline("sample-pipeline")So far the pipeline can not do anything. To make it functional developers can define data flows for the pipeline by creating a flow for a pipeline. Flow APIs provide developers with the flexibility to append operations based on their specific goals subsequently. This approach allows for a modular and customizable workflow where operations can be added as needed. Every time a Flow method is called, it takes the intended output streams from the last flow and assume them as the inputs of current flow. Thus, the order of Flow methods determines how a flow works when the pipeline is started.Create a Simple Video Player Application#The code snippet below demonstrates how to create a simple flow to decode and display a video file.frompyservicemakerimportPipeline,Flowpipeline=Pipeline("playback")video_file="/opt/nvidia/deepstream/deepstream/samples/streams/sample_1080p_h264.mp4"Flow(pipeline).capture([video_file]).render()()Once the flow is complete, it can be invoked with ‘()’ because it is callable. This invocation keeps the pipeline running until the processing is finished. Additionally, developers can choose to call Pipeline.start() and Pipeline.wait(). The code snippet below is effectively equivalent to the above one.frompyservicemakerimportPipeline,Flowpipeline=Pipeline("playback")video_file="/opt/nvidia/deepstream/deepstream/samples/streams/sample_1080p_h264.mp4"Flow(pipeline).capture([video_file]).render()pipeline.start()pipeline.wait()Create an Object Detection Application#A more advanced pipeline for object detection built on top of Sample Video Player Application can be constructed using the infer method for inference as follows:frompyservicemakerimportPipeline,Flowpipeline=Pipeline("detector")infer_config="/opt/nvidia/deepstream/deepstream/samples/configs/deepstream-app/config_infer_primary.yml"video_file="/opt/nvidia/deepstream/deepstream/samples/streams/sample_1080p_h264.mp4"Flow(pipeline).batch_capture([video_file]).infer(infer_config).render()()The application executes an object detection model based on ResNet18 on the captured video. The model’s configuration is specified in the infer_config, and detected objects are automatically converted into bounding boxes, which are overlaid on the display.Customizing Object Detection Sample Application#In many use cases, developers require direct access to inference results for various analyses. To achieve this, the application can be enhanced by attaching a probe to the inference result (For deeper understanding of a probe, please refer toHandling Buffers). This probe allows developers to define custom metadata operators that extract output tensors from the metadata associated with the output buffers. (For more details on metadata, please refer toLeveraging Metadata.)Below is the enhanced version of the above code snippet.frompyservicemakerimportPipeline,Flow,Probe,BatchMetadataOperatorimporttorchclassTensorOutput(BatchMetadataOperator):defhandle_metadata(self,batch_meta):forframe_metainbatch_meta.frame_items:foruser_metainframe_meta.tensor_items:forn,tensorinuser_meta.as_tensor_output().get_layers().items():print(f"tensor name:{n}")print(f"tensor object:{tensor}")# operations on tensors:torch_tensor=torch.utils.dlpack.from_dlpack(tensor.clone())pipeline=Pipeline("detector")infer_config="/opt/nvidia/deepstream/deepstream/samples/configs/deepstream-app/config_infer_primary.yml"video_file="/opt/nvidia/deepstream/deepstream/samples/streams/sample_1080p_h264.mp4"probe=Probe('tensor_retriver',TensorOutput())Flow(pipeline).batch_capture([video_file]).infer(infer_config,output_tensor_meta=True).attach(probe).render()()The enhanced code retrieves inference result as tensors from user metadata associated with the output buffer from infer flow. This is done through a probe that utilizes a customized BatchMetadataOperator to extract and process the metadata.Getting to know Buffer and Tensor#In pyservicemaker, a Buffer represents a chunk of data that flows through the pipeline from one node to another. It contains the actual media data, such as a segment of audio or video, and is passed between nodes for processing or output. In some cases, developers would like to create their own buffers and inject it to the pipeline or consume the buffers from the pipeline. pyservicemaker provides two abstract classes for them to achieved such goals.BufferProvider#BufferProvider specifies an interface for developers to implement, so to generate buffers. The code snippet below demonstrates how to generate buffers from bytes with a sub-class of BufferProvider (For demonstration, the method generates grey pictures):frompyservicemakerimportBufferProvider,BufferclassMyBufferProvider(BufferProvider):def__init__(self,width,height,device='cpu',framerate=30,format="RGB"):super().__init__()self.width=widthself.height=heightself.format=formatself.framerate=framerateself.device=devicedefgenerate(self,size):data=[128]*(self.width*self.height*3)returnBuffer()ifself.count==self.expectedelseBuffer(data)Extra information is recommended for the pipeline to better understand what the data in the buffer represents, such as format, width, height, etc.Developers can create an empty buffer without any argument, and injecting an empty buffer to the pipeline will end it.To inject buffers to pipeline, developers can use inject flow with their customized BufferProvider. The code snippet below shows how to inject video buffers into a pipeline and display it:Flow(Pipeline("playback")).inject([MyBufferProvider(640,480)]).render()()BufferRetriever#BufferRetriever specifies a customizable interface for developers to consume the buffers. It must be used with the retrieve flow. The code snippet below shows how to get buffer data from the pipeline.# Read the decoded video buffers from a sample mp4 filefrompyservicemakerimportPipeline,Flow,BufferRetrieverclassMyBufferRetriever(BufferRetriever):defconsume(self,buffer):tensor=buffer.extract(0)return1To utilize the buffer data, developers must extract it into a tensor. If the data is batched, multiple tensors may be extracted, requiring the use of a batch ID. If there is no batching, 0 should always be passed to extract method. Sub class from BufferRetriever can be used together with the retrieve flow to get buffer from the pipeline:video_file="/opt/nvidia/deepstream/deepstream/samples/streams/sample_1080p_h264.mp4"Flow(Pipeline("retrieve")).capture([video_file]).retrieve(MyBufferRetriever())()Tensor#Tensor is essentially a multidimensional array that is widely used in deep learning space. The pyservicemaker Tensor facilitates data handling on the GPU and offers seamless interoperability with other frameworks through its compatibility with DLPack.A common practice for re-using data from a pyservicemaker Tensor is to clone the tensor and pass it to a PyTorch Tensor. This ensures that the original data remains unchanged while enabling efficient integration with other frameworks for further processing. The code snippet below shows how to transform a pyservicemaker Tensor to a pytorch tensortorch_tensor=torch.utils.dlpack.from_dlpack(tensor.clone())Similarly, developers also can create pyservicemaker Tensors from pytorch tensor, and then wrap it into a buffer. The code snippet below demonstrates how to generate buffers from pytorch tensors within a customized BufferProvider, which can be later used by inject flow.frompyservicemakerimportBufferProvider,ColorFormat,as_tensorimporttorchclassMyBufferProvider(BufferProvider):defgenerate(self,size):torch_tensor=torch.load('tensor_data.pt')ds_tensor=as_tensor(torch_tensor,"HWC")returnds_tensor.wrap(ColorFormat.RGB)previousService Maker for Python Developers(alpha)nextIntroduction to Flow APIsOn this pageGetting to know Pipeline and FlowCreate a Simple Video Player ApplicationCreate an Object Detection ApplicationCustomizing Object Detection Sample ApplicationGetting to know Buffer and TensorBufferProviderBufferRetrieverTensorPrivacy Policy|Manage My Privacy|Do Not Sell or Share My Data|Terms of Service|Accessibility|Corporate Policies|Product Security|ContactCopyright © 2024-2025, NVIDIA Corporation.Last updated on Jan 13, 2025.