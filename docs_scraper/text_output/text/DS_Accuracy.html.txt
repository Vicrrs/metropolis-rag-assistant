Accuracy Tuning Tools — DeepStream documentationSkip to main contentBack to topCtrl+KDeepStream documentationDeepStream documentationTable of ContentsDeepStream Getting StartedWelcome to the DeepStream DocumentationMigration GuideInstallationQuickstart GuideDocker ContainersDeepStream SamplesC/C++ Sample Apps Source DetailsPython Sample Apps and Bindings Source DetailsDeepStream Reference Application - deepstream-appDeepStream Reference Application - deepstream-test5 appDeepStream Reference Application - deepstream-nmos appDeepStream Reference Application on GitHubSample Configurations and StreamsImplementing a Custom GStreamer Plugin with OpenCV Integration ExampleTAO toolkit Integration with DeepStreamTAO Toolkit Integration with DeepStreamTutorials and How-to'sDeepStream-3D Custom Apps and Libs TutorialsDeepStream PerformancePerformanceDeepStream AccuracyAccuracy Tuning ToolsDeepStream Custom ModelUsing a Custom Model with DeepStreamDeepStream Key FeaturesDeepStream-3D Sensor Fusion Multi-Modal Application and FrameworkDeepStream-3D Multi-Modal BEVFusion SetupDeepStream-3D Multi-Modal V2XFusion SetupSmart Video RecordIoTOn the Fly Model UpdateNTP Timestamp in DeepStreamAV Sync in DeepStreamDeepStream With REST API SeverDeepStream 3D Action Recognition AppDeepStream 3D Depth Camera AppDeepStream 3D Lidar Inference AppNetworked Media Open Specifications (NMOS) in DeepStreamGst-nvdspostprocess in DeepStreamDeepStream Can Orientation AppDeepStream Application MigrationApplication Migration to DeepStream 7.1 from DeepStream 7.0DeepStream Plugin GuideGStreamer Plugin OverviewMetaData in the DeepStream SDKGst-nvdspreprocess (Alpha)Gst-nvinferGst-nvinferserverGst-nvtrackerGst-nvstreammuxGst-nvstreammux NewGst-nvstreamdemuxGst-nvmultistreamtilerGst-nvdsosdGst-nvdsmetautilsGst-nvdsvideotemplateGst-nvdsaudiotemplateGst-nvvideoconvertGst-nvdewarperGst-nvofGst-nvofvisualGst-nvsegvisualGst-nvvideo4linux2Gst-nvjpegdecGst-nvimagedecGst-nvjpegencGst-nvimageencGst-nvmsgconvGst-nvmsgbrokerGst-nvdsanalyticsGst-nvdsudpsrcGst-nvdsudpsinkGst-nvdspostprocess (Alpha)Gst-nvds3dfilterGst-nvds3dbridgeGst-nvds3dmixerGst-NvDsUcxGst-nvdsxferGst-nvvideotestsrcGst-nvmultiurisrcbinGst-nvurisrcbinDeepStream Troubleshooting and FAQTroubleshootingFrequently Asked QuestionsDeepStream On WSL2DeepStream On WSLFAQ for Deepstream On WSLDeepStream API GuideDeepStream API GuidesDeepStream Service MakerWhat is Deepstream Service MakerService Maker for C/C++ DevelopersService Maker for Python Developers(alpha)Quick Start GuideIntroduction to Flow APIsIntroduction to Pipeline APIsAdvanced FeaturesMigrating Traditional Deepstream Apps to Service Maker Apps in PythonWhat is a Deepstream Service Maker PluginDeepstream LibrariesDeepStream Libraries (Developer Preview)Graph ComposerOverviewPlatformsSupported platformsGetting StartedApplication Development WorkflowCreating an AI ApplicationReference graphsExtension Development WorkflowDeveloping Extensions for DeepStreamDeepStream ComponentsGXF InternalsGXF InternalsGraph eXecution EngineGraph Execution EngineGraph Composer ContainersGraph Composer and GXF ContainersGXF Component InterfacesGXF Component InterfacesGXF Application API'sGXF App C++ APIsGXF App Python APIsGXF Runtime API'sGXF Core C++ APIsGXF Core C APIsGXF Core Python APIsExtension ManualExtensionsCudaExtensionGXF Stream SyncStandardExtensionPython CodeletsNetworkExtensionNvTritonExtSerializationExtensionMultimediaExtensionVideoEncoderExtensionVideoDecoderExtensionBehavior TreesUCX ExtensionHttpExtensionGrpcExtensionTensorRTExtensionNvDs3dProcessingExtNvDsActionRecognitionExtNvDsAnalyticsExtNvDsBaseExtNvDsCloudMsgExtNvDsConverterExtNvDsDewarperExtNvDsInferenceExtNvDsInferenceUtilsExtNvDsInterfaceExtNvDsMuxDemuxExtNvDsOpticalFlowExtNvDsOutputSinkExtNvDsSampleExtNvDsSampleModelsExtNvDsSourceExtNvDsTemplateExtNvDsTrackerExtNvDsTranscodeExtNvDsTritonExtNvDsUcxExtNvDsUdpExtNvDsVisualizationExtToolsRegistryRegistry Command Line InterfaceComposerContainer BuilderGXF Command Line InterfacePipetuner GuideFAQ GuideFAQDeepStream Legal InformationDeepStream End User License AgreementDeepStream FeedbackFeedback formAccuracy...Accuracy Tuning Tools#A typical DeepStream perception pipeline includes a detector and the multi-object tracker, and each module has a number of parameters listed in the detector (PGIE) and tracker configuration files. For example, the clustering thresholds for detection post-processing, the Kalman filter parameters in tracker, etc. When users deploy such data processing pipelines in diverse applications, such as traffic, retail, warehouse etc., a pain point is how to find the optimal parameters with the highest accuracy KPI for each use case. Manual parameter tuning requires in-depth knowledge on the algorithm and how each parameter would
affect the functionality. Given the large number of parameters, the complexity of such process would increase exponentially.Starting from DeepStream 7.0, a new toolPipeTuneris released for automatic accuracy tuning. It efficiently explores the (potentially very high-dimensional) parameter space and automatically finds the optimal parameters for the pipelines, which yields the highest
KPI on the dataset. The difference between automatic and traditional manual tuning can be summarized as follows:ApproachWorkflowPros/ConsRequirementsAutomatic tuning
using PipeTunerDownload PipeTuner, and prepare a dataset for the target use case.Define a DeepStream pipeline with required models to be used and initial parameters
in the config files. And then register a set of parameters to be tuned with a search range
for each parameter.Launch PipeTuner and it will automatically search the parameters with the highest accuracy KPI.Users are not required to have technical knowledge on the pipeline and its parameters.The automatic tuning algorithm find the best parameter set within the search range
that would yield the highest accuracy KPI.Users need to provide a dataset, which includes a few video streams with the resolution same as the one that will be used in the actual deployments,
and ground truth (i.e. bounding box and object IDs). The videos should have the same resolution,
represent the typical use case, and not be too long, otherwise the tuning turn-around time may take too long; we recommend 1-2 min in length.The pipeline tuning would be carried out through a number of iterations of DeepStream pipeline execution, which may take several hours depending on various
factors, such as the number of iterations (similar to epochs), the size of the dataset and the HW capabilities.Manual tuningRead the technical documentation to understand how each parameter affects the accuracy.Perform heuristic or random search for the parameters and check the result quality.Manually run DeepStream pipeline multiple times to compare which parameters work the best.If a user is very experienced/skilled in DeepStream tuning, then it could be quick fix.Quick fix on some parameters for a corner case may lead to a regression on other cases.Any manual tuning would be sub-optimal as exploring the high-dimensional parameter
space manual is not tractable.Users need special technical knowledge on the pipeline and its parameters, and need to go through a trial-and-error process.For more detailed tutorials, please check outPipeTuner User Guidefrom NGC for step-to-step setup instructions and all the technical details.
In case manual tuning is still needed, we also provide in the following sections the functionality of some of the configuration parameters to give a better understanding on their potential impacts on both performance and accuracy on multi-object tracking operations.PipeTuner for Automatic Tuning (Developer Preview)#Download#PipeTuner is hosted on NGC. Users need to download the following resources to start.PipeTuner Collection: The collection of all Pipetuner resources, including introduction, user guide and setup instructions;PipeTuner Container: PipeTuner docker container;PipeTuner User Guide and Sample Data: PipeTuner user guide and the sample data to run as an example, including a sample dataset for people tracking, configuration files for tuning, and scripts to launch the pipeline.Features and Requirements#Here is a summary of PipeTuner’s functionalities and requirements:DatasetUsers need to provide the typical dataset for their use case. It should include a few sample videos, with bounding box and object ID ground truth annotatedModelsUsers need to provide the required models, including the object detection model (for PGIE) and Re-ID models (Re-ID only required when using NvDCF_accuracy or NvDeepSORT tracker). They can be NVIDIA TAO models in
DeepStream container, NGC, or customized pre-trained ONNX modelsContainerUsers need to download PipeTuner and DeepStream container from NGCAccuracy KPIUsers select one of the multi-object tracking KPIs:HOTA,MOTAorIDF1Setup#Overall steps to setup PipeTuner are as below.Download Container: Pull PipeTuner and DeepStream perception container from NGC repository;Download Sample Data: Download and extract sample data from NGC resource;Data Preparation: Users create their own dataset with the same format as sample data, and update configuration files to match their use case;Launch Tuning: Launch the tuning pipeline using the desired configuration and data;Retrieve Results: Retrieve the optimal parameters and visualize the tuning results;Deploy: Deploy the optimal parameters into the desired use case.PipeTuner searches the optimal parameters by iterating the following three steps until the
accuracy KPI converges or up to the max number of iterations (i.e., epochs) specified:ParamSearch: Given the accuracy KPI score in the previous iteration, make an educated guess on the set of parameters that would yield a higher accuracy KPI. For the very first iteration, a random sampling in the parameter space would be conducted;PipeExec: Given the sampled/guessed parameter set, execute the pipeline with the params and generates metadata to allow accuracy evaluation;PipeEval: Given the metadata outputs from the pipeline and the dataset, perform the accuracy evaluation based on the accuracy metric and generates accuracy KPI score.Multi-Object Tracking Parameter Functionalities for Manual Tuning#This section describes the configuration parameters in each module of the detector and tracker, and their potential impacts on both performance and accuracy. A general introduction toNvMultiObjectTrackertracker library can be found in DeepStream SDK Plugin Manual.Accuracy-Performance Tradeoffs#The visual feature size, detection interval and input frame size have impact on both accuracy and performance. They should be properly set for a good accuracy-performance tradeoff.Visual Feature Types and Feature Sizes#Related ParametersVisual feature typesuseColorNamesuseHogFeature sizesfeatureImgSizeLevelsearchRegionPaddingScaleNvDCF tracker can use multiple types of visual features such as Histogram of Oriented Gradient (HOG) and ColorNames. If both features are used (by settinguseColorNames:1anduseHog:1), then the total number of channels would be 28. The more channels of visual features are used, the more accurately the algorithm would track but would  increase the computational complexity and reduce performance.In addition to the types of the visual features, we can configure the number of pixels used to represent an object for each feature channel. The corresponding parameter isfeatureImgSizeLevel, and its range is from 1 to 5. Each level between 1 and 5 corresponds to 12x12, 18x18, 24x24, 36x36, and 48x48, respectively, for each feature channel. Therefore, if one uses both HOG and ColorNames withfeatureImgSizeLevel:5, then the dimension of visual features that represents an object would be 28x48x48.One thing to note is that the visual features for an object are extracted from a region whose size is a bit larger than the object region in order to make sure that the object in the next frame appears within the region even when there is a movement by the object between frames. This region is referred to asthe search region, whose size is defined by adding a degree of padding to the objectbbox. More details can be found in the section for NvDCF tracker in DeepStream Plugin Manual.Increasing the search region size lowers the probability of missing the object in the next frame; however, given a fixed feature size (i.e.,featureImgSizeLevel), if we increasesearchRegionPaddingScale, it would effectively decrease the number of pixels belonging to the object, resulting in lower resolution in terms of object representation in visual features. This may result in lower accuracy in tracking; however, if the degree of movement of an object between two consecutive frames is expected to be small, the object would be highly likely to appear in the search region in the next frame even with a smaller search region size. It would especially be the case if a state estimator is enabled and the prediction by the state estimator is reasonably accurate, because the search region would be defined at the predicted location in the next frame.Detection Interval#Related ParametersDetection intervalintervalInstead of reducing the visual feature types and sizes, users can explore increasing the detection interval instead (i.e.,intervalin PGIE config). Thanks to the enhanced accuracy and robustness, the NvDCF tracker allows users to increase the detection interval without sacrificing the accuracy too much. Especially when a heavier neural net model is used for the object detection, the performance gain by increasing the detection interval will be higher. Thus, users may consider increasing the detection interval instead of lowering the accuracy setting for NvDCF tracker.Video Frame Size for Tracker#Related ParametersVideo frame size for trackertracker-widthtracker-heightThe video frame size configured in tracker plugin has some impact on the performance, as a higher resolution video frame would take longer time to transfer between memories. If one sets the frame resolution lower, hoping to achieve a higher performance, however, its negative impact on the accuracy may outweigh the performance gain. Therefore, it is recommended to use at least 960x544 resolution (for 1080p source resolution) to minimize the accuracy degradation.Robustness#To deal with false positives and false negatives from the detector, theNvMultiObjectTrackerlibrary utilizes two strategies calledLate ActivationandShadow Tracking(more details can be found in DeepStream SDK Plugin Manual). In addition to the config parameters related to those strategies, there are a few config parameters that affect when a tracker for a new object is created and terminated.Target Creation Policy#Related ParametersTarget CandidacyminDetectorConfidenceminIouDiff4NewTargetLate ActivationprobationAgeearlyTerminationAgeIf an object detected by a detector meets the minimum qualifications (i.e., target candidacy) specified by the following, a new tracker is instantiated for the object:minDetectorConfidenceminIouDiff4NewTargetIf spurious false detections are observed with lower detector confidence values, one can increase the minimum detector confidence (i.e.,minDetectorConfidence) to filter them out. If the maxmimum IOU score of a newly detected object to any of the existing targets is lower thanminIouDiff4NewTarget, a new target tracker would be created to track the object. Thus, if one wishes to further suppress the creation of duplicate bboxes on the same target that may have a bit different bbox sizes,minIouDiff4NewTargetcan be set lower.Once a tracker is instantiated for a new object, it initially starts tracking the object in a temporary mode (i.e.,Tentativemode) until further criteria are met during a period specified byprobationAgein terms of the number of frames. During this probationary period, whenever the tracker bbox is not matched with detector bbox or the tracker confidence gets lower thanminTrackerConfidence, the shadow tracking age (which is an internal variable) is incremented. If the shadow tracking age reaches a predefined threshold (i.e.,earlyTerminationAge), then the tracker will be terminated prematurely, effectively eliminating the false positives.If a higher rate of false detections is expected, then one may consider to increase theprobationAgeand/or decreaseearlyTerminationAgefor stricter creation policy. If the expected detector confidence for the false detections is low while that of the true positives is high, one can setminDetectorConfidenceaccordingly to filter out false detections.Target Termination Policy#Related ParametersShadow TrackingminTrackerConfidencemaxShadowTrackingAgeIn addition to the aforementioned early termination policy during the probationary period, there are certain criteria to be met when a tracker is terminated. Once a tracker starts tracking inActivemode, its status changes toInactivemode if:The tracker confidence is lower thanminTrackerConfidenceorIt is not matched with a detector bbox during data association.The shadow tracking age is incremented every frame when a target is not associated with a detector object. If the tracker gets matched again with a detector bbox, then the shadow tracking age is reset to zero, and the tracker’s mode changes toActivemode again if it was inInactivemode (meaning that the tracker outputs will be reported to the downstream). However, if the shadow tracking age exceeds a predefined threshold (i.e.,maxShadowTrackingAge), the tracker will be terminated.For more robust tracking, one may increase the value formaxShadowTrackingAgebecause it will allow an object to be re-associated even after missed detections over multiple consecutive frames. However, in case that the visual appearance of the object undergoes a significant change during the missed detections (e.g., prolonged occlusions), the learned correlation filter may not yield a high correlation response when the object reappears. In addition, increasingmaxShadowTrackingAgewould allow a tracker to live longer (i.e., more delayed termination), resulting in an increased number of trackers present at the memory at a given time, which would in turn increase the computational load.State Estimation#An object tracker inNvMultiObjectTrackerlibrary maintains a set of states for a target like below:Target location (in 2D camera coordinates)LocationLocation velocityTarget BboxSizeSize velocityKalman Filter#Related ParametersprocessNoiseVar4LocprocessNoiseVar4SizeprocessNoiseVar4VelmeasurementNoiseVar4DetectormeasurementNoiseVar4TrackerThe Kalman Filter (KF) implementation inNvMultiObjectTrackerlibrary mostly follows a standard 2D KF approach where the user needs to define the process noise and measurement noise based on the expected uncertainty level. If the object has relatively simple and linear motion, one may set the process noise lower than the measurement noise, effectively putting more trust on the prediction. If the object is expected to have more dynamic motions or abrupt changes of states, it would be more advised to set the measurement noise lower; otherwise, there could be some lagging if the prediction is not correct.One additional consideration that is put in is to allow users to set different measurement noise for detectorbboxand trackerbboxfor the case where a visual tracker module is enabled (i.e., NvDCF). There is always a possibility of false negatives by the detector or there could be video frames where the inference for object detection is skipped. For such cases, each object tracker makes its own localization using the learned correlation filter, and the results are used to update the Kalman filter. Thus, from KF’s point of view, the measurements are from two different sources: one from the detector and the other from the tracker. In cases that the measurements are expected from multiple sources, such measurements are expected to be fused to estimate the target states properly with appropriate measurement models (i.e., uncertainty modeling for the measurements).Depending on the accuracy characteristics of the detector and the tracker, the measurement noises should be configured accordingly. When a very high accuracy model is used for object detection, one may setmeasurementNoiseVar4Detectorvalue lower thanmeasurementNoiseVar4Tracker, effectively putting more trust on the detector’s measurement than the tracker’s prediction/localization.Data Association#Related ParametersMatching CandidacyminMatchingScore4OverallminMatchingScore4SizeSimilarityminMatchingScore4IouminMatchingScore4VisualSimilarityMatching Score WeightsmatchingScoreWeight4VisualSimilaritymatchingScoreWeight4SizeSimilaritymatchingScoreWeight4IouIn the video frames where the detector performs inference (referred to as the inference frames), the NvDCF tracker performs the data association to match a set of detector objects to a set of existing targets. To reduce the computational cost for matching, it is essential to define a small set of good candidates for each object tracker. That is where the criteria for matching candidacy comes in. For each trackerbbox, only the detectorbboxesthat are qualified in terms of the minimum size similarity, IOU, and the visual similarity are marked as candidates for matching. The visual similarity is computed based on the correlation response of the tracker at the detectorbboxlocation. If one wants to consider only the detectorbboxesthat have at least some overlap with the trackerbbox, for example, thenminMatchingScore4Iouwould need to be set with a non-zero value. One can tune the other parameters in a similar manner.Given a set of candidate detectorbboxesfor each tracker, the data association matrix is constructed between the detectorbboxset and the tracker set with the matching scores as the value for the elements in the matrix. The matching score for each element is computed as a weighted sum of:The visual similarityThe size similarity, andIOU score with the corresponding weights inmatchingScoreWeight4VisualSimilarity,matchingScoreWeight4SizeSimilarity, andmatchingScoreWeight4Iou, respectively.The resulting matching score is put into the data association matrix only if the score exceeds a predefined threshold (i.e.,minMatchingScore4Overall)DCF Core Parameters#Apart from the types and sizes of the visual features employed, there are parameters related to how to learn and update the classifier for each object in DCF frameworks, which would affect the accuracy.DCF Filter Learning#Related ParametersfilterLrfilterChannelWeightsLrgaussianSigmaDCF-based trackers learn a classifier (i.e., discriminative correlation filter) for each object with implicit positive and negative samples. Such learned classifiers are updated on-the-fly for temporal consistency with a predefined learning rate (i.e.,filterLr). If the visual appearance of the target objects is expected to vary quickly over time, one may employ a high learning rate for better adaptation of the correlation filter to the changing appearance. However, there is a risk of learning the background quickly as well, resulting in potentially more frequent track drift.As NvDCF tracker utilizes multi-channel visual features, it is of concern on how to merge those channels for the final correlation response. NvDCF employs an adaptive channel weight approach where the importance of each channel is examined on-the-fly, and the corresponding channel weights are updated over time with a pre-defined learning rate (i.e.,filterChannelWeightsLr). The tuning strategy for this learning rate would be similar to the case offilterLras described before.When a correlation filter is learned,gaussianSigmadetermines how tight we want to fit the resulting filter to the positive sample. A lower value means the tighter fit, but it may result in overfitting. On the other hand, a higher value may result in lower discriminative power in the learned filter.See also theTroubleshooting in Tracker Setup and Parameter Tuningsection for solutions to common problems in tracker behavior and tuning.previousPerformancenextUsing a Custom Model with DeepStreamOn this pagePipeTuner for Automatic Tuning (Developer Preview)DownloadFeatures and RequirementsSetupMulti-Object Tracking Parameter Functionalities for Manual TuningAccuracy-Performance TradeoffsVisual Feature Types and Feature SizesDetection IntervalVideo Frame Size for TrackerRobustnessTarget Creation PolicyTarget Termination PolicyState EstimationKalman FilterData AssociationDCF Core ParametersDCF Filter LearningPrivacy Policy|Manage My Privacy|Do Not Sell or Share My Data|Terms of Service|Accessibility|Corporate Policies|Product Security|ContactCopyright © 2024-2025, NVIDIA Corporation.Last updated on Jan 13, 2025.