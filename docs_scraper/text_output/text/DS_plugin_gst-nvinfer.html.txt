Gst-nvinfer — DeepStream documentationSkip to main contentBack to topCtrl+KDeepStream documentationDeepStream documentationTable of ContentsDeepStream Getting StartedWelcome to the DeepStream DocumentationMigration GuideInstallationQuickstart GuideDocker ContainersDeepStream SamplesC/C++ Sample Apps Source DetailsPython Sample Apps and Bindings Source DetailsDeepStream Reference Application - deepstream-appDeepStream Reference Application - deepstream-test5 appDeepStream Reference Application - deepstream-nmos appDeepStream Reference Application on GitHubSample Configurations and StreamsImplementing a Custom GStreamer Plugin with OpenCV Integration ExampleTAO toolkit Integration with DeepStreamTAO Toolkit Integration with DeepStreamTutorials and How-to'sDeepStream-3D Custom Apps and Libs TutorialsDeepStream PerformancePerformanceDeepStream AccuracyAccuracy Tuning ToolsDeepStream Custom ModelUsing a Custom Model with DeepStreamDeepStream Key FeaturesDeepStream-3D Sensor Fusion Multi-Modal Application and FrameworkDeepStream-3D Multi-Modal BEVFusion SetupDeepStream-3D Multi-Modal V2XFusion SetupSmart Video RecordIoTOn the Fly Model UpdateNTP Timestamp in DeepStreamAV Sync in DeepStreamDeepStream With REST API SeverDeepStream 3D Action Recognition AppDeepStream 3D Depth Camera AppDeepStream 3D Lidar Inference AppNetworked Media Open Specifications (NMOS) in DeepStreamGst-nvdspostprocess in DeepStreamDeepStream Can Orientation AppDeepStream Application MigrationApplication Migration to DeepStream 7.1 from DeepStream 7.0DeepStream Plugin GuideGStreamer Plugin OverviewMetaData in the DeepStream SDKGst-nvdspreprocess (Alpha)Gst-nvinferGst-nvinferserverGst-nvtrackerGst-nvstreammuxGst-nvstreammux NewGst-nvstreamdemuxGst-nvmultistreamtilerGst-nvdsosdGst-nvdsmetautilsGst-nvdsvideotemplateGst-nvdsaudiotemplateGst-nvvideoconvertGst-nvdewarperGst-nvofGst-nvofvisualGst-nvsegvisualGst-nvvideo4linux2Gst-nvjpegdecGst-nvimagedecGst-nvjpegencGst-nvimageencGst-nvmsgconvGst-nvmsgbrokerGst-nvdsanalyticsGst-nvdsudpsrcGst-nvdsudpsinkGst-nvdspostprocess (Alpha)Gst-nvds3dfilterGst-nvds3dbridgeGst-nvds3dmixerGst-NvDsUcxGst-nvdsxferGst-nvvideotestsrcGst-nvmultiurisrcbinGst-nvurisrcbinDeepStream Troubleshooting and FAQTroubleshootingFrequently Asked QuestionsDeepStream On WSL2DeepStream On WSLFAQ for Deepstream On WSLDeepStream API GuideDeepStream API GuidesDeepStream Service MakerWhat is Deepstream Service MakerService Maker for C/C++ DevelopersService Maker for Python Developers(alpha)Quick Start GuideIntroduction to Flow APIsIntroduction to Pipeline APIsAdvanced FeaturesMigrating Traditional Deepstream Apps to Service Maker Apps in PythonWhat is a Deepstream Service Maker PluginDeepstream LibrariesDeepStream Libraries (Developer Preview)Graph ComposerOverviewPlatformsSupported platformsGetting StartedApplication Development WorkflowCreating an AI ApplicationReference graphsExtension Development WorkflowDeveloping Extensions for DeepStreamDeepStream ComponentsGXF InternalsGXF InternalsGraph eXecution EngineGraph Execution EngineGraph Composer ContainersGraph Composer and GXF ContainersGXF Component InterfacesGXF Component InterfacesGXF Application API'sGXF App C++ APIsGXF App Python APIsGXF Runtime API'sGXF Core C++ APIsGXF Core C APIsGXF Core Python APIsExtension ManualExtensionsCudaExtensionGXF Stream SyncStandardExtensionPython CodeletsNetworkExtensionNvTritonExtSerializationExtensionMultimediaExtensionVideoEncoderExtensionVideoDecoderExtensionBehavior TreesUCX ExtensionHttpExtensionGrpcExtensionTensorRTExtensionNvDs3dProcessingExtNvDsActionRecognitionExtNvDsAnalyticsExtNvDsBaseExtNvDsCloudMsgExtNvDsConverterExtNvDsDewarperExtNvDsInferenceExtNvDsInferenceUtilsExtNvDsInterfaceExtNvDsMuxDemuxExtNvDsOpticalFlowExtNvDsOutputSinkExtNvDsSampleExtNvDsSampleModelsExtNvDsSourceExtNvDsTemplateExtNvDsTrackerExtNvDsTranscodeExtNvDsTritonExtNvDsUcxExtNvDsUdpExtNvDsVisualizationExtToolsRegistryRegistry Command Line InterfaceComposerContainer BuilderGXF Command Line InterfacePipetuner GuideFAQ GuideFAQDeepStream Legal InformationDeepStream End User License AgreementDeepStream FeedbackFeedback formGStreamer Plugin OverviewGst-nvinferGst-nvinfer#The Gst-nvinfer plugin does inferencing on input data using NVIDIA®TensorRT™.The plugin accepts batchedNV12/RGBAbuffers from upstream. TheNvDsBatchMetastructure must already be attached to the Gst Buffers.
The low-level library (libnvds_infer) operates on any of INT8 RGB, BGR, or GRAY data with dimension of Network Height and Network Width.
The Gst-nvinfer plugin performs transforms (format conversion and scaling), on the input frame based on network requirements, and passes the transformed data to the low-level library.
The low-level library preprocesses the transformed frames (performs normalization and mean subtraction) and produces final floatRGB/BGR/GRAYplanar data which is passed to the TensorRT engine for inferencing. The output type generated by the low-level library depends on the network type.
The pre-processing function is:y=netscalefactor*(x-mean)Where:x is the input pixel value. It is an int8 with range [0,255].mean is the corresponding mean value, read either from the mean file or as offsets[c], where c is the channel to which the input pixel belongs, and offsets is the array specified in the configuration file. It is a float.net-scale-factor is the pixel scaling factor specified in the configuration file. It is a float.y is the corresponding output pixel value. It is a float.Gst-nvinfer currently works on the following type of networks:Multi-class object detectionMulti-label classificationSegmentation (semantic)Instance SegmentationThe Gst-nvinfer plugin can work in three modes:Primary mode: Operates on full framesSecondary mode: Operates on objects added in the meta by upstream componentsPreprocessed Tensor Input mode: Operates on tensors attached by upstream componentsWhen operating in preprocessed tensor input mode, the pre-processing inside Gst-nvinfer is completely
skipped. The plugin looks forGstNvDsPreProcessBatchMetaattached to the input
buffer and passes the tensor as is to TensorRT inference function without any
modifications. This mode currently supports processing on full-frame and ROI. The
GstNvDsPreProcessBatchMeta is attached by the Gst-nvdspreprocess plugin.When the plugin is operating as a secondary classifier along with the tracker, it tries to improve performance by avoiding re-inferencing on the same objects in every frame. It does this by caching the classification output in a map with the object’s unique ID as the key. The object is inferred upon only when it is first seen in a frame (based on its object ID) or when the size (bounding box area) of the object increases by 20% or more. This optimization is possible only when the tracker is added as an upstream element.
Detailed documentation of the TensorRT interface is available at:https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.htmlThe plugin supports the IPlugin interface for custom layers. Refer to section IPlugin Interface for details.
The plugin also supports the interface for custom functions for parsing outputs of object detectors and initialization of non-image input layers in cases where there is more than one input layer.
Refer to sources/includes/nvdsinfer_custom_impl.h for the custom method implementations for custom models.Downstream components receive a Gst Buffer with unmodified contents plus the metadata created from the inference output of the Gst-nvinfer plugin.
The plugin can be used for cascaded inferencing. That is, it can perform primary inferencing directly on input data, then perform secondary inferencing on the results of primary inferencing, and so on. See the sample application deepstream-test2 for more details.Inputs and Outputs#This section summarizes the inputs, outputs, and communication facilities of the Gst-nvinfer plugin.InputsGst BufferNvDsBatchMeta (attaching NvDsFrameMeta)ONNXTAO Encoded Model and KeyOffline: Supports engine files generated by TAO Toolkit SDK Model convertersLayers: Supports all layers supported by TensorRT, see:https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html.Control parametersGst-nvinfer gets control parameters from a configuration file. You can specify this by setting the property config-file-path. For details, see Gst-nvinfer File Configuration Specifications. Other control parameters that can be set through GObject properties are:Batch sizeInference intervalAttach inference tensor outputs as buffer metadataAttach instance mask output as in object metadataThe parameters set through the GObject properties override the parameters in the Gst-nvinfer configuration file.OutputsGst BufferDepending on network type and configured parameters, one or more of:NvDsObjectMetaNvDsClassifierMetaNvDsInferSegmentationMetaNvDsInferTensorMetaFeatures#The following table summarizes the features of the plugin.Gst-nvinfer plugin features#FeatureDescriptionReleaseExplicit Full Dimension Network SupportRefer tohttps://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#work_dynamic_shapesfor more details.DS 5.0Non-maximum Suppression (NMS)New bounding box clustering algorithm.DS 5.0On-the-fly model update (Engine file only)Update the model-engine-file on-the-fly in a running pipeline.DS 5.0Configurable frame scaling paramsConfigurable options to select the compute hardware and the filter to use while scaling frame/object crops to network resolutionDS 5.0TAO toolkit encoded model support—DS 4.0Gray input model supportSupport for models with single channel gray inputDS 4.0Tensor output as metaRaw tensor output is attached as meta data to Gst Buffers and flowed through the pipelineDS 4.0Segmentation modelSupports segmentation modelDS 4.0Maintain input aspect ratioConfigurable support for maintaining aspect ratio when scaling input frame to network resolutionDS 4.0Custom cuda engine creation interfaceInterface for generating CUDA engines from TensorRT INetworkDefinition and IBuilder APIs instead of model filesDS 4.0ONNX Model support—DS 3.0Multiple modes of operationSupport for cascaded inferencingDS 2.0Asynchronous mode of operation for secondary inferencingInfer asynchronously for secondary classifiersDS 2.0Grouping using CV::Group rectanglesFor detector bounding box clusteringDS 2.0Configurable batch-size processingUser can configure batch size for processingDS 2.0No Restriction on number of output blobsSupports any number of output blobsDS 3.0Configurable number of detected classes (detectors)Supports configurable number of detected classesDS 3.0Support for Classes: configurable (> 32)Supports any number of classesDS 3.0Application access to raw inference outputApplication can access inference output buffers for user specified layerDS 3.0Support for single shot detector (SSD)—DS 3.0Secondary GPU Inference Engines (GIEs) operate as detector on primary bounding boxSupports secondary inferencing as detectorDS 2.0Multiclass secondary supportSupports multiple classifier network outputsDS 2.0Grouping using DBSCANFor detector bounding box clusteringDS 3.0Loading an external lib containing IPlugin implementation for custom layers (IPluginCreator & IPluginFactory)Supports loading (dlopen()) a library containing IPlugin implementation for custom layersDS 3.0Multi GPUSelect GPU on which we want to run inferenceDS 2.0Detection width height configurationFilter out detected objects based on min/max object size thresholdDS 2.0Allow user to register custom parserSupports final output layer bounding box parsing for custom detector networkDS 2.0Bounding box filtering based on configurable object sizeSupports inferencing in secondary mode objects meeting min/max size thresholdDS 2.0Configurable operation intervalInterval for inferencing (number of batched buffers skipped)DS 2.0Select Top and bottom regions of interest (RoIs)Removes detected objects in top and bottom areasDS 2.0Operate on Specific object type (Secondary mode)Process only objects of define classes for secondary inferencingDS 2.0Configurable blob names for parsing bounding box (detector)Support configurable names for output blobs for detectorsDS 2.0Allow configuration file inputSupport configuration file as input (mandatory in DS 3.0)DS 2.0Allow selection of class id for operationSupports secondary inferencing based on class IDDS 2.0Support for Full Frame Inference: Primary as a classifierCan work as classifier as well in primary modeDS 2.0Multiclass secondary supportSupport multiple classifier network outputsDS 2.0Secondary GIEs operate as detector on primary bounding box 
Support secondary inferencing as detector—DS 2.0Supports FP16, FP32 and INT8 models
FP16 and INT8 are platform dependent—DS 2.0Supports TensorRT Engine file as input—DS 2.0Inference input layer initialization
Initializing non-video input layers in case of more than one input layers—DS 3.0Support for FasterRCNN—DS 3.0Support for Yolo detector (YoloV3/V3-tiny/V2/V2-tiny)—DS 4.0Support for yolov3-spp detector—DS 5.0Support Instance segmentation with MaskRCNNSupport for instance segmentation using MaskRCNN. It includes output parser and attach mask in object metadata.DS 5.0Support for NHWC network input—DS 6.0Added support for TAO ONNX model—DS 6.0Support for input tensor metaInferences using already preprocessed raw tensor from input tensor meta (attached as user meta at batch level) and skips preprocessing in nvinfer. In this mode, the batch-size of nvinfer must be equal to the sum of ROIs set in the gst-nvdspreprocess plugin config file.DS 6.0Support for clipping bounding boxes to ROI boundary—DS 6.2Gst-nvinfer File Configuration Specifications#The Gst-nvinfer configuration file uses a “Key File” format described inhttps://specifications.freedesktop.org/desktop-entry-spec/latest.
The[property]group configures the general behavior of the plugin. It is the only mandatory group.
The[class-attrs-all]group configures detection parameters for all classes.
The[class-attrs-<class-id>]group configures detection parameters for a class specified by<class-id>. For example, the[class-attrs-23]group configures detection parameters for class ID23. This type of group has the same keys as[class-attrs-all].
The following two tables respectively describe the keys supported for[property]groups and[class-attrs-…]groups.Gst-nvinfer Property Group Supported Keys#PropertyMeaningType and RangeExample
NotesNetwork Types/ Applicable to GIEs(Primary/­Secondary)num-detected-classesNumber of classes detected by the networkInteger, >0num-detected-classes=­91DetectorBothnet-scale-factorPixel normalization factor (ignored if input-tensor-meta enabled)Float, >0.0net-scale-factor=­0.031AllBothmodel-filePathname of the model file. Not required if model-engine-file is usedStringmodel-file=/home/­ubuntu/­modelAllBothproto-filePathname of the prototxt file. Not required if model-engine-file is usedStringproto-file=/home/­ubuntu/­model.prototxtAllBothint8-calib-filePathname of the INT8 calibration file for dynamic range adjustment with an FP32 modelStringint8-calib-file=­/home/­ubuntu/­int8_calibAllBothbatch-sizeNumber of frames or objects to be inferred together in a batchInteger, >0batch-size=30AllBothinput-tensor-from-metaUse preprocessed input tensors attached as metadata instead of preprocessing inside the plugin. If this is set, ensure that the batch-size of nvinfer is equal to the sum of ROIs set in the gst-nvdspreprocess plugin config file.Booleaninput-tensor-from-meta=1AllPrimarytensor-meta-pool-sizeSize of the output tensor meta poolInteger, >0tensor-meta-pool-size=20AllBothmodel-engine-filePathname of the serialized model engine fileStringmodel-engine-file=/home/­ubuntu/­model.engineAllBothonnx-filePathname of the ONNX model fileStringonnx-file=/home/­ubuntu/­model.onnxAllBothenable-dbscanIndicates whether to use DBSCAN or the OpenCV groupRectangles() function for grouping detected objects.
DEPRECATED. Use cluster-mode instead.Booleanenable-dbscan=1DetectorBothlabelfile-pathPathname of a text file containing the labels for the modelStringlabelfile-path=/home/­ubuntu/­model_labels.txtDetector & classifierBothmean-filePathname of mean data file in PPM format (ignored if input-tensor-meta enabled)Stringmean-file=­/home/­ubuntu/­model_meanfile.ppmAllBothgie-unique-idUnique ID to be assigned to the GIE to enable the application and other elements to identify detected bounding boxes and labelsInteger, >0gie-unique-id=2AllBothoperate-on-gie-idUnique ID of the GIE on whose metadata (bounding boxes) this GIE is to operate onInteger, >0operate-on-gie-id=1AllBothoperate-on-class-idsClass IDs of the parent GIE on which this GIE is to operate onSemicolon delimited integer arrayoperate-on-class-ids=1;2Operates on objects with class IDs 1, 2generated by parent GIEIf operate-on-class-ids is set to -1,it will operate on all class-idsAllBothintervalSpecifies the number of consecutive batches to be skipped for inferenceInteger, >0interval=1AllPrimaryinput-object-min-widthSecondary GIE infers only on objects with this minimum widthInteger, ≥0input-object-min-width=40AllSecondaryinput-object-min-heightSecondary GIE infers only on objects with this minimum heightInteger, ≥0input-object-min-height=40AllSecondaryinput-object-max-widthSecondary GIE infers only on objects with this maximum widthInteger, ≥0input-object-max-width=2560 disables the thresholdAllSecondaryinput-object-max-heightSecondary GIE infers only on objects with this maximum heightInteger, ≥0input-object-max-height=256AllBothnetwork-modeData format to be used by inferenceInteger
0: FP32
1: INT8
2: FP16
3: BESTnetwork-mode=0AllBothoffsetsArray of mean values of color components to be subtracted from each pixel. Array length must equal the number of color components in the frame. The plugin multiplies mean values by net-scale-factor.(ignored if input-tensor-meta enabled)Semicolon delimited float array, all values ≥0offsets=77.5 21.2AllBothparse-bbox-func-nameName of the custom bounding box parsing function. If not specified, Gst-nvinfer uses the internal function for the resnet model provided by the SDKStringparse-bbox-func-name=parse_bbox_resnetDetectorBothparse-bbox-instance-mask-func-nameName of the custom instance segmentation parsing function. It is mandatory for instance segmentation network as there is no internal function.Stringparse-bbox-instance-mask-func-name=NvDsInferParseCustomMrcnnTLTInstance SegmentationPrimarycustom-lib-pathAbsolute pathname of a library containing custom method implementations for custom modelsStringcustom-lib-path=/home/­ubuntu/­libresnet_custom_impl.soAllBothmodel-color-formatColor format required by the model (ignored if input-tensor-meta enabled)Integer
0: RGB
1: BGR
2: GRAYmodel-color-format=0AllBothclassifier-async-modeEnables inference on detected objects and asynchronous metadata attachments. Works only when tracker-ids are attached. Pushes buffer downstream without waiting for inference results. Attaches metadata after the inference results are available to next Gst Buffer in its internal queue.Booleanclassifier-async-mode=1ClassifierSecondaryprocess-modeMode (primary or secondary) in which the element is to operate on (ignored if input-tensor-meta enabled)Integer
1=Primary
2=Secondarygie-mode=1AllBothclassifier-thresholdMinimum threshold label probability. The GIE outputs the label having the highest probability if it is greater than this thresholdFloat, ≥0classifier-threshold=0.4ClassifierBothsecondary-reinfer-intervalRe-inference interval for objects, in framesInteger, ≥0secondary-reinfer-interval=15Detector & ClassifierSecondaryoutput-tensor-metaGst-nvinfer attaches raw tensor output as Gst Buffer metadata.Booleanoutput-tensor-meta=1AllBothoutput-instance-maskGst-nvinfer attaches instance mask output in object metadata.Booleanoutput-instance-mask=1Instance SegmentationPrimaryenable-dlaIndicates whether to use the DLA engine for inferencing.
Note: DLA is supported only on NVIDIA® Jetson AGX Orin™ and NVIDIA® Jetson Orin NX™. Currently work in progress.Booleanenable-dla=1AllBothuse-dla-coreDLA core to be used.
Note: Supported only on Jetson AGX Orin and Jetson Orin NX. Currently work in progress.Integer, ≥0use-dla-core=0AllBothnetwork-typeType of networkInteger0: Detector1: Classifier2: Segmentation3: Instance Segmentationnetwork-type=1AllBothmaintain-aspect-ratioIndicates whether to maintain aspect ratio while scaling input.Booleanmaintain-aspect-ratio=1AllBothsymmetric-paddingIndicates whether to pad image symmetrically while scaling input. DeepStream pads the images asymmetrically by default.Booleansymmetric-padding=1AllBothparse-classifier-func-nameName of the custom classifier output parsing function. If not specified, Gst-nvinfer uses the internal parsing function for softmax layers.Stringparse-classifier-func-name=­parse_bbox_softmaxClassifierBothcustom-network-configPathname of the configuration file for custom networks available in the custom interface for creating CUDA engines.Stringcustom-network-config=/home/­ubuntu/­network.configAllBothtlt-encoded-modelPathname of the TAO toolkit encoded model.Stringtlt-encoded-model=­/home/­ubuntu/­model.etltAllBothtlt-model-keyKey for the TAO toolkit encoded model.Stringtlt-model-key=abcAllBothsegmentation-thresholdConfidence threshold for the segmentation model to output a valid class for a pixel. If confidence is less than this threshold, class output for that pixel is −1.Float, ≥0.0segmentation-threshold=0.3Segmentation, Instance segmentationBothsegmentation-output-orderSegmentation network output layer orderInteger
0: NCHW
1: NHWCsegmentation-output-order=1SegmentationBothworkspace-sizeWorkspace size to be used by the engine, in MBInteger, >0workspace-size=45AllBothforce-implicit-batch-dimWhen a network supports both implicit batch dimension and full dimension, force the implicit batch dimension mode.Booleanforce-implicit-batch-dim=1AllBothengine-create-func-nameName of the custom TensorRT CudaEngine creation function. Refer to the “Custom Model Implementation Interface” section for detailsStringengine-create-func-name=NvDsInferYoloCudaEngineGetAllBothcluster-modeClustering algorithm to use. Refer to the next table for configuring the algorithm specific parameters. ReferClustering algorithms supported by nvinferfor more informationInteger
0: OpenCV groupRectangles()
1: DBSCAN
2: Non Maximum Suppression
3: DBSCAN + NMS Hybrid
4: No clusteringcluster-mode=2cluster-mode=4 for instance segmentationDetectorBothfilter-out-class-idsFilter out detected objects belonging to specified class-idsSemicolon delimited integer arrayfilter-out-class-ids=12scaling-filterThe filter to use for scaling frames / object crops to network resolution (ignored if input-tensor-meta enabled)Integer, refer to enum NvBufSurfTransform_Inter in nvbufsurftransform.h for valid valuesscaling-filter=1AllBothscaling-compute-hwCompute hardware to use for scaling frames / object crops to network resolution (ignored if input-tensor-meta enabled)Integer
0: Platform default – GPU (dGPU), VIC (Jetson)
1: GPU
2: VIC (Jetson only)scaling-compute-hw=2AllBothoutput-io-formatsSpecifies the data type and order for bound output layers. For layers not specified, defaults to FP32 and CHWSemi-colon separated list of format.
<output-layer1-name>:<data-type>:<order>;<output-layer2-name>:<data-type>:<order>data-type should be one of
[fp32, fp16, int32, int8]order should be one of [chw, chw2, chw4, hwc8, chw16, chw32]output-io-formats=conv2d_bbox:fp32:chw;conv2d_cov/Sigmoid:fp32:chwAllBothLayer-device-precisionSpecifies the device type and precision for any layer in the networkSemi-colon separated list of format.
<layer1-name>:<precision>:<device-type>;<layer2-name>:<precision>:<device-type>;precision should be one of
[fp32, fp16, int8]Device-type should be one of [gpu, dla]layer-device-precision=output_cov/Sigmoid:fp32:gpu;output_bbox/BiasAdd:fp32:gpu;AllBothnetwork-input-orderOrder of the network input layer (ignored if input-tensor-meta enabled)Integer
0:NCHW
1:NHWCnetwork-input-order=1AllBothclassifier-typeDescription of what the classifier doesString (alphanumeric, ‘-’ and ‘_’ allowed, no spaces)classifier-type=vehicletypeClassifierBothcrop-objects-to-roi-boundaryClip the object bounding boxes to fit within the specified ROI boundary.Booleancrop-objects-to-roi-boundary=1DetectorBothGst-nvinfer Class-attributes Group Supported Keys#NameDescriptionType and RangeExampleNotes(Primary/Secondary)thresholdDetection thresholdFloat, ≥0threshold=0.5Object detectorBothpre-cluster-thresholdDetection threshold to be applied prior to clustering operationFloat, ≥0pre-cluster-threshold=0.5Object detectorBothpost-cluster-thresholdDetection threshold to be applied post clustering operationFloat, ≥0post-cluster-threshold=0.5Object detectorBothepsEpsilon values for OpenCV grouprectangles() function and DBSCAN algorithmFloat, ≥0eps=0.2Object detectorBothgroup-thresholdThreshold value for rectangle merging for OpenCV grouprectangles() functionInteger, ≥0group-threshold=10 disables the clustering functionalityObject detectorBothminBoxesMinimum number of points required to form a dense region for DBSCAN algorithmInteger, ≥0minBoxes=10 disables the clustering functionalityObject detectorBothdbscan-min-scoreMinimum sum of confidence of all the neighbors in a cluster for it to be considered a valid cluster.Float, ≥0dbscan-min-score=0.7Object detectorBothnms-iou-thresholdMaximum IOU score between two proposals after which the proposal with the lower confidence will be rejected.Float, ≥0nms-iou-threshold=0.2Object detectorBothroi-top-offsetOffset of the RoI from the top of the frame. Only objects within the RoI are output.Integer, ≥0roi-top-offset=200Object detectorBothroi-bottom-offsetOffset of the RoI from the bottom of the frame. Only objects within the RoI are output.Integer, ≥0roi-bottom-offset=200Object detectorBothdetected-min-wMinimum width in pixels of detected objects to be output by the GIEInteger, ≥0detected-min-w=64Object detectorBothdetected-min-hMinimum height in pixels of detected objects to be output by the GIEInteger, ≥0detected-min-h=64Object detectorBothdetected-max-wMaximum width in pixels of detected objects to be output by the GIEInteger, ≥0detected-max-w=2000 disables the propertyObject detectorBothdetected-max-hMaximum height in pixels of detected objects to be output by the GIEInteger, ≥0detected-max-h=2000 disables the propertyObject detectorBothtopkKeep only top K objects with highest detection scores.Integer, ≥0.
-1 to disabletopk=10Object detectorBothNoteUFF model support is removed from TRT 10.3.Gst Properties#The values set through Gst properties override the values of properties in the configuration file. The application does this for certain properties that it needs to set programmatically.
The following table describes the Gst-nvinfer plugin’s Gst properties.Gst-nvinfer Gst Properties#PropertyMeaningType and RangeExample notesconfig-file-pathAbsolute pathname of configuration file for the Gst-nvinfer elementStringconfig-file-path=­config_infer_primary.txtprocess-modeInfer Processing Mode
1=Primary Mode
2=Secondary ModeInteger, 1 or 2process-mode=1unique-idUnique ID identifying metadata generated by this GIEInteger, | 0 to 4,294,967,295unique-id=1infer-on-gie-idSee operate-on-gie-id in the configuration file tableInteger,
0 to 4,294,967,295infer-on-gie-id=1operate-on-class-idsSee operate-on-class-ids in the configuration file tableAn array of colon- separated integers (class-ids)operate-on-class-ids=1:2:4filter-out-class-idsSee filter-out-class-ids in the configuration file tableSemicolon delimited integer arrayfilter-out-class-ids=1;2model-engine-fileAbsolute pathname of the pre-generated serialized engine file for the modeStringmodel-engine-file=­model_b1_fp32.enginebatch-sizeNumber of frames/objects to be inferred together in a batchInteger,
1 – 4,294,967,295batch-size=4IntervalNumber of consecutive batches to be skipped for inferenceInteger, 0 to 32interval=0gpu-idDevice ID of GPU to use for pre-processing/inference (dGPU only)Integer,
0-4,294,967,295gpu-id=1raw-output-file-writePathname of raw inference output fileBooleanraw-output-file-write=1raw-output-generated-callbackPointer to the raw output generated callback functionPointerCannot be set through gst-launchraw-output-generated-userdataPointer to user data to be supplied with raw-output-generated-callbackPointerCannot be set through gst-launchoutput-tensor-metaIndicates whether to attach tensor outputs as meta on GstBuffer.Booleanoutput-tensor-meta=0output-instance-maskGst-nvinfer attaches instance mask output in object metadata.Booleanoutput-instance-mask=1input-tensor-metaUse preprocessed input tensors attached as metadata instead of preprocessing inside the pluginBooleaninput-tensor-meta=1crop-objects-to-roi-boundaryClip the object bounding boxes to fit within the specified ROI boundaryBooleancrop-objects-to-roi-boundary=1Clustering algorithms supported by nvinfer#cluster-mode = 0 | GroupRectangles#GroupRectangles is a clustering algorithm from OpenCV library which clusters rectangles of similar size and location using the rectangle equivalence criteria. Link to API documentation -https://docs.opencv.org/3.4/d5/d54/group__objdetect.html#ga3dba897ade8aa8227edda66508e16ab9cluster-mode = 1 | DBSCAN#Density-based spatial clustering of applications with noise or DBSCAN is a clustering algorithm which which identifies clusters by checking if a specific rectangle has a minimum number of neighbors in its vicinity defined by the eps value. The algorithm further normalizes each valid cluster to a single rectangle which is outputted as valid bounding box if it has a confidence greater than that of the threshold.cluster-mode = 2 | NMS#Non maximum suppression or NMS is a clustering algorithm which filters overlapping rectangles based on a degree of overlap(IOU) which is used as threshold. Rectangles with the highest confidence score is first preserved while the rectangles which overlap greater than the threshold are removed iteratively.cluster-mode = 3 | Hybrid#Hybrid clustering algorithm is a method which uses both DBSCAN and NMS algorithms in a two step process. DBSCAN is first applied to form unnormalized clusters in proposals whilst removing the outliers. NMS is later applied on these clusters to select the final rectangles for output.cluster-mode=4 | No clustering#No clustering is applied and all the bounding box rectangle proposals are returned as it is.Tensor Metadata#The Gst-nvinfer plugin can attach raw output tensor data generated by a TensorRT inference engine as metadata. It is added as anNvDsInferTensorMetain theframe_user_meta_listmember ofNvDsFrameMetafor primary (full frame) mode, or in theobj_user_meta_listmember ofNvDsObjectMetafor secondary (object) mode.To read or parse inference raw tensor data of output layers#Enable propertyoutput-tensor-metaor enable the same-named attribute in the configuration file for the Gst-nvinfer plugin.When operating as primary GIE,` NvDsInferTensorMeta` is attached to each frame’s (eachNvDsFrameMetaobject’s)frame_user_meta_list. When operating as secondary GIE,NvDsInferTensorMetais attached to each eachNvDsObjectMetaobject’sobj_user_meta_list.Metadata attached by Gst-nvinfer can be accessed in a GStreamer pad probe attached downstream from the Gst-nvinfer instance.TheNvDsInferTensorMetaobject’s metadata type is set toNVDSINFER_TENSOR_OUTPUT_META. To get this metadata you must iterate over theNvDsUserMetauser metadata objects in the list referenced byframe_user_meta_listorobj_user_meta_list.For more information about Gst-infer tensor metadata usage, see the source code insources/apps/sample_apps/deepstream_infer_tensor_meta-test.cpp, provided in the DeepStream SDK samples.Segmentation Metadata#The Gst-nvinfer plugin attaches the output of the segmentation model as user meta in an instance ofNvDsInferSegmentationMetawith meta_type set toNVDSINFER_SEGMENTATION_META. The user meta is added to theframe_user_meta_listmember ofNvDsFrameMetafor primary (full frame) mode, or theobj_user_meta_listmember ofNvDsObjectMetafor secondary (object) mode.
For guidance on how to access user metadata, seeUser/Custom Metadata Addition inside NvDsBatchMetaandTensor Metadatasections.previousGst-nvdspreprocess (Alpha)nextGst-nvinferserverOn this pageInputs and OutputsFeaturesGst-nvinfer File Configuration SpecificationsGst PropertiesClustering algorithms supported by nvinfercluster-mode = 0 | GroupRectanglescluster-mode = 1 | DBSCANcluster-mode = 2 | NMScluster-mode = 3 | Hybridcluster-mode=4 | No clusteringTensor MetadataTo read or parse inference raw tensor data of output layersSegmentation MetadataPrivacy Policy|Manage My Privacy|Do Not Sell or Share My Data|Terms of Service|Accessibility|Corporate Policies|Product Security|ContactCopyright © 2024-2025, NVIDIA Corporation.Last updated on Jan 13, 2025.