Application Development Workflow — DeepStream documentationSkip to main contentBack to topCtrl+KDeepStream documentationDeepStream documentationTable of ContentsDeepStream Getting StartedWelcome to the DeepStream DocumentationMigration GuideInstallationQuickstart GuideDocker ContainersDeepStream SamplesC/C++ Sample Apps Source DetailsPython Sample Apps and Bindings Source DetailsDeepStream Reference Application - deepstream-appDeepStream Reference Application - deepstream-test5 appDeepStream Reference Application - deepstream-nmos appDeepStream Reference Application on GitHubSample Configurations and StreamsImplementing a Custom GStreamer Plugin with OpenCV Integration ExampleTAO toolkit Integration with DeepStreamTAO Toolkit Integration with DeepStreamTutorials and How-to'sDeepStream-3D Custom Apps and Libs TutorialsDeepStream PerformancePerformanceDeepStream AccuracyAccuracy Tuning ToolsDeepStream Custom ModelUsing a Custom Model with DeepStreamDeepStream Key FeaturesDeepStream-3D Sensor Fusion Multi-Modal Application and FrameworkDeepStream-3D Multi-Modal BEVFusion SetupDeepStream-3D Multi-Modal V2XFusion SetupSmart Video RecordIoTOn the Fly Model UpdateNTP Timestamp in DeepStreamAV Sync in DeepStreamDeepStream With REST API SeverDeepStream 3D Action Recognition AppDeepStream 3D Depth Camera AppDeepStream 3D Lidar Inference AppNetworked Media Open Specifications (NMOS) in DeepStreamGst-nvdspostprocess in DeepStreamDeepStream Can Orientation AppDeepStream Application MigrationApplication Migration to DeepStream 7.1 from DeepStream 7.0DeepStream Plugin GuideGStreamer Plugin OverviewMetaData in the DeepStream SDKGst-nvdspreprocess (Alpha)Gst-nvinferGst-nvinferserverGst-nvtrackerGst-nvstreammuxGst-nvstreammux NewGst-nvstreamdemuxGst-nvmultistreamtilerGst-nvdsosdGst-nvdsmetautilsGst-nvdsvideotemplateGst-nvdsaudiotemplateGst-nvvideoconvertGst-nvdewarperGst-nvofGst-nvofvisualGst-nvsegvisualGst-nvvideo4linux2Gst-nvjpegdecGst-nvimagedecGst-nvjpegencGst-nvimageencGst-nvmsgconvGst-nvmsgbrokerGst-nvdsanalyticsGst-nvdsudpsrcGst-nvdsudpsinkGst-nvdspostprocess (Alpha)Gst-nvds3dfilterGst-nvds3dbridgeGst-nvds3dmixerGst-NvDsUcxGst-nvdsxferGst-nvvideotestsrcGst-nvmultiurisrcbinGst-nvurisrcbinDeepStream Troubleshooting and FAQTroubleshootingFrequently Asked QuestionsDeepStream On WSL2DeepStream On WSLFAQ for Deepstream On WSLDeepStream API GuideDeepStream API GuidesDeepStream Service MakerWhat is Deepstream Service MakerService Maker for C/C++ DevelopersService Maker for Python Developers(alpha)Quick Start GuideIntroduction to Flow APIsIntroduction to Pipeline APIsAdvanced FeaturesMigrating Traditional Deepstream Apps to Service Maker Apps in PythonWhat is a Deepstream Service Maker PluginDeepstream LibrariesDeepStream Libraries (Developer Preview)Graph ComposerOverviewPlatformsSupported platformsGetting StartedApplication Development WorkflowCreating an AI ApplicationReference graphsExtension Development WorkflowDeveloping Extensions for DeepStreamDeepStream ComponentsGXF InternalsGXF InternalsGraph eXecution EngineGraph Execution EngineGraph Composer ContainersGraph Composer and GXF ContainersGXF Component InterfacesGXF Component InterfacesGXF Application API'sGXF App C++ APIsGXF App Python APIsGXF Runtime API'sGXF Core C++ APIsGXF Core C APIsGXF Core Python APIsExtension ManualExtensionsCudaExtensionGXF Stream SyncStandardExtensionPython CodeletsNetworkExtensionNvTritonExtSerializationExtensionMultimediaExtensionVideoEncoderExtensionVideoDecoderExtensionBehavior TreesUCX ExtensionHttpExtensionGrpcExtensionTensorRTExtensionNvDs3dProcessingExtNvDsActionRecognitionExtNvDsAnalyticsExtNvDsBaseExtNvDsCloudMsgExtNvDsConverterExtNvDsDewarperExtNvDsInferenceExtNvDsInferenceUtilsExtNvDsInterfaceExtNvDsMuxDemuxExtNvDsOpticalFlowExtNvDsOutputSinkExtNvDsSampleExtNvDsSampleModelsExtNvDsSourceExtNvDsTemplateExtNvDsTrackerExtNvDsTranscodeExtNvDsTritonExtNvDsUcxExtNvDsUdpExtNvDsVisualizationExtToolsRegistryRegistry Command Line InterfaceComposerContainer BuilderGXF Command Line InterfacePipetuner GuideFAQ GuideFAQDeepStream Legal InformationDeepStream End User License AgreementDeepStream FeedbackFeedback formApplication...Application Development Workflow#GXF Applications, also called as graphs, can be created by programming them in python / C++ and also in a no-code environment using the
Graph Composer tool. All of these approaches are described below.Python Application Development#The GXF framework provides a rich python API to create graphs with ease. The following section describes the workflow to create GXF applications in python.Python API Setup#Prerequisites(skip if already finished):After installing the Graph Composer runtime debian, GXF Python core APIs at/opt/nvidia/graph-composer/core.See InstallationSync user required GXF extensions using toolregistry.See Sync ExtensionsSteps:install user required GXF extensionsEg, for frequently used extensions:registryextninstall-eStandardExtensionregistryextninstall-ePythonCodeletExtensionregistryextninstall-eMultimediaExtensionregistryextninstall-eSampleExtensionThe extensions will be installed to default GXF workspace path at/var/tmp/gxf/.cache/gxf_workspace.1/var/tmp/gxf/.cache/gxf_workspace2└── gxf3├── core -> /opt/nvidia/graph-composer/core4├── multimedia5├── python_codelet6├── sample7└── stdAdd the workspace path to python pathSuggested practice:Create agxf.pthfile at/home/$USER/.local/lib/python<VERSION>/site-packages/gxf.pth, and add the GXF workspace path to this file.Example commands in python:importosimportsite# /var/tmp/gxf/.cache/gxf_workspacedefault_extensions_path=get_default_extensions_path()# /home/<USER_NAME>/.local/lib/python<VERSION>/site-packagesuser_site_packages_dir=site.getusersitepackages()# Construct the path to the .pth file# /home/<USER_NAME>/.local/lib/python<VERSION>/site-packages/gxf.pthpth_file_path=os.path.join(user_site_packages_dir,"gxf.pth")# Write the path to the .pth filewithopen(pth_file_path,"w")aspth_file:pth_file.write(default_extensions_path)print(f"Created gxf.pth file at{pth_file_path}with the following content:")print(default_extensions_path)NoteAlternatively, the GXF workspace can be added to python path by updating thePYTHONPATHenv variableexportPYTHONPATH=$PYTHONPATH:/var/tmp/gxf/.cache/gxf_workspaceComputeEntity#ComputeEntity automatically adds an entity in the graph with basic scheduling terms. A codelet implemented in c++ with python
binding can be added to a ComputeEntity.Steps to compose a graph:Create a GXF graph objectSet clock and scheduler to the graph objectAdd ComputeEntity with codeletConnect the transmitter and receiver from the ComputeEntityGraph to load extensions, run, and destroyExample graph:importosfromgxf.coreimportGraphfromgxfimportstdfromgxf.stdimportRealtimeClockfromgxf.stdimportGreedySchedulerfromgxf.stdimportComputeEntityfromgxf.sampleimportPingTxfromgxf.sampleimportPingRxdefmain():g=Graph()clock=std.set_clock(g,RealtimeClock(name='clock'))std.set_scheduler(g,GreedyScheduler(max_duration_ms=1000000,clock=clock))g.add(ComputeEntity("tx",count=5)).add_codelet(PingTx(clock=clock))g.add(ComputeEntity("rx")).add_codelet(PingRx())std.connect(g.tx.signal,g.rx.signal)g.load_extensions()g.run()g.destroy()if__name__=="__main__":main()PyComputeEntity#PyComputeEntity automatically adds an entity in the graph with basic scheduling terms. Python codelet can be added in a PyComputeEntity.Steps to compose a graph:Implement a python codelet by inheriting CodeletAdapterCreate a GXF graph objectSet clock and scheduler to the graph objectAdd PyComputeEntity with python codeletConnect the transmitter and receiver from the ComputeEntityGraph to load extensions, run, and destroyExample graph with Python codelet implement:importosimportnumpyasnpfromgxf.coreimportGraphimportgxf.stdasstdfromgxf.stdimportRealtimeClockfromgxf.stdimportGreedySchedulerfromgxf.python_codeletimportCodeletAdapterfromgxf.coreimportMessageEntityfromgxf.python_codeletimportPyComputeEntityfromgxf.stdimportTensor# implement a GXF CodeletclassPyPingTx(CodeletAdapter):def__init__(self,some_param="what"):super().__init__()self.txs=["tx"]self.some_param=some_paramdefstart(self):self.params=self.get_params()self.count=1passdeftick(self):msg=MessageEntity(self.context())t=Tensor(np.array([1+self.count,2+self.count,3+self.count]))Tensor.add_to_entity(msg,t)self.tx.publish(msg,1)print("PyPingTx "+self.name()+": Message Sent: "+str(self.count))self.count+=1returndefstop(self):pass# implement a GXF CodeletclassPyPingRx(CodeletAdapter):def__init__(self):super().__init__()self.rxs=["input"]defstart(self):self.count=1passdeftick(self):msg=self.input.receive()t=Tensor.get_from_entity(msg)assert(np.array_equal(np.array(t),[1+self.count,2+self.count,3+self.count]))print("PyPingRx "+self.name()+": Message Received: "+str(self.count))self.count+=1returndefstop(self):returndefmain():g=Graph()clock=std.set_clock(g,RealtimeClock(name='clock'))std.set_scheduler(g,GreedyScheduler(max_duration_ms=1000000,clock=clock))ptx=g.add(PyComputeEntity("PingTx",count=5))ptx.add_codelet("somename",PyPingTx(some_param="some_value"))prx=g.add(PyComputeEntity("PingRx",count=5))prx.add_codelet("codelet",PyPingRx())std.connect(g.PingTx.tx,prx.input)g.load_extensions()g.run()g.destroy()if__name__=="__main__":main()Tensor zero-copy with other frameworks#GXF supports zero-copy tensor data exchange between different deep learning frameworks and libraries, like NumPy, CuPy, PyTorch, Tensorflow, etc.seeTensorfor API detailsFrom other frameworks to GXF#importnumpyasnpimportcupyascpnp_tensor=np.random.rand(3,4)cp_tensor=cp.random.rand(3,4)fromgxf.stdimportTensorgxf_tensor_host=Tensor.as_tensor(np_tensor)gxf_tensor_cuda=Tensor.as_tensor(cp_tensor)Then the tensor data can travel in GXF graph wrapped by a message entity, eg gxf_msg_out in below exampleTensor.add_to_entity(gxf_msg_out,gxf_tensor_host,"out_tensor_on_host")Tensor.add_to_entity(gxf_msg_out,gxf_tensor_cuda,"out_tensor_on_cuda")From GXF to other frameworks#NumPy / CuPy asarray()np_tensor=np.asarray(gxf_tensor_host)cp_tensor=cp.asarray(gxf_tensor_cuda)NumPy / CuPy from_dlpack()np_tensor=np.from_dlpack(gxf_tensor_host)cp_tensor=cp.from_dlpack(gxf_tensor_cuda)Example Application#importosimportcupyascpimportnumpyasnpimportgxf.stdasstdfromgxf.coreimport(Graph,MessageEntity)fromgxf.stdimport(Entity,GPUDevice,GreedyScheduler,RealtimeClock,Tensor,)fromgxf.python_codeletimport(PyComputeEntity,CodeletAdapter,)classTensorPairGenerator(CodeletAdapter):"""Python codelet to generate a stream of tensors on tick()Transmitter:* host_outTransmits one message containing one pair of GXF Tensors zero-copied from NumPy, on every tick()* cuda_outTransmits one message containing one pair of GXF Tensors zero-copied from CuPy, on every tick()"""def__init__(self):super().__init__()self.txs=["host_out","cuda_out"]defstart(self):self.params=self.get_params()rows=self.params.get("rows",16)cols=self.params.get("cols",64)self.shape=(rows,cols)self.dtype=np.float32returndeftick(self):gxf_msg_out_cuda=MessageEntity(self.context())gxf_msg_out_host=MessageEntity(self.context())# Initialize a pair of tensors using NumPynp1=np.arange(self.shape[0]*self.shape[1],dtype=self.dtype).reshape(self.shape)np2=np.ascontiguousarray(np1.transpose())# Convert NumPy tensors to GXF tensors# GXF implements zero-copy from NumPy tensorfori,arrinenumerate([np1,np2]):gxf_tensor=Tensor.as_tensor(arr)Tensor.add_to_entity(gxf_msg_out_host,gxf_tensor,f"host_tensor{i+1}")# Initialize a pair of cuda tensors using CuPycp1=cp.asarray(np1)cp2=cp.asarray(np2)# Convert CuPy tensors to GXF tensors# GXF implements zero-copy from CuPy tensor, via gxf.Tensor as_tensor() and from_dlpack()fori,dev_arrinenumerate([cp1,cp2]):gxf_tensor_cuda=Tensor.as_tensor(dev_arr)Tensor.add_to_entity(gxf_msg_out_cuda,gxf_tensor_cuda,f"cuda_tensor{i+1}")self.host_out.publish(gxf_msg_out_host)self.cuda_out.publish(gxf_msg_out_cuda)returndefstop(self):passclassDotProduct(CodeletAdapter):"""Python codelet to do dot product of a pair of tensors.Receiver:* rxReceives one message containing one pair of GXF Tensors zero-copied from NumPy or CuPy, on every tick()Transmitter:* txTransmits one message containing one GXF Tensor zero-copied from NumPy or CuPy, on every tick()The `device` parameter can be set to either 'cpu' or 'gpu'."""def__init__(self):super().__init__()self.txs=["tx"]self.rxs=["rx"]defstart(self):self.params=self.get_params()# use NumPy or CuPy based on the 'device' parameterdevice=self.params.get("device","cpu")if(notisinstance(device,str)ordevice.lower()notin["cpu","gpu"]):raiseValueError("device parameter must be one of {'cpu', 'gpu'}")self.xp=cpifdevice=="gpu"elsenpdeftick(self):xp=self.xpgxf_msg_in=self.rx.receive()# Get GXF tensors from GXF graphgxf_tensors=Tensor.find_all_from_entity(gxf_msg_in)np.testing.assert_equal(len(gxf_tensors),2)# Convert GXF tensor to NumPy / CuPy tensor# GXF implements zero-copy interface to NumPy / CuPy tensor, via NumPy / CuPy asarray() or from_dlpack()xp_tensor_in0=xp.asarray(gxf_tensors[0])xp_tensor_in1=xp.asarray(gxf_tensors[1])# NumPy / CuPy libraries to process NumPy / CuPy tensorsxp_tensor_out=xp.dot(xp_tensor_in0,xp_tensor_in1)gxf_msg_out=MessageEntity(self.context())# Convert NumPy / CuPy tensor back to GXF Graph# GXF implements zero-copy from NumPy / CuPy tensor, via gxf.Tensor as_tensor() and from_dlpack()gxf_tensor=Tensor.as_tensor(xp_tensor_out)Tensor.add_to_entity(gxf_msg_out,gxf_tensor,"")self.tx.publish(gxf_msg_out)defstop(self):passclassVerifyEqual(CodeletAdapter):"""Python codelet to compare a GPU tensor to a CPU tensor to within a tolerance.Receiver:* host_inReceives one message containing one GXF Tensor zero-copied from NumPy, on every tick()* cuda_inReceives one message containing one GXF Tensor zero-copied from CuPy, on every tick()"""def__init__(self):super().__init__()self.rxs=["host_in","cuda_in"]defstart(self):self.params=self.get_params()deftick(self):gxf_msg_in_host=self.host_in.receive()gxf_msg_in_cuda=self.cuda_in.receive()# Get GXF tensors from GXF graphgxf_tensors_cuda=Tensor.find_all_from_entity(gxf_msg_in_cuda)np.testing.assert_equal(len(gxf_tensors_cuda),1)gxf_tensors_host=Tensor.find_all_from_entity(gxf_msg_in_host)np.testing.assert_equal(len(gxf_tensors_host),1)# Convert GXF tensor to NumPy / CuPy tensor# GXF implements zero-copy interface to NumPy / CuPy tensor, via NumPy / CuPy asarray() or from_dlpack()cp_tensor=cp.asarray(gxf_tensors_cuda[0])np_tensor=np.asarray(gxf_tensors_host[0])# NumPy / CuPy libraries to process NumPy / CuPy tensors# Check if the two arrays are element-wise equal within a tolerancecp.testing.assert_allclose(cp_tensor,np_tensor,rtol=1e-5)returndefstop(self):passclassDLPackSimpleApp:defrun(self,count=20):g=Graph()clock=std.set_clock(g,RealtimeClock(name="clock"))std.set_scheduler(g,GreedyScheduler(max_duration_ms=1000000,clock=clock))std.enable_job_statistics(g,clock=clock)# create the tensor generator entitysource_entity=g.add(PyComputeEntity("TensorPairGenerator",count=count))source_entity.add_codelet("tensor_pair_generator",TensorPairGenerator(),# Codelet own paramsrows=16,cols=64,)# create the host matrix multiply entityprocess_entity_host=g.add(PyComputeEntity("DotProductHost",count=-1))process_entity_host.add_codelet("host_dot_product",DotProduct(),# Codelet own paramsdevice="cpu",)# create the device matrix multiply entityprocess_entity_cuda=g.add(PyComputeEntity("DotProductCuda",count=-1))process_entity_cuda.add_codelet("cuda_dot_product",DotProduct(),# Codelet own paramsdevice="gpu",)# create the tensor verification entitysink_entity=g.add(PyComputeEntity("VerifyEqual",count=-1))sink_entity.add_codelet("verify_equal",VerifyEqual(),)std.connect(source_entity.host_out,process_entity_host.rx)std.connect(source_entity.cuda_out,process_entity_cuda.rx)std.connect(process_entity_host.tx,sink_entity.host_in)std.connect(process_entity_cuda.tx,sink_entity.cuda_in)# add a GPUDevice for use by the default entity groupg.add(Entity("GPU_0")).add(GPUDevice(name="GPU_0",dev_id=0))g.load_extensions(workspace=self._get_default_extensions_path())g.run_async()g.wait()g.destroy()# default gxf extension path base, /var/tmp/gxf/.cache/gxf_workspacedef_get_default_extensions_path(self):home_dir=os.path.expanduser("~")returnos.path.join(home_dir,".cache","gxf_workspace")if__name__=="__main__":app=DLPackSimpleApp()app.run()UCX Transmit / Receive#Transmit and receive data on remote host or device. Eg, System 1 transmit Tensor with device data to remote system 2.Noteentity UCXSource, UCXSink, and UCX are referred in the example appfuture App API wraps UCXSource, UCXSink, UCX entity implementation, and exposes more concise  higher level APIs for distributed executionGraph with UCX Tx#Remote process 1 sending tensor with device data, via UCX Transmitter.Local data with memory on host or device is transmitted via UCX context, UCX Transmitter to remote host or device.Below comopnents are needed on top of a regular graphUCX entity containing UCX contextUCXSink entity containing UCX TransmitterclassPyPingTx(CodeletAdapter):""" Python codelet to send a msg on tick()Python implementation of Ping Tx.Sends a message to the transmitter on every tick()"""def__init__(self):super().__init__()self.txs=["tx"]defstart(self):self.params=self.get_params()self.allocator=Allocator.get(self.context(),self.cid(),self.params["allocator"])self.shape=self.params.get("shape",[1,2])deftick(self):msg=MessageEntity(self.context())# add ones tensor allocated on cupycp_tensor=cp.ones(self.shape)gxf_tensor=Tensor.as_tensor(cp_tensor)Tensor.add_to_entity(msg,gxf_tensor,"ones_tensor")# add uninitialized tensor allocated by gxf allocatortensor_on_allocator=Tensor.add_to_entity(msg,"zeros_tensor")td=TensorDescription(name="zeros_tensor",storage_type=MemoryStorageType.kDevice,shape=Shape(self.shape),element_type=PrimitiveType.kFloat32,bytes_per_element=4)tensor_on_allocator.reshape(td,self.allocator)self.tx.publish(msg,1)returndefstop(self):passdefrun_ucx_tx_graph():g=Graph()g.set_severity(logging.DEBUG)clock=std.set_clock(g,RealtimeClock(name='clock'))std.enable_job_statistics(g,clock=clock)std.set_scheduler(g,GreedyScheduler(max_duration_ms=5000,clock=clock,stop_on_deadlock=False))g.add(Entity("mem_pool")).add(BlockMemoryPool("device_image_pool",storage_type=1,block_size=1920*1080*4,num_blocks=150,))# g.add(UCX("ucx", allocator=g.mem_pool.device_image_pool))g.add(UCX("ucx"))g.add(PyComputeEntity("PingTx",count=5)).add_codelet("pingtx",PyPingTx(),allocator=g.mem_pool.device_image_pool,shape=[2,3],)g.add(UCXSink("sink",count=-1,address="localhost",port=13338))std.connect(g.PingTx.tx,g.sink.input)g.load_extensions()g.run()g.destroy()if__name__=='__main__':run_ucx_tx_graph()Graph with UCX Rx#Remote process 2 receiving tensor with device data, via UCX Receiver.UCX context, UCX Receiver receives data onto local host or device from remote host or device.Below comopnents are needed on top of a regular graphUCX entity containing UCX contextUCXSource entity containing UCX ReceiverclassPyPingRx(CodeletAdapter):""" Python codelet to send a msg on tick()Python implementation of Ping Tx.Sends a message to the transmitter on every tick()"""def__init__(self):super().__init__()self.rxs=["rx"]defstart(self):self.params=self.get_params()self.shape_expected=self.params.get("shape_expected",[1,2])self.expected_ones=cp.ones(self.shape_expected)self.expected_zeros=cp.zeros(self.shape_expected)deftick(self):msg=self.rx.receive()ones_tensor=Tensor.get_from_entity(msg,"ones_tensor")actual_ones=cp.asarray(ones_tensor)cp.testing.assert_allclose(actual_ones,self.expected_ones,rtol=1e-5)print("Correctly received tensor from remote CuPy over UCX:\n"+str(actual_ones))zeros_tensor=Tensor.get_from_entity(msg,"zeros_tensor")actual_zeros=cp.asarray(zeros_tensor)cp.testing.assert_allclose(actual_zeros,self.expected_zeros,rtol=1e-5)print("Correctly received tensor from remote Allocator over UCX:\n"+str(actual_zeros))returndefstop(self):returndefrun_ucx_rx_graph():g=Graph()g.set_severity(logging.DEBUG)clock=std.set_clock(g,RealtimeClock(name="clock"))std.enable_job_statistics(g,clock=clock)std.set_scheduler(g,GreedyScheduler(max_duration_ms=5000,clock=clock,stop_on_deadlock=False))g.add(Entity("mem_pool")).add(BlockMemoryPool("device_image_pool",storage_type=1,block_size=1920*1080*4,num_blocks=150,))g.add(UCX("ucx",allocator=g.mem_pool.device_image_pool))g.add(UCXSource("source",address="localhost",port=13338))g.add(PyComputeEntity("PingRx",count=-1)).add_codelet("pingrx",PyPingRx(),allocator=g.mem_pool.device_image_pool,shape_expected=[2,3],)std.connect(g.source.output,g.PingRx.rx)g.load_extensions()g.run()g.destroy()if__name__=='__main__':run_ucx_rx_graph()UCXSource, UCXSink, and context implementation for reference#fromgxf.serializationimportEndpointfromgxf.multimediaimportVideoBufferfromgxf.coreimportEntityfromgxf.stdimportForwardfromgxf.stdimportDoubleBufferTransmitter,DoubleBufferReceiverfromgxf.stdimportDownstreamReceptiveSchedulingTerm,CountSchedulingTermfromgxf.stdimportUnboundedAllocatorfromgxf.stdimportMessageAvailableSchedulingTermclassUCXSource(Entity):"""UCXSource Entity containing all the requied components to receive data on ucx address:port"""def__init__(self,name,address="0.0.0.0",port=13337,count=-1,capacity=1,min_message_reception=1,allocator_type=None,**kwargs):super().__init__(name,True)self._address=addressself._port=portself._count=countself._capacity=capacityself._min_message_reception=min_message_receptionself._allocator_type=allocator_typeself._kwargs=kwargsself.add(UnboundedAllocator(name="allocator"))self.add(UcxSerializationBuffer(name="serialization_buffer",allocator=self.allocator))self.add(UcxReceiver(name="input",port=self._port,address=self._address,buffer=self.serialization_buffer))self.add(MessageAvailableSchedulingTerm(name='mast',receiver=self.input,min_size=min_message_reception))self.add(DoubleBufferTransmitter(name="output",capacity=capacity))# 'in' is a keyword in python. can't access as an attributeself.add(Forward(name="forward"))self.forward._params["in"]=self.inputself.forward._params["out"]=self.outputself.add(DownstreamReceptiveSchedulingTerm(name='drst',transmitter=self.output,min_size=min_message_reception))ifcount>=0:self.add(CountSchedulingTerm(name="cst",count=self.count))classUCXSink(Entity):"""UCXSink Entity containing all the required components to push data on a ucx address:port"""def__init__(self,name,address="0.0.0.0",port=13337,count=-1,capacity=1,min_message_available=1,allocator_type=None,**kwargs):super().__init__(name,True)self._address=addressself._port=portself._count=countself._capacity=capacityself._min_message_available=min_message_availableself._allocator_type=allocator_typeself._kwargs=kwargsself.add(UnboundedAllocator(name="allocator"))self.add(UcxSerializationBuffer(name="serialization_buffer",allocator=self.allocator))self.add(UcxTransmitter(name="output",port=self._port,buffer=self.serialization_buffer,receiver_address=self._address))self.add(DoubleBufferReceiver(name="input",capacity=capacity))# in is a keyword in python. can't access as an attributeself.add(Forward(name="forward"))self.forward._params["in"]=self.inputself.forward._params["out"]=self.outputself.add(MessageAvailableSchedulingTerm(name='mast',receiver=self.input,min_size=min_message_available))ifcount>=0:self.add(CountSchedulingTerm(name="cst",count=self._count))classUCX(Entity):"""UCX Entity requied to add UCXSource and UCXSink"""def__init__(self,name,allocator=None):super().__init__(name,True)ifnotallocator:allocator=self.add(UnboundedAllocator(name="allocator"))self.add(UcxComponentSerializer(name="component_serializer",allocator=allocator))self.add(UcxEntitySerializer(name="entity_serializer",component_serializers=[self.component_serializer]))self.add(UcxContext(name="ucx_context",serializer=self.entity_serializer))NoteOn top of existing Python API, Python App API will be available in future release.Python App API will be similar to C++ App API[Coming Soon] More sample applications will be available inGXF github repo https://github.com/NVIDIA-AI-IOT/deepstream_gxf_ref_appsC++ Application Development#The traditional method of creating GXF applications via YAML documents included composing entities in a yaml file which was executed by the
GXE runtime along with an extension manifest. The GXF Application layer simplifies this process by allowing a user to create applications programmatically.
The implementation of this layer is compiled in a shared dynamic librarylibgxf_app.soand packaged in the GXF runtime debian installers.
SeeGXF App C++ APIsfor the API reference of the C++ application layer.The basic building blocks of this layer include Graph Entity, Segment & Application.Graph Entity#A GXF graph consists of one or more entities composed of multiple components. These components typically include a codelet along with other components
like resources, scheduling terms and messages queues (transmitters and receivers). Thenvidia::gxf::GraphEntityclass simplifies the process of
creating and managing programmable graph entities in an application.Graph Entities in an application are created using thenvidia::gxf::Segment::makeEntity<T>(...)api. This is a templated api that accepts a parameter pack of arguments. The template
type is used to specify the type of codelet to be added in the entity and the parameter pack is used to specify the rest of the components to be added and also the
arguments for the codelet itself.Here is an example of an entity containing a PingTx codelet represented in a yamlThe example below creates a simple graph entity consisting of thenvidia::gxf::PingTxcodelet.// create a codelet to generate 10 messagesautotx_entity=makeEntity<PingTx>("Tx",makeTerm<CountSchedulingTerm>("count",Arg("count",10)));Scheduling terms are created with themakeTermapi and a list of parameter values are passed in the form of Args.// create an entity to copy messages from device to hostautocopier=makeEntity<TensorCopier>("Copier",Arg("allocator")=makeResource<UnboundedAllocator>("allocator"),Arg("mode",1));Resources like memory pools and allocators can be created with themakeResourceapi. In this example the TensorCopier codelet is being configured
with an UnboundedAllocator.The transmitter and receiver queues of an entity are not specified while creating an graph entity. These components are auto added by theconnectapis when
an entity is being connected with another. The scheduling terms related to the transmitter and receiver are also added automatically by theconnectapis.Segment#The segment interface is an individual runtime context consisting of one or more graph entities. Each segment has its own scheduler
and its corresponding clock component. TheSegment::Compose()api is a virtual function used to create the entities of a segment and customize
their properties. The Segment layer provides multiple api’s to make the process of creating graph entities simpler.A complex distributed application can consist of multiple segments working with each other. The intra segment connections between entities of a
segment make use of double buffer transmitters and receivers and all inter segment connections make of the UCX transmitters and receivers.Connecting Graph Entities#The simplest connection api accepts only the source and destination graph entities. For such entities, it is assumed that their interfaces are comparable.
i.e The sender entity must have a codelet with only oneParameter<Handle<Transmitter>>and the receiving entity must have a codelet with only one registeredParameter<Handle<Receiver>>. The scheduling terms related to the transmitter and receiver are also added automatically by theconnectapis. The names of
auto added message queues and scheduling terms are the same as the corresponding parameter key of the codelet to which it is connected to.// create a codelet to generate 10 messagesautotx_entity=makeEntity<PingTx>("Tx",makeTerm<CountSchedulingTerm>("count",Arg("count",10)));// create a codelet to receive the messagesautorx_entity=makeEntity<PingRx>("Rx");// add data flow connection tx -> rxconnect(tx_entity,rx_entity);For entities with codelets having multiple inputs and outputs, the connection mapping can be specified in the connect api.voidcompose()override{// create a codelet to generate 10 messagesautoleft_tx=makeEntity<PingTx>("Left Tx",makeTerm<PeriodicSchedulingTerm>("periodic",Arg("recess_period","50Hz")),makeTerm<CountSchedulingTerm>("count",Arg("count",100)));autoright_tx=makeEntity<PingTx>("Right Tx",makeTerm<PeriodicSchedulingTerm>("periodic",Arg("recess_period","50Hz")),makeTerm<CountSchedulingTerm>("count",Arg("count",100)));// create a codelet to receive the messagesautomulti_ping_rx=makeEntity<MultiPingRx>("Multi Rx");// add data flow connection tx -> rxconnect(left_tx,multi_ping_rx,PortPair{"signal","receivers"});connect(right_tx,multi_ping_rx,PortPair{"signal","receivers"});}Application#The Application class is a scaffolding layer to create applications imperatively in GXF. It provides a virtual compose() function where
individual building blocks of an application can be constructed, configured, and connected with each other. The Application class also provides
functions to set the configuration file, load extensions, create segments, and run the application.The application class extends the Segment class and provides all the benefits of the segment apis in the application layer as well.In its most simplest form, an application does not have any segments. It has multiple graph entities governed by a scheduler.
The following example creates two graph entities and connects them with each other.classPingSimpleApp:publicApplication{public:voidcompose()override{// create a codelet to generate 10 messagesautotx_entity=makeEntity<PingTx>("Tx",makeTerm<CountSchedulingTerm>("count",Arg("count",10)));// create a codelet to receive the messagesautorx_entity=makeEntity<PingRx>("Rx");// add data flow connection tx -> rxconnect(tx_entity,rx_entity);// configure the schedulersetScheduler(SchedulerType::kGreedy);}};intmain(intargc,char**argv){autoapp=create_app<PingSimpleApp>();app->loadExtensionManifest(kManifestFilename);app->compose();autoresult=app->run();returnToResultCode(result);}A manifest file in the above example is a YAML file with a single top-level entry ‘extensions’ followed by a
list of filenames of GXF extension shared libraries.Example:extensions:-gxf/std/libgxf_std.so-gxf/sample/libgxf_sample.soThe Application class supports different modes of execution, which are represented by theExecutionModeenum.The different modes are:kUnset: The default mode, which is unset.kSingleSegment: This mode is used when the application contains only one segment.kMultiSegment: This mode is used when the application contains multiple segments being executed in parallel in a single process.kDistributed: This mode is used when the application is distributed across multiple nodes, and the segments are executed in parallel on different nodes.Large applications with complex execution patterns can be broken down into a application containing multiple segments. The example below refactors
the single segment application into a multi segment application. Individual graph entities are composed in the segment, and those segments are
composed and configured in the application.classPingTxSegment:publicSegment{public:voidcompose()override{// create a codelet to generate 10 messagesautotx_entity=makeEntity<PingTx>("Tx",makeTerm<CountSchedulingTerm>("count",Arg("count",10)));// add a scheduler component and configure the clockautoscheduler=setScheduler<Greedy>(Arg("stop_on_deadlock",false),Arg("max_duration_ms",5000));}};classPingRxSegment:publicSegment{public:voidcompose()override{// create a codelet to receive the messagesautorx_entity=makeEntity<PingRx>("Rx");// add a scheduler component and configure the clockautoscheduler=setScheduler<Greedy>(Arg("max_duration_ms",5000),Arg("stop_on_deadlock",false));}};classPingSegmentApp:publicApplication{public:voidcompose()override{autotx_segment=createSegment<PingTxSegment>("TxSegment");autorx_segment=createSegment<PingRxSegment>("RxSegment");// add data flow connection tx -> rxconnect(tx_segment,rx_segment,{SegmentPortPair("Tx.signal","Rx.signal")});}};intmain(intargc,char**argv){autoapp=create_app<PingSegmentApp>();app->loadExtensionManifest(kManifestFilename);app->compose();autoresult=app->run();returnToResultCode(result);}Runtime Codelet Registration#The application layer further simplifies the process of creating apps by enabling runtime registration of codelets.
All the prior examples made use of pre-defined codelets from the sample extension which was pre-loaded by the application.The application layer also has support for registering new codelets on the fly as shown in the following example. A new codeletPingTxRuntimeis defined along with the sample application it is used in. The new codelet does not have to be wrapped in an
extension library to be used in the application.classPingTxRuntime:publicCodelet{public:virtual~PingTxRuntime()=default;gxf_result_tregisterInterface(Registrar*registrar)override{Expected<void>result;result&=registrar->parameter(signal_,"signal","Signal","Transmitter channel publishing messages to other graph entities");returnToResultCode(result);}gxf_result_ttick()override{automessage=Entity::New(context());if(!message){GXF_LOG_ERROR("Failure creating message entity.");returnmessage.error();}autoresult=signal_->publish(message.value());GXF_LOG_INFO("Message Sent: %d",this->count);this->count=this->count+1;returnToResultCode(message);}private:Parameter<Handle<Transmitter>>signal_;intcount=1;};classPingRuntimeApp:publicApplication{public:voidcompose()override{// create a codelet to generate 10 messagesautotx_entity=makeEntity<PingTxRuntime>("Tx",makeTerm<CountSchedulingTerm>("count",Arg("count",10)));// create a codelet to receive the messagesautorx_entity=makeEntity<PingRx>("Rx");// add data flow connection tx -> rxconnect(tx_entity,rx_entity);// configure the scheduler componentsetScheduler(SchedulerType::kGreedy);}};intmain(intargc,char**argv){autoapp=create_app<PingRuntimeApp>();app->loadExtensionManifest(kManifestFilename);app->compose();autoresult=app->run();returnToResultCode(result);}Distributed Application#A multi segment application, can also be distributed across multiple nodes in a distributed application.
As introduced inDistributedExecutionsection, a segment is the basic unit being distributed.
Segments communicate each other via UCX transmitters and receivers.However users do not need to worry about adding the UCX connection. The Application API implementation automatically completes the connection,
when calling connect() API on the pair of segments. The connected pair of segments can run in single node / single process, single node / multi process
or on multi nodes / multi process. No matter which way, since UCX transmitter and receiver are use between the connected pair of segment,
it is considered as distributed execution use case.As compared to a single segment application, in a multi segment or a distributed application, instead of creating and connecting entities, segments are
created and connected with each as show in the example below.classSampleSegmentApp:publicApplication{public:voidcompose()override{// create segmentsautotx_segment=createSegment<PingTxSegment>("TxSegment");autofwd_segment=createSegment<ForwardSegment>("FwdSegment");autorx_segment=createSegment<PingRxSegment>("RxSegment");// connect segmentsconnect(tx_segment,fwd_segment,{SegmentPortPair("Tx.signal","Fwd.in")});connect(fwd_segment,rx_segment,{SegmentPortPair("Fwd.out","Rx.signal")});}};Creating a Segmenttemplate<typenameSegmentT>std::shared_ptr<SegmentT>Application::createSegment(constchar*name,)#In a distributed application, the same application binary is executed on multiple nodes. This does not necessarily mean
that a node has to execute all the segment in an application. Each application binary instance enables only a subset of
of all the segments created in the source code.Each node can be configured with the specific segments that it is supposed to execute. This is done by the segment config file
as shown below. If no segment config file provided to execute the app, all segments get enabled in that instance.Connecting SegmentsExpected<void>Application::connect(std::shared_ptr<nvidia::gxf::Segment>source,std::shared_ptr<nvidia::gxf::Segment>target,std::vector<SegmentPortPair>port_maps,)#Segments are connected viaSegmentPortPairto form a global segment connection map.
It’s a vector of connection because we support more than one connection between a pair of segments.
For the most common case, the port_maps size is 1, i.e. there is only one connection between source and target segment.Please note it is normal that source and target segments are not enabled in the same application instance,
the app instance that enables the segment automatically completes corresponding ucx components.
And GraphDriver later resolves the connection address between the two instances.Segment configSegment config is a YAML file consisting of one node with identifiersegment_config. There are 3 member types:segment_config.member:enabled_segmentsMandatory. Specify the segment by name when create in source code.segment_config.member:workerMandatory. Specify driver IP and port; optionally specify its own port.segment_config.member:driverOptional. Driver member can be enabled in any app instance or a standalone instance.For example, we can run the same SampleSegmentApp executable binary in 3 remote processes and each process executes one segment from the app.
Process 1 to run TxSegment; Process 2 to run FwdSegment; Process 3 to run RxSegment.segment config for instance in process 1:---segment_config:-member:enabled_segmentsparameters:enabled:-TxSegment-member:workerparameters:enabled:Truename:worker_Txport:50001driver_ip:"localhost"driver_port:50000-member:driverparameters:enabled:Truename:driver_50000port:50000segment config for instance in process 2:---segment_config:-member:enabled_segmentsparameters:enabled:-FwdSegment-member:workerparameters:enabled:Truename:worker_Fwdport:50002driver_ip:"localhost"driver_port:50000segment config for instance in process 3:---segment_config:-member:enabled_segmentsparameters:enabled:-RxSegment-member:workerparameters:enabled:Truename:worker_Rxport:50003driver_ip:"localhost"driver_port:50000NoteThe distributed execution in C++ App API is facing segfault after finishing execution during destroy; The equivalent YAML API execution doesn’t observe the issue.The same Python App API for distributed execution will be available in future release.[Coming Soon] More sample applications will be available inGXF github repo https://github.com/NVIDIA-AI-IOT/deepstream_gxf_ref_appsGraph Composer#This section helps you get familiar with the application development workflow using Graph Composer which includes the following:Launch Graph ComposerSync extensions from NVIDIA Cloud repositoryCreate simple application using Graph ComposerRun applicationCreate container image for the applicationWe will start by first setting up the system and explain the basic layout of the Composer on Ubuntu 22.04 x86_64. Then, we will load, understand, and run a simple application. This will provide an understanding of how the Composer works. Finally, we will create a simple application without writing a single line of code. Graph development is currently supported only on x86. Graph Composer package forarm64can be used to deploy or execute graph on Jetson.Installation stepinstalls all tools in the/opt/nvidia/graph-composerdirectory with links to tools in/usr/bindirectory. You can access the tools without switching to the installation directory. After installation, check if the installation was successful using the following commands in a terminal:registry--helpusage:registry[-h][-v]...positionalarguments:cachePerformactionsoncacherepoPerformactionsonrepositoriescompPerformactionsoncomponentsextnPerformactionsonextensionsgraphPerformactionsongraphoptionalarguments:-h,--helpshowthishelpmessageandexit-v,--versionPrintregistrytoolandGXFSpecversioncontainer_builder--helpusage:container_builder[-h][-v][--log-level{DEBUG,INFO,WARNING,ERROR,CRITICAL}][--log-fileLOG_FILE]...builddockerimagesfromconfigfilepositionalarguments:buildBuildcontainerimageusingconfigfilespushPushlocalcontainerimagetoremoterepooptionalarguments:-h,--helpshowthishelpmessageandexit-v,--versionContainerBuilderVersion--log-level{DEBUG,INFO,WARNING,ERROR,CRITICAL}setloglevel,defaultisINFO--log-fileLOG_FILEOptional,setlogoutputfileIf you still don’t see the components, check theFAQsection.Launch Graph Composer#There are two options to launch Composer:Native workstation:Launch Composer from native workstation using following command:composerDeepStream SDK devel container image:Launch Composer from DeepStream SDK devel container image, installation on local system is not required for it:dockerpullnvcr.io/nvidia/deepstream:7.1-gc-triton-develxhost+dockerrun-it--entrypoint/bin/bash--gpusall--rm--network=host-eDISPLAY=${DISPLAY}-v/tmp/.X11-unix/:/tmp/.X11-unix--privileged-v/var/run/docker.sock:/var/run/docker.socknvcr.io/nvidia/deepstream:7.1-gc-triton-develcomposerNoteWhen using the Composer from the devel container image, users could have a problem browsing the “/” folder from the file browser, in this case they can just type the file path directly or copy and paste it.Sync Extensions#Before any graph can be executed or container built, extensions from NGC public repo must be synced. Follow the steps below to sync the extensions:Ensure the gxf_server has started by running the following command in a terminal:systemctl--userstatusgxf_serverIf the service is currently running. You’ll see “Active: active (running)” in the output. If the service is not running, run commandsystemctl--userstartgxf_serverto start it.
By default, gxf_server runs on port50051. It can be changed byexportGXF_SERVER_PORT=<port_number>before installation of graph-composer.
To change the port after the service has already started automatically post-installation, use commandsudosystemctlset-environmentGXF_SERVER_PORT=<port_number>to set new port. Then, restart the service to apply the changes using command:systemctl--userrestartgxf_serverAlso change the port number in Composer.Open thePreferenceswindow.Change the port number in the server tab.Be sure no graph is opened. If there is an graph being opened, it must be closed to make the registry menu usable.Open theRegistrymenu from the menubar at the top and click onSyncRepoSelectngc-publicfrom the drop-down list and click onSyncThe composer reports the current status using a progress bar.Once the extension sync is complete, the composer displays a success message.On clicking ‘OK’, the composer automatically refreshes component list. You can see the refreshed list in the component list window on the right.Create a Graph#Now, let’s create a simple graph and run it. For this example we will create a simplePingGraph using components present in the Sample Extension and Standard Extension.
In thisPingGraph, we simply send a message from one entity to another periodically for certain number of counts. It uses the following components:Transmitter:DoubleBufferTransmitter- This is a queue which is holds a message being transmitted.PingTx- This component creates and publishes a message every time it’s executed.PeriodicSchedulingTerm- Scheduling Terms determine when to execute an entity in this case Transmitter. PeriodicSchedulingTerm is used to execute entities periodically.CountSchedulingTerm- CountSchedulingTerm is used to stop the execution after a certain count. If you want to keep it running then skip adding this component.Receiver:DoubleBufferReceiver- This is a queue which hold the messages sent by other components.PingRx- This component receives a message on DoubleBufferReceiver every time it’s executed.MessageAvailableSchedulingTerm- This Scheduling Term determines if a new message has arrived and only then PingRx codelet is ticked.Scheduler:GreedyScheduler- Scheduler determines the order in which components are executed. GreedyScheduler is a simple single-threaded scheduler which executes components one after another.RealtimeClock- A clock used by Scheduler to track time.Follow the steps:Add PingTx, PingRx and GreedyScheduler by dragging and dropping them from the components panel the graph window.Add the rest of the components such as CountSchedulingTerm, PeriodicSchedulingTerm and MessageAvailableSchedulingTerm by dragging and dropping into the respective entity node.Now, right click on the signal in PingTx and click Create DoubleBufferTransmitter. Follow the same steps for PingRx’s signal and GreedyScheduler’s clock.We can create a graph by simply dragging and dropping components from the Component Panel and add more components to it.After adding the components your graph will look like the image below:Now we make connections between components. For instance, you will have to connect aDoubleBufferTransmitterto aDoubleBufferReceiverto pass messages between them.PingTx/clockneeds to be linked toGreedyScheduler/RealtimeClock. These connections are made by creating an edge between the components as shown below:Finally, we have to set the required parameters for the components:InPingRx/MessageAvailableSchedulingTerm: setmin_sizeto1InPingTx/CountSchedulingTerm: setcountto5InPingTx/PeriodicSchedulingTerm: setrecess_periodto5Now you can save the graph usingFile->SaveGraph(as). This will create ayamlfile with all the components and the connections.application:name:MyGraph---dependencies:-extension:SampleExtensionuuid:a6ad78b6-1682-11ec-9621-0242ac130002version:1.5.0-extension:StandardExtensionuuid:8ec2d5d6-b5df-48bf-8dee-0252606fdd7eversion:2.5.0---components:-name:ping_tx0parameters:clock:GreedyScheduler/realtime_clock12signal:double_buffer_transmitter10type:nvidia::gxf::PingTx-name:periodic_scheduling_term3type:nvidia::gxf::PeriodicSchedulingTerm-name:count_scheduling_term4type:nvidia::gxf::CountSchedulingTerm-name:double_buffer_transmitter10type:nvidia::gxf::DoubleBufferTransmittername:PingTxui_property:position:x:56.0y:103.0---components:-name:ping_rx1parameters:signal:double_buffer_receiver11type:nvidia::gxf::PingRx-name:message_available_scheduling_term5parameters:receiver:double_buffer_receiver11type:nvidia::gxf::MessageAvailableSchedulingTerm-name:double_buffer_receiver11type:nvidia::gxf::DoubleBufferReceivername:PingRxui_property:position:x:489.0y:106.0---components:-name:greedy_scheduler2parameters:clock:realtime_clock12type:nvidia::gxf::GreedyScheduler-name:realtime_clock12type:nvidia::gxf::RealtimeClockname:GreedySchedulerui_property:position:x:486.0y:314.0---components:-name:connection13parameters:source:PingTx/double_buffer_transmitter10target:PingRx/double_buffer_receiver11type:nvidia::gxf::Connectionname:node1Run Graph from Graph Composer#You can deploy the graph using one of the following methods:To execute the currently open graph, click on theRunGraphbutton from the
toolbar on the left. This will open theRunGraphdialog.Local System#Make suregxf_serveris running on the local system and the IP address in the Edit/Preferences is of local host.Select appropriate Platform config file using the file browser.Click onRun. The graph execution progress will be reported via logs in the console window.Remote System#Execute on Jetson or another remote systemMake suregxf_serveris running on the remote system and the IP address in the Edit/Preferences is of remote host.Select appropriate Platform config file (aarch64 or x86_64) based on the remote machine configuration.Click onRun. The graph execution progress will be reported via logs in the console window.Please note that this requires Graph Composer package be installed on the remote system.Execute on Jetson or another remote system through WindowsExecuting graph through Windows is very similar to executing graph on Jetson or another remote system.
Please note that this requires Graph Composer package be installed on the remote system.Run Graph from Command line#Execute Graph using commandline (execute_graph.sh script)Theexecute_graph.shscript provided with the graph composer helps with graph execution and provides added functionality.Complete usage reference:Usage: /opt/nvidia/graph-composer/execute_graph.sh [options] <graph-file> [additional graph files]

Options:
  -d, --graph-target "<graph-target-file>"    [Required] Graph target config file
  -s, --subgraphs <subgraph1>,<subgraph2>,... [Optional] Paths of subgraphs used by the application, comma-separated list
      --resources <graph-resources-file>      [Optional] Graph resources file
  -f, --fresh-manifest                        [Optional] Re-install graph and generate a new manifest file
  -g, --with-gdb                              [Optional] Execute the graph under gdb
  -m, --use-manifest <existing-manifest>      [Optional] Use an existing manifest file
  -r, --remove-install-dir                    [Optional] Remove graph installation directory during exit
  -t, --target <username@host>                [Optional] Target to execute the graph on. SSH will be used
      --target-env-vars "<env-vars>"          [Optional] Separated list of environment variables to be set before running on target
  -a  --app-root <app-root>                   [Optional] Root path for gxe to search subgraphsNoteTo execute graphs on a remote target:
* Graph Composer package must already be installed on the target
* It is recommended that a password-less login method be used for SSHTo execute a graph locally, run:/opt/nvidia/graph-composer/execute_graph.sh<graph-file>-d<graph-target>Forexample,ondGPUhost,run:..code-block::text/opt/nvidia/graph-composer/execute_graph.sh<graph-file>-d/opt/nvidia/graph-composer/config/target_x86_64.yamlTo execute on a remote Jetson target, run:/opt/nvidia/graph-composer/execute_graph.sh <graph-file> -d /opt/nvidia/graph-composer/config/target_aarch64.yaml \
-t <username@host> --target-env-vars "DISPLAY=:0"NoteIf a graph has resources associated with it described in a resources YAML file, an additional argument--resources<resources.yaml>can be passed to the script. The resources would be copied to the remote target before
graph executionNoteWhen executing a graph that uses subgraphs, you must pass additional argument-s<subgraph1>,<subgraph2>,...containing paths
to the subgraph files. You must not pass the subgraphs as graph file arguments without an option.NoteTo run the graph on the remote machine, install the following packages:openssh-clientsshfsUsessh-keygento generate an ssh key pair. Copy key to target usingssh-copy-id ${TARGET}Create Container Image from Graph Composer#Container image can be created for Ubuntu 22.04 x86_64 or Jetson but creation is supported only on Ubuntu 22.04 x86_64. Following scenarios are supported for it.To build a container, first click on theBuildContainerbutton from the
toolbar on the top. This will open theBuildContainerwindow.Local System#For creating a container on the local system,Make suregxf_serveris running on the local system and the IP address in the Edit/Preferences is of local host.Launch the file browser using the button next to theConfigurationFileinput.Select a container builder configuration file and open it.Click the button next to thePlatformconfigFileinput to launch the file browser. Select a platform config file and open it.Click onBuildto start the build process. Composer reports the container build status using a progress bar.On successful completion, composer will show a success message.Remote System (Windows)#Building container image through Windows is very similar to building container image on Linux system.Add remote system’s IP address and port number in the Server tab in Edit/Preferences window.For creating a container on the remote system,
Choose the container builder config file and target config file and click onBuildImage.Please note that this requires Graph Composer package be installed on the remote system.DeepStream Application#Previous application was simple demonstrating application workflow. Similar workflow can be used to create, load and run DeepStream applications using GXF. It requires that DeepStream 7.1 and reference graphs packages are installed on the system with all the dependencies.Open theFilemenu from the menubar at the top and click onOpenGraphto launch the file browser. You may alternatively use theCtrl+Okey combination.Browse to a valid graph file, select it and click onOkayto open the graph.Composer should now show the application graph.To load component parameters from a separate file, right-click on the graph and selectLoadparametersfrom the context menu to launch the file browser.Browse to an appropriate parameters file for the currently open and visible graph, select it and click onOkayto load parameter values from the file.Rest of the steps to run the application or build container image are same as demonstrated earlier.previousSupported platformsnextCreating an AI ApplicationOn this pagePython Application DevelopmentPython API SetupComputeEntityPyComputeEntityTensor zero-copy with other frameworksFrom other frameworks to GXFFrom GXF to other frameworksExample ApplicationUCX Transmit / ReceiveGraph with UCX TxGraph with UCX RxUCXSource, UCXSink, and context implementation for referenceC++ Application DevelopmentGraph EntitySegmentConnecting Graph EntitiesApplicationRuntime Codelet RegistrationDistributed ApplicationcreateSegment()connect()Graph ComposerLaunch Graph ComposerSync ExtensionsCreate a GraphRun Graph from Graph ComposerLocal SystemRemote SystemRun Graph from Command lineCreate Container Image from Graph ComposerLocal SystemRemote System (Windows)DeepStream ApplicationPrivacy Policy|Manage My Privacy|Do Not Sell or Share My Data|Terms of Service|Accessibility|Corporate Policies|Product Security|ContactCopyright © 2024-2025, NVIDIA Corporation.Last updated on Jan 13, 2025.