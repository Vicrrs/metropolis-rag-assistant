NvTritonExt — DeepStream documentationSkip to main contentBack to topCtrl+KDeepStream documentationDeepStream documentationTable of ContentsDeepStream Getting StartedWelcome to the DeepStream DocumentationMigration GuideInstallationQuickstart GuideDocker ContainersDeepStream SamplesC/C++ Sample Apps Source DetailsPython Sample Apps and Bindings Source DetailsDeepStream Reference Application - deepstream-appDeepStream Reference Application - deepstream-test5 appDeepStream Reference Application - deepstream-nmos appDeepStream Reference Application on GitHubSample Configurations and StreamsImplementing a Custom GStreamer Plugin with OpenCV Integration ExampleTAO toolkit Integration with DeepStreamTAO Toolkit Integration with DeepStreamTutorials and How-to'sDeepStream-3D Custom Apps and Libs TutorialsDeepStream PerformancePerformanceDeepStream AccuracyAccuracy Tuning ToolsDeepStream Custom ModelUsing a Custom Model with DeepStreamDeepStream Key FeaturesDeepStream-3D Sensor Fusion Multi-Modal Application and FrameworkDeepStream-3D Multi-Modal BEVFusion SetupDeepStream-3D Multi-Modal V2XFusion SetupSmart Video RecordIoTOn the Fly Model UpdateNTP Timestamp in DeepStreamAV Sync in DeepStreamDeepStream With REST API SeverDeepStream 3D Action Recognition AppDeepStream 3D Depth Camera AppDeepStream 3D Lidar Inference AppNetworked Media Open Specifications (NMOS) in DeepStreamGst-nvdspostprocess in DeepStreamDeepStream Can Orientation AppDeepStream Application MigrationApplication Migration to DeepStream 7.1 from DeepStream 7.0DeepStream Plugin GuideGStreamer Plugin OverviewMetaData in the DeepStream SDKGst-nvdspreprocess (Alpha)Gst-nvinferGst-nvinferserverGst-nvtrackerGst-nvstreammuxGst-nvstreammux NewGst-nvstreamdemuxGst-nvmultistreamtilerGst-nvdsosdGst-nvdsmetautilsGst-nvdsvideotemplateGst-nvdsaudiotemplateGst-nvvideoconvertGst-nvdewarperGst-nvofGst-nvofvisualGst-nvsegvisualGst-nvvideo4linux2Gst-nvjpegdecGst-nvimagedecGst-nvjpegencGst-nvimageencGst-nvmsgconvGst-nvmsgbrokerGst-nvdsanalyticsGst-nvdsudpsrcGst-nvdsudpsinkGst-nvdspostprocess (Alpha)Gst-nvds3dfilterGst-nvds3dbridgeGst-nvds3dmixerGst-NvDsUcxGst-nvdsxferGst-nvvideotestsrcGst-nvmultiurisrcbinGst-nvurisrcbinDeepStream Troubleshooting and FAQTroubleshootingFrequently Asked QuestionsDeepStream On WSL2DeepStream On WSLFAQ for Deepstream On WSLDeepStream API GuideDeepStream API GuidesDeepStream Service MakerWhat is Deepstream Service MakerService Maker for C/C++ DevelopersService Maker for Python Developers(alpha)Quick Start GuideIntroduction to Flow APIsIntroduction to Pipeline APIsAdvanced FeaturesMigrating Traditional Deepstream Apps to Service Maker Apps in PythonWhat is a Deepstream Service Maker PluginDeepstream LibrariesDeepStream Libraries (Developer Preview)Graph ComposerOverviewPlatformsSupported platformsGetting StartedApplication Development WorkflowCreating an AI ApplicationReference graphsExtension Development WorkflowDeveloping Extensions for DeepStreamDeepStream ComponentsGXF InternalsGXF InternalsGraph eXecution EngineGraph Execution EngineGraph Composer ContainersGraph Composer and GXF ContainersGXF Component InterfacesGXF Component InterfacesGXF Application API'sGXF App C++ APIsGXF App Python APIsGXF Runtime API'sGXF Core C++ APIsGXF Core C APIsGXF Core Python APIsExtension ManualExtensionsCudaExtensionGXF Stream SyncStandardExtensionPython CodeletsNetworkExtensionNvTritonExtSerializationExtensionMultimediaExtensionVideoEncoderExtensionVideoDecoderExtensionBehavior TreesUCX ExtensionHttpExtensionGrpcExtensionTensorRTExtensionNvDs3dProcessingExtNvDsActionRecognitionExtNvDsAnalyticsExtNvDsBaseExtNvDsCloudMsgExtNvDsConverterExtNvDsDewarperExtNvDsInferenceExtNvDsInferenceUtilsExtNvDsInterfaceExtNvDsMuxDemuxExtNvDsOpticalFlowExtNvDsOutputSinkExtNvDsSampleExtNvDsSampleModelsExtNvDsSourceExtNvDsTemplateExtNvDsTrackerExtNvDsTranscodeExtNvDsTritonExtNvDsUcxExtNvDsUdpExtNvDsVisualizationExtToolsRegistryRegistry Command Line InterfaceComposerContainer BuilderGXF Command Line InterfacePipetuner GuideFAQ GuideFAQDeepStream Legal InformationDeepStream End User License AgreementDeepStream FeedbackFeedback formExtensionsNvTritonExtNvTritonExt#NVIDIA Triton Inference components. This extension is intended to be used with Triton 2.49.0 (x86_64) and 2.40.0 (Jetpack 6.1).Refer to the officialNVIDIA Tritondocumentation for support matrix and more.UUID: a3c95d1c-c06c-4a4e-a2f9-8d9078ab645cVersion: 0.5.0Author: NVIDIALicense: ProprietaryComponents#nvidia::triton::TritonServer#Triton inference server component using theTriton C API.Component ID: 26228984-ffc4-4162-9af5-6e3008aa2982Base Type: nvidia::gxf::ComponentParameters#log_levelLogging level for Triton.Valid values:0: Error1: Warn2: Info3+: VerboseFlags: GXF_PARAMETER_FLAGS_NONE (1 = default)Type: GXF_PARAMETER_TYPE_UINT32enable_strict_model_configEnables strict model configuration to enforce presence of config. If disabled, TensorRT,
TensorFlow saved-model, and ONNX models do not require a model configuration file. Triton can derive all the required settings automatically.Flags: GXF_PARAMETER_FLAGS_NONE (true = default)Type: GXF_PARAMETER_TYPE_BOOLmin_compute_capabilityMinimum Compute Capability for GPU. Refer tohttps://developer.nvidia.com/cuda-gpus.Flags: GXF_PARAMETER_FLAGS_NONE (6.0 = default)Type: GXF_PARAMETER_TYPE_FLOAT64model_repository_pathsList of Triton Model Repository Paths. Refer tobytedance/triton-inference-serverFlags: GXF_PARAMETER_FLAGS_NONEType: GXF_PARAMETER_TYPE_STRINGtf_gpu_memory_fractionThe portion of GPU memory to be reserved for TensorFlow models.Flags: GXF_PARAMETER_FLAGS_NONE (0.0 = default)Type: GXF_PARAMETER_TYPE_FLOAT64tf_disable_soft_placement_Allow Tensorflow to use CPU operation when GPU implementation is not available.Flags: GXF_PARAMETER_FLAGS_NONE (true = default)Type: GXF_PARAMETER_TYPE_BOOLbackend_directory_pathPath to Triton backend directory.Flags: GXF_PARAMETER_FLAGS_NONE (”” = default)Type: GXF_PARAMETER_TYPE_STRINGmodel_control_modeTriton model control mode.Valid values:“none”: Load all models in the model repository at startup.“explicit”: Allow models to load when needed.Flags: GXF_PARAMETER_FLAGS_NONE (“explicit” = default)Type: GXF_PARAMETER_TYPE_STRINGbackend_configsTriton backend configurations in the format:backend,setting=value.
Refer to Backend specific documentation:triton-inference-server/tensorflow_backend,triton-inference-server/python_backend’.Flags: GXF_PARAMETER_FLAGS_OPTIONALType: GXF_PARAMETER_TYPE_STRINGnvidia::triton::TritonInferencerInterface#Helper component that provides an interface for Triton inferencing.Component ID: 1661c015-6b1c-422d-a6f0-248cdc197b1aBase Type: nvidia::gxf::Componentnvidia::triton::TritonInferencerImpl#Component that implements theTritonInferencerInterfaceto obtain inferences from theTritonServercomponent or from an external Triton instance.Component ID: b84cf267-b223-4df5-ac82-752d9fae1014Base Type: nvidia::triton::TritonInferencerInterfaceParameters#serverTriton server. This optional handle must be specified if theinference_modeof this
component isDirect.Flags: GXF_PARAMETER_FLAGS_OPTIONALType: GXF_PARAMETER_TYPE_HANDLEHandle Type: nvidia::triton::TritonServermodel_nameTriton model name to run inference.Flags: GXF_PARAMETER_FLAGS_NONEType: GXF_PARAMETER_TYPE_STRINGmodel_versionTriton model version of the model name to run inference.Flags: GXF_PARAMETER_FLAGS_NONEType: GXF_PARAMETER_TYPE_INT64max_batch_sizeMax batch size to run inference. This should match the value in the Triton model repository.Flags: GXF_PARAMETER_FLAGS_NONEType: GXF_PARAMETER_TYPE_UINT32num_concurrent_requestsMaximum number of concurrent inference requests for this model version. This is used to define a
pool of requests.Flags: GXF_PARAMETER_FLAGS_NONE (1 = default)Type: GXF_PARAMETER_TYPE_UINT32async_scheduling_termAsynchronous scheduling term that determines when a response is ready.Flags: GXF_PARAMETER_FLAGS_NONEType: GXF_PARAMETER_TYPE_HANDLEHandle Type: nvidia::gxf::AsynchronousSchedulingTerminference_modeTriton inferencing mode.Valid values:Direct: This mode requires aTritonServercomponent handle to be passed to the optionalserverparameter.RemoteGrpc: This mode requires the optionalserver_endpointpoint to an external Triton gRPC server URL.Flags: GXF_PARAMETER_FLAGS_NONEType: GXF_PARAMETER_TYPE_STRINGserver_endpointServer endpoint URL for an external Triton instance. This optional string must be specified if theinference_modeof this component is of theRemotevariety.Flags: GXF_PARAMETER_FLAGS_OPTIONALType: GXF_PARAMETER_TYPE_STRINGnvidia::triton::TritonInferenceRequest#Generic codelet that requests a Triton Inference. This will use a handle to anInferencerImplto
interface with Triton.Component ID: 34395920-232c-446f-b5b7-46f642ce84dfBase Type: nvidia::gxf::CodeletParameters#inferencerHandle to Triton inference implementation. This is used to request an inference.Flags: GXF_PARAMETER_FLAGS_NONEType: GXF_PARAMETER_TYPE_HANDLEHandle Type: nvidia::triton::TritonInferencerInterfacerxList of receivers to take input tensors.Flags: GXF_PARAMETER_FLAGS_NONEType: GXF_PARAMETER_TYPE_HANDLEHandle Type: nvidia::gxf::Receiverinput_tensor_namesNames of input tensors that exist in the ordered receivers inrx.Flags: GXF_PARAMETER_FLAGS_NONEType: GXF_PARAMETER_TYPE_STRINGinput_binding_namesNames of input bindings corresponding to Triton’s config inputs in the same order of what is
provided ininput_tensor_names.Flags: GXF_PARAMETER_FLAGS_NONEType: GXF_PARAMETER_TYPE_STRINGnvidia::triton::TritonInferenceResponse#Generic codelet that obtains a response from a Triton Inference. This will use a handle to anInferencerImplto interface with Triton.Component ID: 4dd957a7-aa55-4117-90d3-9a98e31ee176Base Type: nvidia::gxf::CodeletParameters#inferencerHandle to Triton inference implementation. This is used to request an inference.Flags: GXF_PARAMETER_FLAGS_NONEType: GXF_PARAMETER_TYPE_HANDLEHandle Type: nvidia::triton::TritonInferencerInterfaceoutput_tensor_namesNames of output tensors in the order to be retrieved from the model.Flags: GXF_PARAMETER_FLAGS_NONEType: GXF_PARAMETER_TYPE_STRINGoutput_binding_namesNames of output bindings in the model in the same order of of what
is provided inoutput_tensor_names.Flags: GXF_PARAMETER_FLAGS_NONEType: GXF_PARAMETER_TYPE_STRINGtxSingle transmitter to publish output tensors.Flags: GXF_PARAMETER_FLAGS_NONEType: GXF_PARAMETER_TYPE_HANDLEHandle Type: nvidia::gxf::Transmitternvidia::triton::TritonOptions#Generic struct that represent Triton Inference Options for model control and sequence control.Component ID: 087696ed-229d-4199-876f-05b92d3887f0nvidia::triton::TritonRequestReceptiveSchedulingTerm#Triton Scheduling Term that schedules Request Codelet when the inferencer can accept a new request.Component ID: f8602412-1242-4e43-9dbf-9c559d496b84Base Type: nvidia::gxf::SchedulingTermParameters#inferencerHandle to Triton inference implementation. This is used to check the accecptability of a new request.Flags: GXF_PARAMETER_FLAGS_NONEType: GXF_PARAMETER_TYPE_HANDLEHandle Type: nvidia::triton::TritonInferencerInterfacepreviousNetworkExtensionnextSerializationExtensionOn this pageComponentsnvidia::triton::TritonServerParametersnvidia::triton::TritonInferencerInterfacenvidia::triton::TritonInferencerImplParametersnvidia::triton::TritonInferenceRequestParametersnvidia::triton::TritonInferenceResponseParametersnvidia::triton::TritonOptionsnvidia::triton::TritonRequestReceptiveSchedulingTermParametersPrivacy Policy|Manage My Privacy|Do Not Sell or Share My Data|Terms of Service|Accessibility|Corporate Policies|Product Security|ContactCopyright © 2024-2025, NVIDIA Corporation.Last updated on Jan 13, 2025.