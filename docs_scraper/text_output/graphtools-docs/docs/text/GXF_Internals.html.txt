GXF Internals — DeepStream documentationSkip to main contentBack to topCtrl+KDeepStream documentationDeepStream documentationTable of ContentsDeepStream Getting StartedWelcome to the DeepStream DocumentationMigration GuideInstallationQuickstart GuideDocker ContainersDeepStream SamplesC/C++ Sample Apps Source DetailsPython Sample Apps and Bindings Source DetailsDeepStream Reference Application - deepstream-appDeepStream Reference Application - deepstream-test5 appDeepStream Reference Application - deepstream-nmos appDeepStream Reference Application on GitHubSample Configurations and StreamsImplementing a Custom GStreamer Plugin with OpenCV Integration ExampleTAO toolkit Integration with DeepStreamTAO Toolkit Integration with DeepStreamTutorials and How-to'sDeepStream-3D Custom Apps and Libs TutorialsDeepStream PerformancePerformanceDeepStream AccuracyAccuracy Tuning ToolsDeepStream Custom ModelUsing a Custom Model with DeepStreamDeepStream Key FeaturesDeepStream-3D Sensor Fusion Multi-Modal Application and FrameworkDeepStream-3D Multi-Modal BEVFusion SetupDeepStream-3D Multi-Modal V2XFusion SetupSmart Video RecordIoTOn the Fly Model UpdateNTP Timestamp in DeepStreamAV Sync in DeepStreamDeepStream With REST API SeverDeepStream 3D Action Recognition AppDeepStream 3D Depth Camera AppDeepStream 3D Lidar Inference AppNetworked Media Open Specifications (NMOS) in DeepStreamGst-nvdspostprocess in DeepStreamDeepStream Can Orientation AppDeepStream Application MigrationApplication Migration to DeepStream 7.1 from DeepStream 7.0DeepStream Plugin GuideGStreamer Plugin OverviewMetaData in the DeepStream SDKGst-nvdspreprocess (Alpha)Gst-nvinferGst-nvinferserverGst-nvtrackerGst-nvstreammuxGst-nvstreammux NewGst-nvstreamdemuxGst-nvmultistreamtilerGst-nvdsosdGst-nvdsmetautilsGst-nvdsvideotemplateGst-nvdsaudiotemplateGst-nvvideoconvertGst-nvdewarperGst-nvofGst-nvofvisualGst-nvsegvisualGst-nvvideo4linux2Gst-nvjpegdecGst-nvimagedecGst-nvjpegencGst-nvimageencGst-nvmsgconvGst-nvmsgbrokerGst-nvdsanalyticsGst-nvdsudpsrcGst-nvdsudpsinkGst-nvdspostprocess (Alpha)Gst-nvds3dfilterGst-nvds3dbridgeGst-nvds3dmixerGst-NvDsUcxGst-nvdsxferGst-nvvideotestsrcGst-nvmultiurisrcbinGst-nvurisrcbinDeepStream Troubleshooting and FAQTroubleshootingFrequently Asked QuestionsDeepStream On WSL2DeepStream On WSLFAQ for Deepstream On WSLDeepStream API GuideDeepStream API GuidesDeepStream Service MakerWhat is Deepstream Service MakerService Maker for C/C++ DevelopersService Maker for Python Developers(alpha)Quick Start GuideIntroduction to Flow APIsIntroduction to Pipeline APIsAdvanced FeaturesMigrating Traditional Deepstream Apps to Service Maker Apps in PythonWhat is a Deepstream Service Maker PluginDeepstream LibrariesDeepStream Libraries (Developer Preview)Graph ComposerOverviewPlatformsSupported platformsGetting StartedApplication Development WorkflowCreating an AI ApplicationReference graphsExtension Development WorkflowDeveloping Extensions for DeepStreamDeepStream ComponentsGXF InternalsGXF InternalsGraph eXecution EngineGraph Execution EngineGraph Composer ContainersGraph Composer and GXF ContainersGXF Component InterfacesGXF Component InterfacesGXF Application API'sGXF App C++ APIsGXF App Python APIsGXF Runtime API'sGXF Core C++ APIsGXF Core C APIsGXF Core Python APIsExtension ManualExtensionsCudaExtensionGXF Stream SyncStandardExtensionPython CodeletsNetworkExtensionNvTritonExtSerializationExtensionMultimediaExtensionVideoEncoderExtensionVideoDecoderExtensionBehavior TreesUCX ExtensionHttpExtensionGrpcExtensionTensorRTExtensionNvDs3dProcessingExtNvDsActionRecognitionExtNvDsAnalyticsExtNvDsBaseExtNvDsCloudMsgExtNvDsConverterExtNvDsDewarperExtNvDsInferenceExtNvDsInferenceUtilsExtNvDsInterfaceExtNvDsMuxDemuxExtNvDsOpticalFlowExtNvDsOutputSinkExtNvDsSampleExtNvDsSampleModelsExtNvDsSourceExtNvDsTemplateExtNvDsTrackerExtNvDsTranscodeExtNvDsTritonExtNvDsUcxExtNvDsUdpExtNvDsVisualizationExtToolsRegistryRegistry Command Line InterfaceComposerContainer BuilderGXF Command Line InterfacePipetuner GuideFAQ GuideFAQDeepStream Legal InformationDeepStream End User License AgreementDeepStream FeedbackFeedback formGXF InternalsGXF Internals#This section covers upon the guide that helps in understanding the basic building blocks of GXF
One of the key components in the application development is theCodeletsand it is important
to understand its lifecycle.LifeCycle of a Codelet#The life cycle of a codelet is governed by the below five stagesInitialize: Used for light-weight initialization. Called directly when the codelet is created for
the first time. Called exactly once over the lifetime of a codelet.Deinitialize: Used for light-weight deinitialization. Called directly before the codelet is
destroyed. Called exactly once over the lifetime of a codelet.Start: Used for heavy-weight initialization to allocate resources which are necessary for
execution. Can be called multiple times over the lifetime of the codelet according to order
defined by the lifecycle.Stop: Used for heavy-weight deinitialization to deallocate all resources which were allocated by
the start function. Can be called multiple times over the lifetime of the codelet according to
order defined by the lifecycle.Tick: Called when the codelet is triggered. Triggering can happen when data arrives or based on a
time trigger. Normally called many times over the lifetime of a codelet.Codelet component is used to define a custom codelet that will accomplish the desired work.
Below example describes on how to implement a custom codelet. For now please ignore
:ref:registerInterfacewhich will covered in the later sections.classTest:publicCodelet{public:gxf_result_tstart()override{returnGXF_SUCCESS;}gxf_result_ttick()override{// Do Something ...returnGXF_SUCCESS;}gxf_result_tstop()override{returnGXF_SUCCESS;}gxf_result_tregisterInterface(Registrar*registrar)override{Expected<void>result;result&=registrar->parameter(some_parameter_,"Some Parameter");returnToResultCode(result);}Parameter<Handle<Receiver>>some_parameter_;};Below example shows on how to create a simple custom codelet that prints “Hello World”classHelloWorld:publicCodelet{public:gxf_result_tstart()override{returnGXF_SUCCESS;}gxf_result_ttick()override{GXF_LOG_INFO("Hello World from GXF");returnGXF_SUCCESS;}gxf_result_tstop()override{returnGXF_SUCCESS;}gxf_result_tregisterInterface(Registrar*registrar)override{Expected<void>result;result&=registrar->parameter(some_parameter_,"Some Parameter");returnToResultCode(result);}Parameter<Handle<Receiver>>some_parameter_;};Below example explores on how to use the initialize/deinitialize  and start/stop stagesclassFile:publicCodelet{public:gxf_result_tinitialize(){access_file_=true;returnGXF_SUCCESS;}gxf_result_tstart()override{if(access_file_){file_=fopen("Some File Name","Read or Write Mode");if(file_==nullptr){GXF_LOG_ERROR("%s",strerror(errno));returnUnexpected{GXF_FAILURE};}}returnGXF_SUCCESS;}gxf_result_ttick()override{// Do some file operationreturnGXF_SUCCESS;}gxf_result_tstop()override{constintresult=fclose(file_);if(result!=0){GXF_LOG_ERROR("%s",strerror(errno));returnUnexpected{GXF_FAILURE};}file_=nullptr;returnGXF_SUCCESS;}gxf_result_tdeinitialize()override{access_file_=false;returnGXF_SUCCESS;}gxf_result_tregisterInterface(Registrar*registrar)override{Expected<void>result;result&=registrar->parameter(some_parameter_,"Some Parameter");// Not used herereturnToResultCode(result);}Parameter<Handle<Receiver>>some_parameter_;boolaccess_file_{false};std::FILE*file_;};The GXF Scheduler#The execution of entities in a graph is governed by the scheduler and
the scheduling terms associated with every entity. A scheduler is a
component responsible for orchestrating the execution of all the
entities defined in a graph. A scheduler typically keeps track of the
graph entities and their current execution states and passes them on to
a nvidia::gxf::EntityExecutor component when ready for execution. The
following diagram depicts the flow for an entity execution.Figure: Entity execution sequenceAs shown in the sequence diagram, the schedulers begin executing the
graph entities via the nvidia::gxf::System::runAsync_abi() interface
and continue this process until it meets the certain ending criteria. A
single entity can have multiple codelets. These codelets are executed in
the same order in which they were defined in the entity. A failure in
execution of any single codelet stops the execution of all the entities.
Entities are naturally unscheduled from execution when any one of their
scheduling term reaches NEVER state.Scheduling terms are components used to define the execution readiness
of an entity. An entity can have multiple scheduling terms associated
with it and each scheduling term represents the state of an entity using
SchedulingCondition.The table below shows various states of nvidia::gxf::SchedulingConditionType described
using nvidia::gxf::SchedulingCondition.SchedulingConditionTypeDescriptionNEVEREntity will never execute againREADYEntity is ready for executionWAITEntity may execute in the futureWAIT_TIMEEntity will be ready for execution after specified durationWAIT_EVENTEntity is waiting on an asynchronous event with unknown time intervalSchedulers define deadlock as a condition when there are no entities which
are in READY, WAIT_TIME or WAIT_EVENT state which guarantee execution
at a future point in time. This implies all the entities are
in WAIT state for which the scheduler does not know if they ever will
reach the READY state in the future. The scheduler can be configured to
stop when it reaches such a state using
the stop_on_deadlock parameter, else the entities are polled to check
if any of them have reached READY state. max_duration config parameter
can be used to stop execution of all entities regardless of their state
after a specified amount of time has elapsed.Figure: Entity State transition for all schedulersThere are four types of schedulers currently supported by GXFGreedy SchedulerMultithread SchedulerEpoch SchedulerEvent Based SchedulerGreedy Scheduler#This is a basic single threaded scheduler which tests scheduling term
greedily. It is great for simple use cases and predictable execution but
may incur a large overhead of scheduling term execution, making it
unsuitable for large applications. The scheduler requires a clock to
keep track of time. Based on the choice of clock the scheduler will
execute differently. If a Realtime clock is used the scheduler will
execute in real-time. This means pausing execution - sleeping the
thread, until periodic scheduling terms are due again. If a ManualClock
is used scheduling will happen “time-compressed”. This means flow of
time is altered to execute codelets in immediate succession.The GreedyScheduler maintains a running count of entities which are
in READY, WAIT_TIME and WAIT_EVENT states. The following activity
diagram depicts the gist of the decision making for scheduling an entity
by the greedy scheduler -Figure: Greedy Scheduler Activity DiagramGreedy Scheduler Configuration#The greedy scheduler takes in the following parameters from the
configuration fileParameter nameDescriptionclockThe clock used by the scheduler to define the flow of time. Typical choices are RealtimeClock or ManualClockmax_duration_msThe maximum duration for which the scheduler will execute (in ms). If not specified, the scheduler will run until all work is done. If periodic terms are present this means the application will run indefinitelystop_on_deadlockIf stop_on_deadlock is disabled, the GreedyScheduler constantly polls for the status of all the waiting entities to check if any of them are ready for execution.Example usage -
The following code snippet configures a Greedy scheduler with a ManualClock option specified.name:schedulercomponents:-type:nvidia::gxf::GreedySchedulerparameters:max_duration_ms:3000clock:misc/clockstop_on_deadlock:true---name:misccomponents:-name:clocktype:nvidia::gxf::ManualClockMultithread Scheduler#The MultiThread scheduler is more suitable for large applications with
complex execution patterns. The scheduler consists of a dispatcher
thread which checks the status of an entity and dispatches it to a
thread pool of worker threads responsible for executing them. Worker
threads enqueue the entity back on to the dispatch queue upon completion
of execution. The number of worker threads can be configured
using worker_thread_number parameter. The MultiThread scheduler also
manages a dedicated queue and thread to handle asynchronous events. The
following activity diagram demonstrates the gist of the multithread
scheduler implementation.Figure: MultiThread Scheduler Activity DiagramAs depicted in the diagram, when an entity reaches WAIT_EVENT state,
it’s moved to a queue where they wait to receive event done
notification. The asynchronous event handler thread is responsible for
moving entities to the dispatcher upon receiving event done
notification. The dispatcher thread also maintains a running count of
the number of entities in READY, WAIT_EVENT and WAIT_TIME states and
uses these statistics to check if the scheduler has reached a
deadlock. The scheduler also needs a clock component to keep track of
time and it is configured using the clock parameter.MultiThread scheduler is more resource efficient compared to the Greedy
Scheduler and does not incur any additional overhead for constantly
polling the states of scheduling terms.
The check_recession_period_ms parameter can be used to configure the
time interval the scheduler must wait to poll the state of entities
which are in WAIT state.Multithread Scheduler Configuration#The multithread scheduler takes in the following parameters from the
configuration fileParameter nameDescriptionclockThe clock used by the scheduler to define the flow of time. Typical choices are RealtimeClock or ManualClock.max_duration_msThe maximum duration for which the scheduler will execute (in ms). If not specified, the scheduler will run until all work is done. If periodic terms are present this means the application will run indefinitely.check_recess_period_msDuration to sleep before checking the condition of an entity again [ms]. This is the maximum duration for which the scheduler would wait when an entity is not yet ready to run.stop_on_deadlockIf enabled the scheduler will stop when all entities are in a waiting state, but no periodic entity exists to break the dead end. Should be disabled when scheduling conditions can be changed by external actors, for example by clearing queues manually.worker_thread_numberNumber of threads.Example usage -
The following code snippet configures a Multithread scheduler with the number of worked threads and max duration specified -name:schedulercomponents:-type:nvidia::gxf::MultiThreadSchedulerparameters:max_duration_ms:5000clock:misc/clockworker_thread_number:5check_recession_period_ms:3stop_on_deadlock:false---name:misccomponents:-name:clocktype:nvidia::gxf::RealtimeClockEvent Based Scheduler#The Event Based scheduler is more suitable for large applications with
complex execution patterns. This scheduler maintains three different
queues holding entities in READY or WAIT_TIME, WAIT_EVENT and WAIT
states. An event is defined as a change in the scheduling term of any
entity which can be triggered either by expiration of a time duration,
execution of an entity, API called by a user thread, message
transmission across the edges, etc. Events are generated at the end of
an entity execution, or after change in state of scheduling conditions.
In addition, the event-based scheduler launches several worker threads
, async event handler thread and a dispatcher thread. Worker threads
pop entities from the READY or WAIT_TIME queue and acquire them for
execution. After the execution is complete, the worker thread
generates an event. The dispatcher thread responds to events by moving
entities from one queue to another by evaluating their scheduling
condition. The async event handler thread responds to external events.
The dispatcher thread and async event handler thread sleep between
events thus saving CPU cycles. The number of worker threads can be
configured using worker_thread_number parameter.Messaging and Events#Components communicate with each other to accomplish the defined
pipeline. Messages are the form of communication by components with
each other. Entities have transmitter and receiver queues in order to
transmit and receive messages. The transmitter of an upstream entity
is connected to the receiver of a downstream entity. GXF allows M × N
connections between transmitters and receivers where M is the number
of transmitters and N is the number of receivers. After a component
consumes a message from its receiver queue, an event is generated to
notify the upstream entity so that it can evaluate its scheduling
terms and allow a change in its scheduling condition. Similarly, when
a component publishes a message to a downstream entity , it generates
an event to notifies the entity. The dispatcher thread is responsible
for processing the event and evaluating the scheduling terms. This
operation is called dispatching an entity.Figure: Messaging Events Activity DiagramThe dispatcher thread also maintains a running count of the number of
entities in READY, WAIT_EVENT and WAIT_TIME states and uses these
statistics to check if the scheduler has reached a deadlock. The
scheduler also needs a clock component to keep track of time and it is
configured using the clock parameter.Event Based Scheduler Configuration#The Event Based scheduler takes in the following parameters from the
configuration fileParameter nameDescriptionclockThe clock used by the scheduler to define the flow of time. Typical choices are RealtimeClock or ManualClock.max_duration_msThe maximum duration for which the scheduler will execute (in ms). If not specified, the scheduler will run until all work is done. If periodic terms are present this means the application will run indefinitely.stop_on_deadlockIf enabled the scheduler will stop when all entities are in a waiting state, but no periodic entity exists to break the dead end. Should be disabled when scheduling conditions can be changed by external actors, for example by clearing queues manually.worker_thread_numberNumber of threads.thread_pool_allocation_autoIf enabled, only one thread pool will be created. If disabled, user should enumerate pools and priorities.stop_on_deadlock_timeoutScheduler will wait this amount of time when stop_on_dead_lock indicates should stop. It will reset if a job comes in during the wait. Negative value means not stop on deadlock.Example usage -
The following code snippet configures a Event Based scheduler with the number of worked threads and max duration specified -name:schedulercomponents:-type:nvidia::gxf::EventBasedSchedulerparameters:max_duration_ms:5000clock:misc/clockstop_on_deadlock:falseworker_thread_number:5thread_pool_allocation_auto:truestop_on_deadlock_timeout:500---name:misccomponents:-name:clocktype:nvidia::gxf::RealtimeClockEpoch Scheduler#The Epoch scheduler is used for running loads in externally managed threads. Each run is called an
Epoch. The scheduler goes over all entities that are known to be active and executes them one by one.
If the epoch budget is provided (in ms), it would keep running all codelets until the budget is
consumed or no codelet is ready. It might run over budget since it guarantees to cover all codelets
in epoch. In case the budget is not provided, it would go over all the codelets once and execute
them only once.The epoch scheduler takes in the following parameters from the
configuration file -Parameter nameDescriptionclockThe clock used by the scheduler to define the flow of time. Typical choice is a RealtimeClock.Example usage -
The following code snippet configures an Epoch scheduler -name:schedulercomponents:-name:clocktype:nvidia::gxf::RealtimeClock-name:epochtype:nvidia::gxf::EpochSchedulerparameters:clock:clockNote that the epoch scheduler is intended to run from an external thread. TherunEpoch(floatbudget_ms);can be used to set the budget_ms and run the scheduler from the external thread. If the specified budget is not positive, all the nodes are executed once.SchedulingTerms#A SchedulingTerm defines a specific condition that is used by an entity
to let the scheduler know when it’s ready for execution. There are
various scheduling terms currently supported by GXF. If multiple scheduling
terms are present in an entity they all have to be true for an entity to
execute.PeriodicSchedulingTerm#An entity associated with nvidia::gxf::PeriodicSchedulingTerm is ready
for execution after periodic time intervals specified using
its recess_period parameter. The PeriodicSchedulingTerm can either be
in READY or WAIT_TIME state.Example usage --name:scheduling_termtype:nvidia::gxf::PeriodicSchedulingTermparameters:recess_period:50000000CountSchedulingTerm#An entity associated with nvidia::gxf::CountSchedulingTerm is executed
for a specific number of times specified using its count parameter.
The CountSchedulingTerm can either be in READY or NEVER state. The
scheduling term reaches the NEVER state when the entity has been
executed count number of times.Example usage --name:scheduling_termtype:nvidia::gxf::CountSchedulingTermparameters:count:42MessageAvailableSchedulingTerm#An entity associated withnvidia::gxf::MessageAvailableSchedulingTermis executed when the associated receiver queue has at least a certain number of elements.
The receiver is specified using thereceiverparameter of the scheduling term. The minimum number of messages that permits the execution of the entity is specified bymin_size. An optional parameter for this scheduling term isfront_stage_max_size, the maximum front stage message count. If this parameter is set, the scheduling term will only allow execution if the number of messages in the queue does not exceed this count. It can be used for codelets which do not consume all messages from the queue.In the example shown below, the minimum size of the queue is configured to be 4. So, the entity will not be executed till there are at least 4 messages in the queue.-type:nvidia::gxf::MessageAvailableSchedulingTermparameters:receiver:tensorsmin_size:4MultiMessageAvailableSchedulingTerm#An entity associated withnvidia::gxf::MultiMessageAvailableSchedulingTermis executed when a list of provided input receivers combined have at least a given number of messages. Thereceiversparameter is used to specify a list of the input channels/receivers. The minimum number of messages needed to permit the entity execution is set bymin_sizeparameter.Consider the example shown below. The associated entity will be executed when the number of messages combined for all the three receivers is at least the min_size, i.e. 5.-name:input_1type:nvidia::gxf::test::MockReceiverparameters:max_capacity:10-name:input_2type:nvidia::gxf::test::MockReceiverparameters:max_capacity:10-name:input_3type:nvidia::gxf::test::MockReceiverparameters:max_capacity:10-type:nvidia::gxf::MultiMessageAvailableSchedulingTermparameters:receivers:[input_1,input_2,input_3]min_size:5BooleanSchedulingTerm#An entity associated withnvidia::gxf::BooleanSchedulingTermis executed when its internal state is set to tick. The parameterenable_tickis used to control the entity execution. The scheduling term also has two APIsenable_tick()anddisable_tick()to toggle its internal state. The entity execution can be controlled by calling these APIs. Ifenable_tickis set to false, the entity is not executed (Scheduling condition is set toNEVER). Ifenable_tickis set to true, the entity will be executed (Scheduling condition is set toREADY). Entities can toggle the state of the scheduling term by maintaining a handle to it.Example usage --type:nvidia::gxf::BooleanSchedulingTermparameters:enable_tick:trueAsynchronousSchedulingTerm#AsynchronousSchedulingTerm is primarily associated with entities which
are working with asynchronous events happening outside of their regular
execution performed by the scheduler. Since these events are
non-periodic in nature, AsynchronousSchedulingTerm prevents the
scheduler from polling the entity for its status regularly and reduces
CPU utilization. AsynchronousSchedulingTerm can either be
in READY, WAIT, WAIT_EVENT or NEVER states based on asynchronous event
it’s waiting on.The state of an asynchronous event is described
using nvidia::gxf::AsynchronousEventState and is updated using
the setEventState API.AsynchronousEventStateDescriptionREADYInit state, first tick is pendingWAITRequest to async service yet to be sent, nothing to do but waitEVENT_WAITINGRequest sent to an async service, pending event done notificationEVENT_DONEEvent done notification received, entity ready to be tickedEVENT_NEVEREntity does not want to be ticked again, end of executionEntities associated with this scheduling term most likely have an
asynchronous thread which can update the state of the scheduling term
outside of it’s regular execution cycle performed by the gxf scheduler.
When the scheduling term is in WAIT state, the scheduler regularly polls
for the state of the entity. When the scheduling term is
in EVENT_WAITING state, schedulers will not check the status of the
entity again until they receive an event notification which can be
triggered using the GxfEntityEventNotify api. Setting the state of the
scheduling term to EVENT_DONE automatically sends this notification to
the scheduler. Entities can use the EVENT_NEVER state to indicate the
end of its execution cycle.Example usage --name:async_scheduling_termtype:nvidia::gxf::AsynchronousSchedulingTermDownstreamReceptiveSchedulingTerm#This scheduling term specifies that an entity shall be executed if the receiver for a given transmitter can accept new messages.Example usage --name:downstream_sttype:nvidia::gxf::DownstreamReceptiveSchedulingTermparameters:transmitter:outputmin_size:1TargetTimeSchedulingTerm#This scheduling term permits execution at a user-specified timestamp. The timestamp is specified on the clock provided.Example usage --name:target_sttype:nvidia::gxf::TargetTimeSchedulingTermparameters:clock:clock/manual_clockExpiringMessageAvailableSchedulingTerm#This scheduling waits for a specified number of messages in the receiver. The entity is executed when the first message received in the queue is expiring or when there are enough messages in the queue. Thereceiverparameter is used to set the receiver to watch on. The parametersmax_batch_sizeandmax_delay_nsdictate the maximum number of messages to be batched together and the maximum delay from first message to wait before executing the entity respectively.In the example shown below, the associated entity will be executed when the number of messages in the queue is greater thanmax_batch_size, i.e 5, or when the delay from the first message to current time is greater thanmax_delay_ns, i.e 10000000.-name:target_sttype:nvidia::gxf::ExpiringMessageAvailableSchedulingTermparameters:receiver:signalmax_batch_size:5max_delay_ns:10000000clock:misc/clockMessageAvailableFrequencyThrottler#A scheduling term which lets an entity maintain a specific execution (min) frequency. The scheduling term will also monitor messages incoming via
multiple receivers and switch to READY state if any messages are available.-type:nvidia::gxf::MessageAvailableFrequencyThrottlerparameters:receivers:[receiver_0,receiver_1]execution_frequency:100Hzmin_sum:1sampling_mode:SumOfAllMemoryAvailableSchedulingTerm#A scheduling term which waits until a given number of blocks are available in a pool. This can be used to force a codelet to wait
until a minimum number of its in-flight buffers have returned from downstream consumers.-type:nvidia::gxf::MemoryAvailableSchedulingTermparameters:allocator:allocatormin_bytes:256min_blocks:1024BTSchedulingTerm#A BT (Behavior Tree) scheduling term is used to schedule a behavior tree entity itself and its child entities (if any) in a Behavior tree.Example usage -name:rootcomponents:-name:root_controllertype:nvidia::gxf::EntityCountFailureRepeatControllerparameters:max_repeat_count:0-name:root_sttype:nvidia::gxf::BTSchedulingTermparameters:is_root:true-name:root_codelettype:nvidia::gxf::SequenceBehaviorparameters:children:[child1/child1_st]s_term:root_stcontroller:root_controllerCombining SchedulingTerms#An entity can be associated with multiple scheduling terms which define
it’s execution behavior. Scheduling terms are AND combined to describe
the current state of an entity. For an entity to be executed by the
scheduler, all the scheduling terms must be in READY state and
conversely, the entity is unscheduled from execution whenever any one of
the scheduling term reaches NEVER state. The priority of various states
during AND combine follows the
order NEVER, WAIT_EVENT, WAIT, WAIT_TIME, and READY.Example usage -components:-name:integerstype:nvidia::gxf::DoubleBufferTransmitter-name:fibonaccitype:nvidia::gxf::DoubleBufferTransmitter-type:nvidia::gxf::CountSchedulingTermparameters:count:100-type:nvidia::gxf::DownstreamReceptiveSchedulingTermparameters:transmitter:integersmin_size:1Cuda based scheduling terms#Producer codelets based on cuda based processing can queue in the jobs to the cuda stream without
waiting for the jobs to be complete. The consumers can wait for the availability of the data with
the help of below two described scheduling terms.Cuda Stream SchedulingTerm#CudaStreamSchedulingTerm specifies the availability of data at the receiver on completion of
the work on the provided cuda stream with the help of callback function to host.This scheduling term will register a call back function which will be called once the work on the
specified cuda stream completes indicating that the data is available for consumption.Example usage -components:-type:nvidia::gxf::CudaStreamSchedulingTermparameters:receiver:rx0-type:nvidia::gxf::MessageAvailableSchedulingTermparameters:receiver:rx1min_size:1Cuda Event SchedulingTerm#CudaEventSchedulingTerm specifies the availability of data at the receiver on completion of the
work on the provided cuda stream with the help of cuda event.
This scheduling term will keep polling on the event provided to check for data availability for
consumption.Example usage -components:-type:nvidia::gxf::CudaEventSchedulingTermparameters:receiver:rx0-type:nvidia::gxf::MessageAvailableSchedulingTermparameters:receiver:rx1min_size:1Connection Topologies#GXF supports creating multiple connection topologies between graph entities.1 : 1 ConnectionThe simplest connection between a single transmitter and a single receiver. The underlying codelets have either aHandle<Transmitter>andHandle<Receiver>as the registered parameter.1 : m ConnectionA single transmitter can be connected to a single receiver and vice-versa. The underlying codelets have either aHandle<Transmitter>andHandle<Receiver>as the registered parameter.Multiple 1 : 1 ConnectionsA 1 : m connection can alternatively be realized by creating multiple 1 : 1 connections. In this scenario, the underlying codelet in
the receiver entity must have either astd::vector<Handle<Receiver>>orstd::Array<Handle<Receiver>,N>parameter.
The same is applicable to m : 1 connections as well. Each 1 : 1 connection will have its own scheduling terms to monitor the
incoming and outgoing message queues.Messages#In the GXF graph, many a timesCodeletsmay have to communicate with each other to accomplish
the defined pipeline. Messages are the form of communication by codelets with each other.
When publishing, a message will always have an associatedTimestampcomponent with the name“timestamp”.
If the user doesn’t add it, it will automatically be added when the entity is published.ATimestampcomponent contains two different time values (See thegxf/std/timestamp.hppheader file for more information.):1.acqtime- This is the time when the message entity is acquired, for instance, this would generally
be the driver time of the camera when it captures an image. You must provide this timestamp if you are publishing a message in a codelet.1.pubtime- This is the time when the message entity is published by a node in the graph. This
will automatically get updated using the clock of the scheduler.Transmitteris used for transmitting the message andReceiveris used for receiving
the message. Messages are transmitted or received at tick() of codelet.Transmitter#All the messages from the transmitter are sent as an entity. The transmitter after encapsulating the
message will give apublish()call which will send the message to target recipient.In a codelet, when publishing message entities using aTransmitter(tx), there are two ways to add
the requiredTimestamp:1.tx.publish(Entitymessage): You can manually add a component of typeTimestampwith the name
“timestamp” and set theacqtime. Thepubtimein this case should be set to0. The message is
published using thepublish(Entitymessage).This will be deprecated in
the next release.2.tx.publish(Entitymessage,int64_tacqtime): You can simply callpublish(Entitymessage,int64_tacqtime)with theacqtime. Timestamp will be added automatically.Receiver#All the messages from the transmitter are received as an entity. The receiver on getting a tick()
call will give areceive()call that helps in receiving the message.Below is an example of transmitter and receiverTransmitter Example#gxf_result_tPingTx::tick(){Expected<Entity>message=Entity::New(context());if(!message){GXF_LOG_ERROR("Failure creating message entity.");returnmessage.error();}autoint_value=message.value().add<int32_t>("Integer");autovalue=int_value.value();*value=9999;autoresult=signal_->publish(message.value());GXF_LOG_INFO("Message Sent: int_value = %d",*value);returnToResultCode(message);}Receiver Example#gxf_result_tPingRx::tick(){automessage=signal_->receive();if(!message||message.value().is_null()){returnGXF_CONTRACT_MESSAGE_NOT_AVAILABLE;}autovalue=message.value();autorx_value=message->findAll<int32_t>();GXF_LOG_INFO("Message Received: rx_value = %d",*(rx_value->at(0).value()));returnGXF_SUCCESS;}Memory Management#GXF provides a way for allocation and de-allocation of memory that would be required by the
codelets. There are various types of memory allocation that are provided as mentioned below.System Memory
Allocates specified bytes of system memory which typically would make use of underlying OS calls
for allocation of requested memory.Host Memory
Allocates specified bytes of host memory that is page-locked and accessible to the device.
Allocating excessive amounts of memory with cudaMallocHost() may degrade system performance,
since it reduces the amount of memory available to the system for paging. This memory is best used
sparingly to allocate staging areas for data exchange between host and device.Device Memory
Allocates specified bytes of device memory.GXF provides a component callednvidia::gxf::BlockMemoryPoolwhich is used to allocate memory
in multiple/single blocks of same size which can be specified as parameters.Below example specifies on how to allocate host memory-name:host_memory_pooltype:nvidia::gxf::BlockMemoryPoolparameters:storage_type:0# host memoryblock_size:1024num_blocks:5Below example specifies on how to allocate device memory-name:cuda_device_memory_pooltype:nvidia::gxf::BlockMemoryPoolparameters:storage_type:1# device memoryblock_size:1024num_blocks:5Below example specifies on how to allocate system memory-name:system_memory_pooltype:nvidia::gxf::BlockMemoryPoolparameters:storage_type:2# system memoryblock_size:1024num_blocks:5Below example specifies on how to use allocator with codelets-name:host_memory_pooltype:nvidia::gxf::BlockMemoryPoolparameters:storage_type:0# host memoryblock_size:1024num_blocks:5-name:cuda_device_memory_pooltype:nvidia::gxf::BlockMemoryPoolparameters:storage_type:1# device memoryblock_size:1024num_blocks:5-name:generatortype:nvidia::gxf::test::cuda::StreamTensorGeneratorparameters:cuda_tensor_pool:cuda_device_memory_poolhost_tensor_pool:host_memory_poolCuda Stream Order Allocator#GXF provides a way for allocation and de-allocation of memory using native Cuda API in stream ordered approach that
would be required by the codelets. There is only device memory allocation type which is supported as mentioned below.Device Memory :
Allocates device memory in stream ordered approach. This stream-ordered allocation ensures that the memory operations
are properly synchronized with other GPU operations in the same stream. This allocation method utilizes CUDA’s asynchronous
memory management functions, specifically cudaMallocAsync and cudaFreeAsync, to allocate and deallocate memory respectively.GXF provides a component callednvidia::gxf::StreamOrderedAllocatorwhich is used to allocate/deallocate device
memory in stream ordered approach. This component enables efficient and stream-ordered memory allocation for GPU operations
within the GXF ecosystem.Below example specifies on how to allocate device memory. The memory allocation type supported here is Device Memory.-name:device_memory_pooltype:nvidia::gxf::StreamOrderedAllocatorparameters:device_memory_initial_size:"32KB"# 32 KB initial pool sizedevice_max_memory_pool_size:"64MB"# 64 MB max pool sizeRMM: RAPIDS Memory Manager#GXF provides a way for allocation and de-allocation of memory that would be required by the
codelets using RMM. There are various types of memory allocation that are provided as mentioned below.Host Memory :
Allocates specified bytes of pinned host memory that is page-locked and accessible to the device.
It takes advantage of RMM’s pool allocation strategy for better performanceDevice Memory :
Allocates specified bytes of device memory in stream ordered approach. This stream-ordered
allocation ensures that the memory operations are properly synchronized with other GPU
operations in the same stream.GXF provides a component callednvidia::gxf::RMMAllocatorwhich is used to allocate/deallocate memory
using Rapids Memory Manager (RMM). This component enables efficient and stream-ordered memory
allocation for GPU operations within the GXF ecosystem.Below example specifies on how to allocate host/device memory. The memory allocation strategy is
determined by the parameter value passed to the allocation function. Depending on this value, the function
decides whether to allocate memory on the host or on the device.-name:memory_pooltype:nvidia::gxf::RMMAllocatorparameters:device_memory_initial_size:"32B"# 32 bytes initial pool sizehost_memory_initial_size:"32KB"# 32 KB initial pool sizedevice_max_memory_pool_size:"64MB"# 64 MB max pool sizehost_memory_max_size:"64GB"# 64 GB max pool sizeDistributed Execution#Segment is a group of graph entities created in a single GXF runtime context. A segment
will have its own scheduler. Graph entities in a segment are connected with each other via
double buffer transmitter and receiver components. A segment is connected to other
segments via ucx transmitters and receivers. The connected pair of segments can run in two remote processes,
or within the same process.The execution of segment is governed by the graph worker and the graph driver.
The graph worker is anvidia::gxf::Systemcomponent responsible for orchestrating the execution
of each segment configured to this worker. The graph worker provides and manages
threads for each segment to run, and communicates with the graph driver to determine
the life cycle of each segment.The graph worker and driver are GXF System components, that blocks main thread during its life cycle and
spawns its own threads for execution. It’s very similar to GXF schedulers in a way that,
each scheduler orchestrates one GXF runtime from inside; whereas graph worker orchestrate one or multiple
GXF runtimes from outside. The graph driver orchestrates one or more graph workers to go through the
execution sequence.Both graph worker and graph driver consists of an IPC server and an IPC client.
The IPC server’s services are implemented within graph worker and graph driver respectively.
These IPC services are exposed via the IPC server,
that a corresponding IPC client on remote process can call.
graph worker’s IPC client sends remote request to graph driver’s services;
graph driver’s IPC client sends remote request to graph worker’s services.GraphWorker#Graph worker implements a series of IPC services to handle different steps in running a graph segment.
Calling on these IPC services via the IPC server like Http or Grpc triggers these steps.
Each IPC service ultimately translates a non-blocking IPC call to enqueuing an event.The threading of graph worker is based on an event-based queue thread, and a table of segment runners.Figure: Graph worker execution sequenceAs shown in the sequence diagram, the caller main thread starts graph worker in 4 steps.register the series of IPC services and start the server;start the event-based queue thread;enqueue event to instantiate the table of segment runners;enqueue event to request communication with graph driver. Then the caller main thread gets blocked waiting for completion of graph worker.Graph worker’s first communication to graph driver is to register all segments along with their address
info, eg IP and port. When graph driver discovers all segments,
it possesses global knowledge of all segments and their addresses.
It is graph driver to resolve the UCX connections between all segment pairs, and
for each pair spread UCX receiver addresses to UCX transmitter via the graph worker.Example graph worker entity:Figure: Entity containing GraphWorker and its dependency---name:worker_entitycomponents:-name:graph_workertype:nvidia::gxf::GraphWorkerparameters:server:this_worker_serverclient:client_to_remote_drivergraph-specs:ucx_upstream:app-path:gxf/ucx/tests/test_ping_ucx_tx.yamlmanifest-path:gxf/test/distributed/test_graph_worker_manifest.yamlseverity:4ucx_downstream:app-path:gxf/ucx/tests/test_ping_ucx_rx.yamlmanifest-path:gxf/test/distributed/test_graph_worker_manifest.yamlseverity:4-name:this_worker_servertype:nvidia::gxf::HttpServerparameters:port:50001remote_access:'True'-name:client_to_remote_drivertype:nvidia::gxf::HttpIPCClientparameters:server_ip_address:localhostport:50000GraphDriver#Graph driver implements a series of IPC services to handle different steps in
discovering all segments in their graph workers; and resolving segments UCX connection addresses.
Calling on these IPC services via the IPC server like Http or Grpc triggers these steps.
Each IPC service ultimately translates a non-blocking IPC call to enqueuing an event.The threading of graph driver is based on an event-based queue thread.Figure: Graph driver execution sequenceAs shown in the sequence diagram, the caller main thread starts graph driver in 3 steps.read the global segment connection map configuration file;register the series of IPC services and start the server;start the event-based queue thread. Then the caller main thread gets blocked waiting for completion of graph driver.After startup, graph driver listens to requests from all graph workers.
Upon each graph worker startup, it sends request to the graph driver to report its segments and address info.
Graph driver is expected to starts before all graph workers. However if graph worker starts before graph driver,
it keeps re-trying the request for a configurable times.Example graph driver entity:Figure: Entity containing GraphDriver and its dependency---name:drivercomponents:-name:driver_servertype:nvidia::gxf::HttpServerparameters:port:50000-name:driver_clienttype:nvidia::gxf::HttpIPCClient-name:graph_drivertype:nvidia::gxf::GraphDriverparameters:server:driver_serverclient:driver_clientconnections:-source:ucx_upstream.tx.signaltarget:ucx_downstream.rx.signalLogging#GXF logging macros are automatically included when implementing a codelet.
The usage is similar to printf(), but they will also print file name and line numbers.
See example usage in the sample extension section.GXF_LOG_VERBOSE(...)#Example:GXF_LOG_VERBOSE("This is a test message, codelet eid %ld, cid %ld, name %s",eid(),cid(),name());GXF_LOG_DEBUG(...)#Example:GXF_LOG_DEBUG("This is a test message, codelet eid %ld, cid %ld, name %s",eid(),cid(),name());GXF_LOG_INFO(...)#Example:GXF_LOG_INFO("This is a test message, codelet eid %ld, cid %ld, name %s",eid(),cid(),name());GXF_LOG_WARNING(...)#Example:GXF_LOG_WARNING("This is a test message, codelet eid %ld, cid %ld, name %s",eid(),cid(),name());GXF_LOG_ERROR(...)#Example:GXF_LOG_ERROR("This is a test message, codelet eid %ld, cid %ld, name %s",eid(),cid(),name());Logging LevelsGXF Logging supports the following log levelsGXF_LOG_LEVEL_PANIC#Defined as0.GXF_LOG_LEVEL_ERROR#Defined as1.GXF_LOG_LEVEL_WARNING#Defined as2.GXF_LOG_LEVEL_INFO#Defined as3.GXF_LOG_LEVEL_DEBUG#Defined as4.GXF_LOG_LEVEL_VERBOSE#Defined as5.DefineGXF_LOG_ACTIVE_LEVELbefore includingcommon/logger.hppto control the logging level at compile time.
This allows you to skip logging at certain levels.Example:#define GXF_LOG_ACTIVE_LEVEL 2#include"common/logger.hpp"With this setting, logging will occur only at the WARNING(2), ERROR(1), and PANIC(0) levels.You can define GXF_LOG_ACTIVE_LEVEL in your build system. For instance, in CMake, use:target_compile_definitions(my_targetPRIVATEGXF_LOG_ACTIVE_LEVEL=2)This sets the active logging level to WARNING(2) for the targetmy_target.Alternatively, define GXF_LOG_ACTIVE_LEVEL at compile time by passing-DGXF_LOG_ACTIVE_LEVEL=2directly to the compiler.In the Bazel build system, set this in your build configuration as follows:cc_binary(
  name = "my_binary",
  srcs = ["my_binary.cc"],
  copts = ["-DGXF_LOG_ACTIVE_LEVEL=2"],
)This sets the active logging level to WARNING(2) for the targetmy_binary.Or, when using a Bazel build command:bazelbuild--copt=-DGXF_LOG_ACTIVE_LEVEL=3//path:to_your_targetThis sets the active logging level to INFO(3) for the target//path:to_your_target.GXF_LOG_ACTIVE_LEVELCan also be used enable logging severity specific to each source file.Example:FileA#define GXF_LOG_ACTIVE_LEVEL 2#include"common/logger.hpp"CodeletB#define GXF_LOG_ACTIVE_LEVEL 5#include"common/logger.hpp"File A will have warning and above logs, and File B will have verbose and above logs.Component Factory#An extension contains its own component factory, in which all components are explicitly registered.
GXF provides helper macros to easily implement the component factory needed for any extension.
Normally extension factory implementation is organized as a standalone cpp file at the extension directory.
It starts with GXF_EXT_FACTORY_BEGIN() and ends with GXF_EXT_FACTORY_END().A component can be registered using the macro GXF_EXT_FACTORY_ADD(). For each
component the base class need to be specified. Components base classes must be registered before
they can be used as a base class in a component registration. If a component does not have a base
class the macro GXF_EXT_FACTORY_ADD_0() is used instead.Components can have at most one base class. Multiple base classes are not supported.A unique 128-bit identifier must be provided for the factory and each component. The identifier must be
unique across all existing extensions.Example:GXF_EXT_FACTORY_BEGIN()GXF_EXT_FACTORY_SET_INFO(0xd8629d822909316d,0xa9ee7410c8c1a7b6,"test","A Dummy Example","","1.0.0","NVIDIA");// ...GXF_EXT_FACTORY_ADD(0xd39d70014cab3ecf,0xb397c9d200cf9e8d,sample::test::HelloWorld,nvidia::gxf::Codelet,"Dummy example source codelet.");// ...GXF_EXT_FACTORY_END()The following macros are provided to help simplify the process of creating a component factory. The terms Component Factory, Extension
and Extension are used interchangeably in these macros.Note that the extension factory can also be created manually without using these macros.Create a Component FactoryGXF_EXT_FACTORY_BEGIN()#Start defining CreateComponentFactory() function, that starts with creating anvidia::gxf::DefaultExtensionobject.
H1 and H2 are first and second half hash values of the 128-bit identifier.GXF_EXT_FACTORY_SET_INFO(H1,H2,NAME,DESC,AUTHOR,VERSION,LICENSE)#Set info for this extension factory.Example:GXF_EXT_FACTORY_SET_INFO(0xd8629d822909316d,0xa9ee7410c8c1a7b6,"test","A Dummy Example","","1.0.0","NVIDIA");GXF_EXT_FACTORY_SET_DISPLAY_INFO(DISPLAY_NAME,CATEGORY,BRIEF)#Set additional display info this component factory.Example:GXF_EXT_FACTORY_SET_DISPLAY_INFO("Dummy Extension","Dummy","GXF Dummy Extension");GXF_EXT_FACTORY_END()#Close the CreateComponentFactory() function and then call CreateComponentFactory() to return the factory object.Register ComponentsGXF_EXT_FACTORY_ADD(H1,H2,TYPE,BASE,DESC)#Register a new component to this component factory.Example:GXF_EXT_FACTORY_ADD(0xd39d70014cab3ecf,0xb397c9d200cf9e8d,nvidia::gxf::DummyCodelet,nvidia::gxf::Codelet,"Description");GXF_EXT_FACTORY_ADD_VERBOSE(H1,H2,TYPE,BASE,DISPLAY_NAME,BRIEF,DESC,)#Register a new component to this component factory with verbose metadata.Example:GXF_EXT_FACTORY_ADD_VERBOSE(0xd39d70014cab3ecf,0xb397c9d200cf9e8d,nvidia::gxf::DummyCodelet,nvidia::gxf::Codelet,"Display Name","Brief","Description");GXF_EXT_FACTORY_ADD_LITE(H1,H2,TYPE,BASE)#Register a new component to this component factory with minimal metadata.Example:GXF_EXT_FACTORY_ADD_LITE(0xd39d70014cab3ecf,0xb397c9d200cf9e8d,nvidia::gxf::DummyCodelet,nvidia::gxf::Codelet);Register Components without a base typeGXF_EXT_FACTORY_ADD_0(H1,H2,TYPE,DESC)#Register a new component to this component factory with minimal metadata.Example:GXF_EXT_FACTORY_ADD_0(0xd39d70014cab3ecf,0xb397c9d200cf9e8d,sample::test::Helper,"Description");GXF_EXT_FACTORY_ADD_0_VERBOSE(H1,H2,TYPE,DISPLAY_NAME,BRIEF,DESC)#Register a new component to this component factory with verbose metadata.Example:GXF_EXT_FACTORY_ADD_0_VERBOSE(0xd39d70014cab3ecf,0xb397c9d200cf9e8d,sample::test::Helper,"Description","Brief","Description");GXF_EXT_FACTORY_ADD_0_LITE(H1,H2,TYPE)#Example:GXF_EXT_FACTORY_ADD_0_LITE(0xd39d70014cab3ecf,0xb397c9d200cf9e8d,sample::test::Helper);previousDeepStream ComponentsnextGraph Execution EngineOn this pageLifeCycle of a CodeletThe GXF SchedulerGreedy SchedulerGreedy Scheduler ConfigurationMultithread SchedulerMultithread Scheduler ConfigurationEvent Based SchedulerMessaging and EventsEvent Based Scheduler ConfigurationEpoch SchedulerSchedulingTermsPeriodicSchedulingTermCountSchedulingTermMessageAvailableSchedulingTermMultiMessageAvailableSchedulingTermBooleanSchedulingTermAsynchronousSchedulingTermDownstreamReceptiveSchedulingTermTargetTimeSchedulingTermExpiringMessageAvailableSchedulingTermMessageAvailableFrequencyThrottlerMemoryAvailableSchedulingTermBTSchedulingTermCombining SchedulingTermsCuda based scheduling termsCuda Stream SchedulingTermCuda Event SchedulingTermConnection TopologiesMessagesTransmitterReceiverTransmitter ExampleReceiver ExampleMemory ManagementCuda Stream Order AllocatorRMM: RAPIDS Memory ManagerDistributed ExecutionGraphWorkerGraphDriverLoggingGXF_LOG_VERBOSE()GXF_LOG_DEBUG()GXF_LOG_INFO()GXF_LOG_WARNING()GXF_LOG_ERROR()Component FactoryGXF_EXT_FACTORY_BEGIN()GXF_EXT_FACTORY_SET_INFO()GXF_EXT_FACTORY_SET_DISPLAY_INFO()GXF_EXT_FACTORY_END()GXF_EXT_FACTORY_ADD()GXF_EXT_FACTORY_ADD_VERBOSE()GXF_EXT_FACTORY_ADD_LITE()GXF_EXT_FACTORY_ADD_0()GXF_EXT_FACTORY_ADD_0_VERBOSE()GXF_EXT_FACTORY_ADD_0_LITE()Privacy Policy|Manage My Privacy|Do Not Sell or Share My Data|Terms of Service|Accessibility|Corporate Policies|Product Security|ContactCopyright © 2024-2025, NVIDIA Corporation.Last updated on Jan 13, 2025.