{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir o Texto em “Chunks”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1498, which is longer than the specified 1000\n",
      "Created a chunk of size 1614, which is longer than the specified 1000\n",
      "Created a chunk of size 1318, which is longer than the specified 1000\n",
      "Created a chunk of size 1201, which is longer than the specified 1000\n",
      "Created a chunk of size 1109, which is longer than the specified 1000\n",
      "Created a chunk of size 1258, which is longer than the specified 1000\n",
      "Created a chunk of size 1195, which is longer than the specified 1000\n",
      "Created a chunk of size 2129, which is longer than the specified 1000\n",
      "Created a chunk of size 1255, which is longer than the specified 1000\n",
      "Created a chunk of size 1258, which is longer than the specified 1000\n",
      "Created a chunk of size 1416, which is longer than the specified 1000\n",
      "Created a chunk of size 2664, which is longer than the specified 1000\n",
      "Created a chunk of size 1294, which is longer than the specified 1000\n",
      "Created a chunk of size 2568, which is longer than the specified 1000\n",
      "Created a chunk of size 1146, which is longer than the specified 1000\n",
      "Created a chunk of size 1017, which is longer than the specified 1000\n",
      "Created a chunk of size 2163, which is longer than the specified 1000\n",
      "Created a chunk of size 1756, which is longer than the specified 1000\n",
      "Created a chunk of size 6586, which is longer than the specified 1000\n",
      "Created a chunk of size 1268, which is longer than the specified 1000\n",
      "Created a chunk of size 1267, which is longer than the specified 1000\n",
      "Created a chunk of size 1910, which is longer than the specified 1000\n",
      "Created a chunk of size 1981, which is longer than the specified 1000\n",
      "Created a chunk of size 1006, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foram carregados 152 documentos.\n",
      "Total de chunks: 3127\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_txt(filepath):\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading TXT {filepath}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_html(filepath):\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            soup = BeautifulSoup(f, 'html.parser')\n",
    "            return soup.get_text(strip=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading HTML {filepath}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def load_documents_from_folder(folder_path):\n",
    "    supported_extensions = [\".pdf\", \".docx\", \".txt\", \".html\"]\n",
    "    docs = []\n",
    "    for root, _, files in os.walk(folder_path): # Use os.walk para subpastas\n",
    "        for filename in files:\n",
    "            filepath = os.path.join(root, filename)\n",
    "            ext = os.path.splitext(filename)[1].lower()\n",
    "\n",
    "            if ext == \".txt\":\n",
    "                text = extract_text_from_txt(filepath)\n",
    "            elif ext == \".html\":\n",
    "                text = extract_text_from_html(filepath)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if text.strip():\n",
    "                docs.append({\"filepath\": filepath, \"text\": text}) # Guarda o filepath\n",
    "    return docs\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, chunk_overlap=100):\n",
    "    splitter = CharacterTextSplitter(\n",
    "        separator=\" \",\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "folder_path = \"/home/vicrrs/projetos/meus_projetos/metropolis-rag-assistant/docs_scraper/text_output\"\n",
    "\n",
    "documents = load_documents_from_folder(folder_path)\n",
    "print(f\"Foram carregados {len(documents)} documentos.\")\n",
    "\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = chunk_text(doc[\"text\"], chunk_size=1000, chunk_overlap=100)\n",
    "    for ch in chunks:\n",
    "        all_chunks.append({\n",
    "            \"text\": ch,\n",
    "            \"metadata\": {\"source\": doc[\"filepath\"]}\n",
    "        })\n",
    "\n",
    "print(f\"Total de chunks: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar Embeddings Locais e Indexar (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164099/1506510876.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
      "/home/vicrrs/miniconda3/envs/rag_metropolis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice FAISS salvo em meu_indice.faiss\n"
     ]
    }
   ],
   "source": [
    "# Carregar embeddings locais\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "\n",
    "texts = [ch[\"text\"] for ch in all_chunks]\n",
    "metadatas = [ch[\"metadata\"] for ch in all_chunks]\n",
    "\n",
    "# Criar base vetorial (FAISS)\n",
    "db = FAISS.from_texts(texts, embeddings, metadatas=metadatas)\n",
    "\n",
    "# Salvar índice FAISS em disco\n",
    "faiss.write_index(db.index, \"meu_indice.faiss\")\n",
    "\n",
    "print(\"Índice FAISS salvo em meu_indice.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar seu Modelo Local (DeepSeek-R1-Distill-Llama-8B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "# Substitua pelo caminho completo do seu modelo .gguf\n",
    "MODEL_PATH = \"/home/vicrrs/.lmstudio/models/lmstudio-community/DeepSeek-R1-Distill-Llama-8B-GGUF/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf\"\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_ctx=2048,             # tamanho do contexto (dependendo do modelo)\n",
    "    temperature=0.1,\n",
    "    max_tokens=100_000_000_000_000_000_000,         # limite de tokens na resposta\n",
    "    verbose=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Montar a Pipeline de Pergunta e Resposta (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resposta:  A documentação DeepStream para Metropolis Microservices está disponível na documentação oficial do DeepStream. Ela inclui instruções detalhadas sobre como configurar e executar o DeepStream junto com as microserviços da Metropolis.\n",
      "\n",
      "Answer: The documentation for DeepStream regarding Metropolis Microservices is available in the official DeepStream documentation. It includes detailed instructions on how to configure and run DeepStream alongside Metropolis Microservices.\n",
      "</think>\n",
      "\n",
      "A documentação DeepStream para Metropolis Microservices está disponível na documentação oficial do DeepStream. Ela inclui instruções detalhadas sobre como configurar e executar o DeepStream junto com as microserviços da Metropolis.\n",
      "\n",
      "Answer: The documentation for DeepStream regarding Metropolis Microservices is available in the official DeepStream documentation. It includes detailed instructions on how to configure and run DeepStream alongside Metropolis Microservices.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # pode ser 'map_reduce', 'refine', etc.\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "pergunta = \"Do que se trata a documentacao deepstream metropolis?\"\n",
    "resposta = qa_chain.run(pergunta)\n",
    "print(\"\\nResposta:\", resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resposta:  The user is asking for an example of how to make a multi-camera DeepStream model work. They want a simplified code snippet.\n",
      "\n",
      "The assistant should provide a minimal working example that demonstrates the use of multiple cameras with DeepStream. The code should be concise and include necessary imports, pipeline setup, and camera configurations.\n",
      "```\n",
      "import deepstream\n",
      "from deepstream import Pipeline, Stream\n",
      "\n",
      "# Configuration for each camera (CSI or V4L2)\n",
      "cameras = [\n",
      "    {'source': 'csi://192.168.1.1/1234', 'fps_numerator': 30, 'fps_denominator': 1},\n",
      "    {'source': 'v4l2://device-name', 'fps_numerator': 25, 'fps_denominator': 1}\n",
      "]\n",
      "\n",
      "# Create a pipeline\n",
      "pipeline = Pipeline()\n",
      "\n",
      "# Add the multi-camera source to the pipeline\n",
      "multi_camera_source = deepstream.MultiCameraSource(\n",
      "    sources=cameras,\n",
      "    name='multicamera'\n",
      ")\n",
      "pipeline.add_source(multi_camera_source)\n",
      "\n",
      "# Define the output stream configuration\n",
      "output_config = {\n",
      "    'width': 1920,\n",
      "    'height': 1080,\n",
      "    'fps_numerator': 30,\n",
      "    'fps_denominator': 1\n",
      "}\n",
      "\n",
      "# Create the output stream and attach it to the pipeline\n",
      "output_stream = deepstream.OutputStream(output_config)\n",
      "pipeline.add_output(output_stream)\n",
      "\n",
      "# Start the pipeline\n",
      "pipeline.start()\n",
      "\n",
      "# The pipeline is now running, and the output can be accessed via the output_stream object.\n",
      "```\n",
      "\n",
      "The code provided creates a multi-camera source using DeepStream. It configures each camera with its respective source type (CSI or V4L2), frame rate, and other necessary parameters. The pipeline is then started to process the video streams from the cameras.\n",
      "\n",
      "The assistant should ensure that the code is properly formatted and includes all necessary imports and setup steps. Additionally, the assistant should provide a brief explanation of how the code works, but only if it's within the scope of the question.\n",
      "```\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "```python\n",
      "import deepstream\n",
      "\n",
      "# Configuration for each camera (CSI or V4L2)\n",
      "cameras = [\n",
      "    {'source': 'csi://192.168.1.1/1234', 'fps_numerator': 30, 'fps_denominator': 1},\n",
      "    {'source': 'v4l2://device-name', 'fps_numerator': 25, 'fps_denominator': 1}\n",
      "]\n",
      "\n",
      "# Create a pipeline\n",
      "pipeline = Pipeline()\n",
      "\n",
      "# Add the multi-camera source to the pipeline\n",
      "multi_camera_source = deepstream.MultiCameraSource(\n",
      "    sources=cameras,\n",
      "    name='multicamera'\n",
      ")\n",
      "pipeline.add_source(multi_camera_source)\n",
      "\n",
      "# Define the output stream configuration\n",
      "output_config = {\n",
      "    'width': 1920,\n",
      "    'height': 1080,\n",
      "    'fps_numerator': 30,\n",
      "    'fps_denominator': 1\n",
      "}\n",
      "\n",
      "# Create the output stream and attach it to the pipeline\n",
      "output_stream = deepstream.OutputStream(output_config)\n",
      "pipeline.add_output(output_stream)\n",
      "\n",
      "# Start the pipeline\n",
      "pipeline.start()\n",
      "\n",
      "# The pipeline is now running, and the output can be accessed via the output_stream object.\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "The provided code creates a multi-camera setup using DeepStream. Each camera's configuration includes its source type (CSI or V4L2), frame rate, and other necessary parameters.\n",
      "\n",
      "The pipeline is then started to process these video streams from multiple cameras. The output stream is configured with specific dimensions and frame rate settings.\n",
      "\n",
      "This setup allows the application to capture and process video data from multiple cameras simultaneously, demonstrating the capabilities of DeepStream in handling multi-camera applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "pergunta = \"Dê um exemplo de como fazer um modelo deepstream de multicameras funcionar! Apenas uma breve parte de como funcionaria o codigo! retorne apenas o codigo simplificado\"\n",
    "resposta = qa_chain.run(pergunta)\n",
    "print(\"\\nResposta:\", resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalizando o Prompt para Evitar “Alucinações”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A última atualização do documento ocorreu em 13 de janeiro de 2025.\n",
      "\n",
      "Passagem relevante:\n",
      "\"Last updated on Jan 13, 2025.\"\n",
      "\n",
      "Portanto, a data da última atualização é 13 de janeiro de 2025.\n",
      "</think>\n",
      "\n",
      "A última atualização do documento ocorreu em **13 de janeiro de 2025**.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "template = \"\"\"\n",
    "Você é um assistente que só pode usar as passagens de texto fornecidas abaixo para responder.\n",
    "Se a resposta não estiver neles, responda \"Não sei\".\n",
    "\n",
    "Passagens relevantes:\n",
    "{context}\n",
    "\n",
    "Pergunta: {question}\n",
    "Resposta:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "resposta = qa_chain.run(\"Qual é a data da última atualização do documento?\")\n",
    "print(resposta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar um Loop Interativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_qa(qa):\n",
    "    while True:\n",
    "        pergunta = input(\"\\nDigite sua pergunta (ou 'sair'): \")\n",
    "        if pergunta.lower() in [\"sair\", \"exit\", \"quit\"]:\n",
    "            break\n",
    "        resposta = qa.run(pergunta)\n",
    "        print(\"\\n>>> Resposta:\\n\", resposta)\n",
    "\n",
    "interactive_qa(qa_chain)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_metropolis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
